{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/piggyatbaqaqi/skol/blob/main/IST691/workspace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdUfyeOEulxm"
   },
   "source": [
    "# SKOL III: Feature extraction\n",
    "\n",
    "created by:\n",
    "* La Monte Yarroll\n",
    "* Padmaja Kurmaddali\n",
    "* Patrick Le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn6VsuAXrh7D"
   },
   "source": [
    "# **Mapping Data Files: Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5jGKzUfrwaV",
    "outputId": "0a93ef06-4f13-493a-e933-101070690969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "%cd /content\n",
    "content = Path('/content')\n",
    "skol = content / 'drive/My Drive/SKOL'\n",
    "piggyatbaqaqi = skol / 'github.com/piggyatbaqaqi'\n",
    "drive.mount(str(content / \"drive\"), force_remount=True)\n",
    "cache_path = content / 'cache'\n",
    "ollama_cache_path = content / 'ollama_cache'\n",
    "nb_path = content / 'packages'\n",
    "if not os.path.exists(nb_path):\n",
    "  nb_path.symlink_to(skol / 'packages')\n",
    "skol_client = content / 'skol'\n",
    "if not os.path.exists(skol_client):\n",
    "  skol_client.symlink_to(piggyatbaqaqi / 'skol')\n",
    "if not os.path.exists(cache_path):\n",
    "  cache_path.symlink_to(skol / 'pip_cache')\n",
    "if not os.path.exists(ollama_cache_path):\n",
    "  ollama_cache_path.symlink_to(skol / 'ollama_cache')\n",
    "os.environ['OLLAMA_MODELS'] = str(ollama_cache_path)\n",
    "sys.path.insert(0, str(nb_path))\n",
    "sys.path.insert(0, str(piggyatbaqaqi / 'skol'))pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CsjgFhE-A3s",
    "outputId": "f301876c-6432-4361-b100-44d0810da871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "lrwxrwxrwx 1 root root   38 Jun 12 14:09 cache -> '/content/drive/My Drive/SKOL/pip_cache'\n",
      "drwx------ 7 root root 4096 Jun 12 14:09 drive\n",
      "lrwxrwxrwx 1 root root   41 Jun 12 14:09 ollama_cache -> '/content/drive/My Drive/SKOL/ollama_cache'\n",
      "lrwxrwxrwx 1 root root   37 Jun 12 14:09 packages -> '/content/drive/My Drive/SKOL/packages'\n",
      "drwxr-xr-x 1 root root 4096 Jun 10 13:39 sample_data\n",
      "lrwxrwxrwx 1 root root   58 Jun 12 14:09 skol -> '/content/drive/My Drive/SKOL/github.com/piggyatbaqaqi/skol'\n",
      "/content: directory\n",
      "['/content/drive/My Drive/SKOL/github.com/piggyatbaqaqi/skol', '/content/packages', '/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/usr/local/lib/python3.11/dist-packages/setuptools/_vendor', '/root/.ipython']\n"
     ]
    }
   ],
   "source": [
    "!ls -l /content/\n",
    "!file /content\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cAPsvOTUUKh"
   },
   "source": [
    "## Set up git clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtP_WaRJDT3U",
    "outputId": "a6819c61-fc68-47b0-e483-62b4b05306ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/SKOL/IST691\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(piggyatbaqaqi):\n",
    "  %mkdir -p $piggyatbaqaqi\n",
    "if not os.path.exists(piggyatbaqaqi / 'skol'):\n",
    "  %cd $piggyatbaqaqi\n",
    "  !git clone https://github.com/piggyatbaqaqi/skol.git\n",
    "sys.path.insert(0, piggyatbaqaqi / 'skol')\n",
    "if not os.path.exists(piggyatbaqaqi / 'dr-drafts-mycosearch'):\n",
    "  %cd $piggyatbaqaqi\n",
    "  !git clone https://github.com/piggyatbaqaqi/dr-drafts-mycosearch.git\n",
    "workdir = skol / 'IST691'\n",
    "%cd $workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5tda4LKZUtUu",
    "outputId": "88eb303b-78c0-49eb-8181-33ece09b619e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-4.0.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dataproc-spark-connect 0.7.5 requires pyspark[connect]~=3.5.1, but you have pyspark 4.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed py4j-0.10.9.9 pyspark-4.0.0\n",
      "\u001b[33mWARNING: Target directory /content/packages/pyspark-4.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/py4j already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/pyspark already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/py4j-0.10.9.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/share already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sparknlp\n",
      "  Using cached sparknlp-1.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting ollama\n",
      "  Using cached ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting spark-nlp (from sparknlp)\n",
      "  Downloading spark_nlp-6.0.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy (from sparknlp)\n",
      "  Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting httpx>=0.27 (from ollama)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Using cached pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting anyio (from httpx>=0.27->ollama)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx>=0.27->ollama)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27->ollama)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.27->ollama)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27->ollama)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama)\n",
      "  Using cached pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic>=2.9->ollama)\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.27->ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\n",
      "Using cached ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "Downloading spark_nlp-6.0.3-py2.py3-none-any.whl (713 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m713.0/713.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: spark-nlp, typing-extensions, sniffio, numpy, idna, h11, certifi, annotated-types, typing-inspection, sparknlp, pydantic-core, httpcore, anyio, pydantic, httpx, ollama\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
      "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 anyio-4.9.0 certifi-2025.4.26 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 numpy-2.3.0 ollama-0.5.1 pydantic-2.11.5 pydantic-core-2.33.2 sniffio-1.3.1 spark-nlp-6.0.3 sparknlp-1.0.0 typing-extensions-4.14.0 typing-inspection-0.4.1\n",
      "\u001b[33mWARNING: Target directory /content/packages/h11-0.16.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/typing_inspection-0.4.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/pydantic-2.11.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/h11 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/sniffio-1.3.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/sniffio already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/pydantic_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/httpx already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/pydantic_core-2.33.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/sparknlp-1.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/typing_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/certifi-2025.4.26.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/numpy-2.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/annotated_types-0.7.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/httpcore-1.0.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/ollama-0.5.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/sparknlp already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/typing_inspection already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/pydantic already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/httpcore already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/httpx-0.28.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/anyio already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/anyio-4.9.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/ollama already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/annotated_types already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/com already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/typing_extensions-4.14.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /content/packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "4913c9c7992c40239b05dd8542d82469",
       "pip_warning": {
        "packages": [
         "certifi",
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# install PySpark\n",
    "! pip install --cache-dir=$cache_path --target=$nb_path pyspark\n",
    "! pip install --cache-dir=$cache_path --target=$nb_path sparknlp ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeMBGpu8rYXf"
   },
   "outputs": [],
   "source": [
    "#import needed modules\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name, collect_list, concat_ws, col, udf,\n",
    "    explode, collect_list, regexp_extract, regexp_replace,\n",
    "    split, flatten, transform, concat)\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, HashingTF, IDF, StringIndexer, CountVectorizer,\n",
    "    PCA, VectorAssembler)\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import split, row_number, min, expr, struct\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "from pyspark.sql.types import DoubleType, StructField, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXgY1VO8Sw_q"
   },
   "source": [
    "SKOL Data manipulation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIoHo7jTRMwG"
   },
   "outputs": [],
   "source": [
    "from finder import read_files, parse_annotated, target_classes\n",
    "from label import Label\n",
    "from taxon import Taxon, group_paragraphs\n",
    "\n",
    "SEED=12345\n",
    "default_label = Label('Misc-exposition')\n",
    "keep_labels = [Label('Description'), Label('Nomenclature')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUT4tUcDu4qR"
   },
   "source": [
    "# **Checking the file counts in the directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFPefKgUuAWv"
   },
   "outputs": [],
   "source": [
    "raw_directory_path = skol / 'raw_2025_02_05/'\n",
    "ann_directory_path = skol / 'annotated_2025_02_27/journals'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puPtns5oiHJr"
   },
   "source": [
    "## Checking the file counts in the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsnHqX23dsxV"
   },
   "outputs": [],
   "source": [
    "# Function that reports all the txt files under a Google Drive folder path\n",
    "def listFiles(folder: str) -> List[str]:\n",
    "  # List all files in the folder\n",
    "  try:\n",
    "      files = [file for file in glob.glob(f'{folder}/**/*.txt*', recursive=True) if 'Sydowia' not in file]\n",
    "      return files\n",
    "  except FileNotFoundError:\n",
    "      print(f\"Folder '{folder}' not found.\")\n",
    "  except PermissionError:\n",
    "      print(f\"Permission denied to access folder '{folder}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85IAP2WKhQYp"
   },
   "outputs": [],
   "source": [
    "# check files in raw directory\n",
    "listFiles(raw_directory_path)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLYkfNVyR_lR"
   },
   "outputs": [],
   "source": [
    "# check files in annotated directory\n",
    "training_files = listFiles(ann_directory_path)\n",
    "training_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBswbvOcVmRo"
   },
   "outputs": [],
   "source": [
    "len(training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3MOeHDSaf_L"
   },
   "outputs": [],
   "source": [
    "paragraphs = list(parse_annotated(read_files(random.sample(training_files, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5v56N-_Gaj2V"
   },
   "outputs": [],
   "source": [
    "relabeled = list(target_classes(default=default_label, keep=keep_labels, paragraphs=paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZo29oZ-fpcs"
   },
   "outputs": [],
   "source": [
    "print(f'len(relabeled): {len(relabeled)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WefPFrKugSLT"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'filename': [pp.filename for pp in relabeled],\n",
    "    'label': [pp.top_label().label if pp.top_label() else None for pp in relabeled],\n",
    "    'paragraph_number': [pp.paragraph_number for pp in relabeled],\n",
    "    'page_number': [pp.page_number for pp in relabeled],\n",
    "    'empirical_page_number': [pp.empirical_page_number for pp in relabeled],\n",
    "    'line_number': [pp.first_line.line_number if pp.first_line else None for pp in relabeled],\n",
    "    'body': [str(pp) for pp in relabeled]\n",
    "})\n",
    "df.label = pd.Categorical(df.label)\n",
    "df['label_code'] = df.label.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sE7cRbJ-g2eA"
   },
   "outputs": [],
   "source": [
    "df.groupby('label', observed=True).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Pg0ssXT274o"
   },
   "outputs": [],
   "source": [
    "#load in library to open terminal inside google colab\n",
    "# !pip install  --cache-dir=$cache_path --target=$nb_path colab-xterm\n",
    "# %load_ext colabxterm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C04SAEHlFs6S"
   },
   "outputs": [],
   "source": [
    "!ls /usr/local/lib/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dYgPWsg3dHB"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "import ollama\n",
    "#open up terminal\n",
    "# %xterm\n",
    "#once open, first time do:\n",
    "if not os.path.exists(\"/usr/local/lib/ollama\"):\n",
    "  !curl https://ollama.ai/install.sh | sh\n",
    "ollama_server = subprocess.Popen([\"ollama\", \"serve\"])\n",
    "time.sleep(5)  # Let the server come all the way up.\n",
    "#then start the server with ollama serve &\n",
    "#first time also will need to pull in a ollama version using ollama pull mistral\n",
    "# preferred_model = 'gemma3:12b'\n",
    "preferred_model = 'mistral'\n",
    "found = False\n",
    "for _, l in ollama.list():\n",
    "  for m in l:\n",
    "    if m.model.startswith(preferred_model):\n",
    "      found = True\n",
    "      break\n",
    "if not found:\n",
    "  print(f\"Pulling model {preferred_model}\")\n",
    "  !ollama pull $preferred_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h3pM0E51IbY"
   },
   "outputs": [],
   "source": [
    "#check here if ollama has a version and can be used, will say model you pulled\n",
    "!ollama list\n",
    "# !ollama pull mistral\n",
    "# !ollama pull gemma3:12b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TksUL6vfSpWH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSiIZYTiSouw"
   },
   "outputs": [],
   "source": [
    "    import ollama\n",
    "\n",
    "    response = ollama.generate(\n",
    "        model=preferred_model,\n",
    "        prompt=\"What is the capital of France?\"\n",
    "    )\n",
    "    print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9l04Xgx1Vmv"
   },
   "outputs": [],
   "source": [
    "#test if ollama call works\n",
    "import ollama\n",
    "response = ollama.generate(model=preferred_model, prompt='Why is the sky blue?')\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHO9EXYTiLD9"
   },
   "outputs": [],
   "source": [
    "grouped = group_paragraphs(relabeled)\n",
    "prompt = '''Please extract features, subfeatures, optional subsubfeatures, and values from the following species description.\n",
    "Translate Latin paragraphs to English before any other processing.\n",
    "Format the output as JSON.\n",
    "The top level of the JSON is feature names. The next level in is subfeature names . The optional next level in is subsubfeature names.\n",
    "The innermost layer is lists of string-valued values.\n",
    "Lists are only present at the innermost level of the JSON.\n",
    "Feature values that are comma-separated strings should be broken down into separate values.\n",
    "'''\n",
    "for i, tax in enumerate(grouped):\n",
    "  # if i > 10:\n",
    "  #   break\n",
    "  print(f'Send to LLM:\\n\\n{tax.as_row()[\"description\"]}')\n",
    "  #sample message to turn data into json format\n",
    "  response = ollama.chat(model=preferred_model, messages=[{\n",
    "     'role': 'user',\n",
    "     'content': f'{prompt}\\n\\n{tax.as_row()[\"description\"]}',},\n",
    "  ])\n",
    "  print('Result:')\n",
    "  print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tXbuuIFVo9A"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUqFSyGeWQiT"
   },
   "outputs": [],
   "source": [
    "# Initial implementation came from a Google search of\n",
    "# \"cosine similarity of two JSON objects python\".\n",
    "# We need to adjust this to handle a 3 level structure of\n",
    "# (feature, subfeature, value), where the values may be list based, and\n",
    "# may be categorical. We need to build a dictionary of known features,\n",
    "# subfeatures, and values to be used to assign numerical values.\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity_json(json1, json2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two JSON objects.\n",
    "\n",
    "    Args:\n",
    "        json1 (dict): The first JSON object.\n",
    "        json2 (dict): The second JSON object.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity between the two JSON objects.\n",
    "    \"\"\"\n",
    "    all_keys = set(json1.keys()) | set(json2.keys())\n",
    "\n",
    "    vector1 = np.array([json1.get(key, 0) for key in all_keys])\n",
    "    vector2 = np.array([json2.get(key, 0) for key in all_keys])\n",
    "\n",
    "    if not np.any(vector1) or not np.any(vector2):\n",
    "      return 0  # handle zero vector case\n",
    "\n",
    "    return np.dot(vector1, vector2) / (norm(vector1) * norm(vector2))\n",
    "\n",
    "# Example Usage\n",
    "json_string1 = '{\"a\": 1, \"b\": 2, \"c\": 3}'\n",
    "json_string2 = '{\"b\": 4, \"c\": 5, \"d\": 6}'\n",
    "\n",
    "json_object1 = json.loads(json_string1)\n",
    "json_object2 = json.loads(json_string2)\n",
    "\n",
    "similarity = cosine_similarity_json(json_object1, json_object2)\n",
    "print(f\"Cosine similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKC4Kdg9nizt"
   },
   "outputs": [],
   "source": [
    "prompt = '''Please extract features, subfeatures, optional subsubfeatures, and values from the following species description.\n",
    "Format the output as JSON.\n",
    "The top level of the JSON is feature names. The next level in is subfeature names . The optional next level in is subsubfeature names.\n",
    "The innermost layer is lists of string-valued values.\n",
    "Lists are only present at the innermost level of the JSON.\n",
    "Feature values that are comma-separated strings should be broken down into separate values.\n",
    "Translate Latin paragraphs to English.\n",
    "'''\n",
    "\n",
    "### The JSON keys are by feature, further broken down by subfeature (make sure to distinguish Type from Shape) and further broken down by optional subsubfeature, with lists of string values at the innermost layer.\n",
    "\n",
    "description = \"\"\"Fungus anamorphicus. Coloniae in substrato naturali eﬀusae, nigrae. Mycelium\n",
    "superﬁciale, ex hyphis ramosis, septatis, pallide brunneis vel brunneis, laevibus, 1.5–3\n",
    "μm crassis compositum. Conidiophora nulla vel brevis, 1–3-septata, brunnea vel\n",
    "atrobrunnea, 11–28 × 4.5–5 μm. Cellula conidiogena monoblastica, determinatae,\n",
    "solitaria, simplicia, lageniformia vel ampulliformia, brunnea vel atrobrunnea, laevia,\n",
    "4.5–6.5 × 3.5–5 μm, ad apicem 3–4.5 μm crassa et truncatae. Conidiorum secessio\n",
    "schizolytica. Conidia holoblastica, solitaria, acrogena, recta vel curvata, obclavata vel\n",
    "obclavata-rostrata, atrobrunnea vel brunnea, laevia, 13–19-distoseptata, 130–190 μm\n",
    "longa, 7–9 μm crassa, apicem versus ad 2–3 μm attenuata; cellula apicalis rotundata;\n",
    "cellula basalis cylindrica vel conico-truncata, ad basim 3.5–4.5 μm crassa; Appendicibus\n",
    "lateralibus 0–2, brunneae, septata, cylindricae, surgentibus ex cellulla e apicem 2nd vel\n",
    "3rd.\n",
    "\n",
    "Anamorphic fungi. Colonies on natural substrate eﬀuse, black. Mycelium\n",
    "superﬁcial, composed of branched, septate, pale brown to brown, smoothwalled hyphae, 1.5–3 μm thick. Conidiophores absent or short, 1–3-septate,\n",
    "brown to dark brown, 11–28 × 4.5–5 μm. Conidiogenous cells monoblastic,\n",
    "determinate, solitary, simple, lageniform or ampulliform, brown to dark brown,\n",
    "smooth, 4.5–6.5 × 3.5–5 μm, 3–4.5 μm wide at the truncate apex. Conidial\n",
    "secession schizolytic. Conidia holoblastic, solitary, acrogenous, straight or\n",
    "curved, obclavate to obclavate-rostrate, dark brown to brown, smooth, 13–19distoseptate, 130–190 μm long, 7–9 μm thick in the broadest part, tapering\n",
    "to 2–3 μm near the apex; apical cells rounded; basal cell cylindrical, truncate,\n",
    "3.5–4.5 μm wide; lateral appendages 0–2, brown, septate, cylindrical, arising\n",
    "from the 2nd or 3rd cells from the apex.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l1sRuCTfrnu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5-xzwp0ftQI"
   },
   "outputs": [],
   "source": [
    "response = ollama.chat(model=preferred_model, messages=[{\n",
    "    'role': 'user',\n",
    "    'content': f'{prompt}:\\n\\n{description}'},\n",
    "])\n",
    "print('Result:')\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ix6m6JFJCjVe"
   },
   "outputs": [],
   "source": [
    "def load_json_training(filename: str) -> List[Dict[str, Any]]:\n",
    "  retval = []\n",
    "  state = 'START'  # 'description', 'result'\n",
    "  with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = []\n",
    "    description = ''\n",
    "    for line in file:\n",
    "      if line.startswith('Send to LLM:'):\n",
    "        if state == \"result\":\n",
    "          result = ''.join(lines)\n",
    "          try:\n",
    "            result_dict = json.loads(result)\n",
    "          except json.JSONDecodeError as err:\n",
    "            print(f'Err: {err}\\n{result}')\n",
    "          retval.append({'description': description, 'result': json.dumps(result_dict)})\n",
    "        lines = []\n",
    "        state = 'description'\n",
    "      elif line.startswith('Result:'):\n",
    "        if state == \"description\":\n",
    "          description = ''.join(lines)\n",
    "          lines = []\n",
    "        state = 'result'\n",
    "      else:\n",
    "        lines.append(line)\n",
    "    if state == 'result' and len(lines) > 0:\n",
    "      result = ''.join(lines)\n",
    "      try:\n",
    "        result_dict = json.loads(result)\n",
    "      except json.JSONDecodeError as err:\n",
    "        print(f'Err: {err}\\n{result}')\n",
    "      retval.append({'description': description, 'result': json.dumps(result_dict)})\n",
    "  return retval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGu2lNoPCzSU"
   },
   "outputs": [],
   "source": [
    "json_training = load_json_training(workdir / 'json_training.txt')\n",
    "print(json_training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xzjml35RTYp"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nXJKyYXTmEn"
   },
   "outputs": [],
   "source": [
    "#log in to hugging face client to access model\n",
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODxyD4NaRsi3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjW9tvlWSCsL"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.Dataset.from_list(json_training)\n",
    "\n",
    "new_dataset = datasets.Dataset.train_test_split(dataset,int(1))\n",
    "temp_dataset = new_dataset[\"train\"]\n",
    "test_dataset = new_dataset[\"test\"]\n",
    "new_dataset2 = datasets.Dataset.train_test_split(temp_dataset,int(1))\n",
    "train_dataset = new_dataset2[\"train\"]\n",
    "val_dataset = new_dataset2[\"test\"]\n",
    "\n",
    "print(train_dataset,val_dataset,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKndKTR7bnvJ"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEQ02fyvbir7"
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQtJPsvEbyJ6"
   },
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"{prompt}:\n",
    "    description:\n",
    "    {data_point[\"description\"]}\n",
    "    result:\n",
    "    {data_point[\"result\"]}\n",
    "\"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKJ8Jpldhw1l"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = val_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_test_dataset = test_dataset.map(generate_and_tokenize_prompt)\n",
    "eval_prompt = f\"\"\"{prompt}:\n",
    "    description:\n",
    "    {train_dataset[0][\"description\"]}\n",
    "    result:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1uN7VLUf0K4"
   },
   "outputs": [],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
