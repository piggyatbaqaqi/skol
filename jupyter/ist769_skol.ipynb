{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a626f7e-4127-4227-9906-dc08dd9135ce",
   "metadata": {},
   "source": [
    "# SKOL IV: All the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0692319-632c-402b-8db5-c7f63fe7d3e0",
   "metadata": {},
   "source": [
    "Synoptic Key of Life (SKOL) is a web site and application that aims to provide easy access to all of the open taxonomic literature in Mycology. A synoptic key is a tool that helps you identify an organism making successive observations, building up a detailed description of the organism in front of you. There are many fine synoptic keys available for particular taxa, but they are all hand-built. SKOL uses AI to build the synoptic key automatically.\n",
    "\n",
    "The goal is to make it easier for advanced amateur mycologists to build technical descriptions of fungi.''\n",
    "\n",
    "## Storage needs\n",
    "\n",
    "SKOL uses a diverse set of databases to hold different artifacts.\n",
    "\n",
    "The original literature is ingested into the document database CouchDB (citation needed) along with available publication metadata. The originals are typically PDF files which are stored as attachments on the CouchDB ingestion records.\n",
    "\n",
    "Text is extracted from the ingested files, using OCR if necessary. This text is a second attachment on the ingestion record.\n",
    "\n",
    "A classifier is trained from hand-annotated articles and stored in Redis. The model has an expiration period of several weeks. The classifier then annotates each text document with labels for Nomenclature, Description, and Misc-exposition. It stores these annotated articles as attachments on the CouchDB ingestion records.\n",
    "\n",
    "Taxon names (typically species names with literature annotation) and combined with matching descriptions into Taxon records and stored in another CouchDB database. These records are the core data for SKOL.\n",
    "\n",
    "The Taxon records are processed a number of ways: sentence embedding, JSON encoding, and artificial cladograms.\n",
    "\n",
    "The sentence embedding is done with an SBERT model (citation needed) and stored as a blob in Redis. The embedding has an expiration period of 24 hours.\n",
    "\n",
    "A Mistral model (citation needed) converts each Taxon record description is converted into a hierarchy of features, subfeatures, and values. The epectation is that these data structures will eventually form the basis of pull-down menus in the SKOL user interface. These JSON structures are stored in another CouchDB database.\n",
    "\n",
    "The sentence embeddings are further processed into a single tree of Taxon reccords based on their distance from each other in the sentence embedding space. This tree is stored in a neo4j database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ae6b20-7abd-4ef6-b2cf-8d221a40d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/13 21:08:21 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 172.16.227.68 instead (on interface wlp130s0f0)\n",
      "25/12/13 21:08:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8f332d8a-9b59-4997-9c8c-1bd75d4019a7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 209ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8f332d8a-9b59-4997-9c8c-1bd75d4019a7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/5ms)\n",
      "25/12/13 21:08:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Spark context Web UI available at http://172.16.227.68:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1765660104615).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 11.0.1)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> :quit\n"
     ]
    }
   ],
   "source": [
    "bahir_package = 'org.apache.bahir:spark-sql-cloudant_2.12:2.4.0'\n",
    "!spark-shell --packages $bahir_package < /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41b1092-2658-441b-8c72-ee758d7759af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Forces synchronous execution, making it easier to track GPU operations.\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' \n",
    "\n",
    "# Enables basic CUDA debug logging.\n",
    "os.environ['CUDA_DEBUG'] = '1' \n",
    "\n",
    "# Other potentially useful variables for more detailed logging:\n",
    "# os.environ['CUDA_API_CALLS'] = '1' # Logs CUDA API calls\n",
    "os.environ['CUDA_LOG_LEVEL'] = 'DEBUG' # Or 'DEBUG', 'WARNING', etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9615b87-4962-47ca-b10f-98558713196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660108.564316 2546505 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "I0000 00:00:1765660108.595133 2546505 cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660109.252239 2546505 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import pickle\n",
    "import requests\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('error', category=UserWarning)\n",
    "\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/nvidia/cusparselt/lib'\n",
    "\n",
    "# Be sure to get version 2: https://simple-repository.app.cern.ch/project/bibtexparser/2.0.0b8/description\n",
    "import bibtexparser\n",
    "import couchdb\n",
    "import feedparser\n",
    "import fitz # PyMuPDF\n",
    "\n",
    "import pandas as pd  # TODO(piggy): Remove this dependency in favor of pure pyspark DataFrames.\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, CountVectorizer, IDF, StringIndexer, VectorAssembler, IndexToString\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name, collect_list, regexp_extract, col, udf,\n",
    "    explode, trim, row_number, min, expr, concat, lit\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, BooleanType, IntegerType, MapType, NullType,\n",
    "    StringType, StructType, StructField\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import redis\n",
    "from uuid import uuid4\n",
    "\n",
    "# Local modules\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "parent_path = Path(parent_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from couchdb_file import CouchDBFile as CDBF\n",
    "from extract_taxa_to_couchdb import (\n",
    "    TaxonExtractor,\n",
    "    generate_taxon_doc_id,\n",
    "    extract_taxa_from_partition,\n",
    "    convert_taxa_to_rows\n",
    ")\n",
    "from fileobj import FileObject\n",
    "from finder import parse_annotated, remove_interstitials\n",
    "import line\n",
    "from line import Line\n",
    "\n",
    "# Import SKOL classifiers\n",
    "from skol_classifier.classifier_v2 import SkolClassifierV2 as SC\n",
    "from skol_classifier.couchdb_io import CouchDBConnection as CDBC\n",
    "from skol_classifier.model import SkolModel\n",
    "from skol_classifier.output_formatters import CouchDBOutputWriter as CDBOW, YeddaFormatter\n",
    "from skol_classifier.preprocessing import SuffixTransformer, ParagraphExtractor\n",
    "from skol_classifier.utils import get_file_list\n",
    "\n",
    "from taxon import group_paragraphs, Taxon\n",
    "\n",
    "from taxa_json_translator import TaxaJSONTranslator as TJT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd14626-a4bd-4684-81ef-ec5682a1d9aa",
   "metadata": {},
   "source": [
    "## Important constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc77721a-05a7-48f0-8780-fa71734ff01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "couchdb_host = \"127.0.0.1:5984\" # e.g., \"ACCOUNT.cloudant.com\" or \"localhost\"\n",
    "couchdb_username = \"admin\"\n",
    "couchdb_password = \"SU2orange!\"\n",
    "ingest_db_name = \"skol_dev\"  # Development ingestion database\n",
    "taxon_db_name = \"skol_taxa_dev\"  # Development Taxa database\n",
    "json_taxon_db_name = \"skol_taxa_full_dev\"  # Development Taxa database with JSON translations\n",
    "\n",
    "redis_host = 'localhost'\n",
    "redis_port = 6379\n",
    "\n",
    "embedding_name = 'skol:embedding:v1.0'\n",
    "embedding_expire = 60 * 60 * 24 * 2  # Expire after 2 days.\n",
    "classifier_model_name = \"skol:classifier:model:rnn-v1.0\"\n",
    "classifier_model_expire = 60 * 60 * 24  * 2 # Expire after 2 days.\n",
    "\n",
    "neo4j_uri = \"bolt://localhost:7687\"\n",
    "\n",
    "couchdb_url = f'http://{couchdb_host}'\n",
    "\n",
    "cores = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe0cba-ccae-4b9f-aff7-adccf30df427",
   "metadata": {},
   "source": [
    "## robots.txt\n",
    "\n",
    "We want to be a well-behaved web scraper. Respect `robots.txt`, a standardized file that tells us what parts of a web site a scraper is allowed to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca180b6-224c-497b-b5c2-7ca3673d46e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: cloudant.protocol\n",
      "Warning: Ignoring non-Spark config property: cloudant.password\n",
      "Warning: Ignoring non-Spark config property: cloudant.host\n",
      "Warning: Ignoring non-Spark config property: cloudant.username\n",
      "25/12/13 21:08:30 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 172.16.227.68 instead (on interface wlp130s0f0)\n",
      "25/12/13 21:08:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-261815bf-255d-435b-9ee6-c321566b7a91;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 219ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-261815bf-255d-435b-9ee6-c321566b7a91\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/5ms)\n",
      "25/12/13 21:08:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "user_agent = \"synoptickeyof.life\"\n",
    "\n",
    "ingenta_rp = RobotFileParser()\n",
    "ingenta_rp.set_url(\"https://www.ingentaconnect.com/robots.txt\")\n",
    "ingenta_rp.read() # Reads and parses the robots.txt file from the URL\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CouchDB Spark SQL Example in Python using dataframes\") \\\n",
    "    .master(f\"local[{cores}]\") \\\n",
    "    .config(\"cloudant.protocol\", \"http\") \\\n",
    "    .config(\"cloudant.host\", couchdb_host) \\\n",
    "    .config(\"cloudant.username\", couchdb_username) \\\n",
    "    .config(\"cloudant.password\", couchdb_password) \\\n",
    "    .config(\"spark.jars.packages\", bahir_package) \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\",\n",
    "            \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.security.action=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.util.calendar=ALL-UNNAMED\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\",\n",
    "            \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.security.action=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.util.calendar=ALL-UNNAMED\") \\\n",
    "    .config(\"spark.submit.pyFiles\",\n",
    "            f'{parent_path / \"line.py\"},{parent_path / \"fileobj.py\"},'\n",
    "            f'{parent_path / \"couchdb_file.py\"},{parent_path / \"finder.py\"},'\n",
    "            f'{parent_path / \"taxon.py\"},{parent_path / \"paragraph.py\"},'\n",
    "            f'{parent_path / \"label.py\"},{parent_path / \"file.py\"},'\n",
    "            f'{parent_path / \"extract_taxa_to_couchdb.py\"}'\n",
    "           ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!\n",
    "\n",
    "couch = couchdb.Server(couchdb_url)\n",
    "couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "\n",
    "if ingest_db_name not in couch:\n",
    "    db = couch.create(ingest_db_name)\n",
    "else:\n",
    "    db = couch[ingest_db_name]\n",
    "\n",
    "# Connect to Redis\n",
    "redis_client = redis.Redis(\n",
    "    host=redis_host,\n",
    "    port=redis_port,\n",
    "    db=0,\n",
    "    decode_responses=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e7f93-bb66-4922-b3f9-c9429bc304ac",
   "metadata": {},
   "source": [
    "## The Data Sources\n",
    "\n",
    "The goal is to collect all the open access taxonomic literature in Mycology. Most of the sources below mainly cover macro-fungi and slime molds.\n",
    "\n",
    "### Ingested Data Sources\n",
    "\n",
    "* [Mycotaxon at Ingenta Connect](https://www.ingentaconnect.com/content/mtax/mt)\n",
    "* [Studies in Mycology at Ingenta Connect](https://www.studiesinmycology.org/)\n",
    "\n",
    "### Source of many older public domain and open access works\n",
    "\n",
    "Mycoweb includes scans of many older works in mycology. I have local copies but need to write ingesters for them.\n",
    "\n",
    "* [Mycoweb](https://mykoweb.com/)\n",
    "\n",
    "### Journals in hand\n",
    "\n",
    "These are journals we've collected over the years. The initial annotated issues are from early years of Mycotaxon. We still need to write ingesters for all of these.\n",
    "\n",
    "* Mycologia (back issues)\n",
    "* [Mycologia at Taylor and Francis](https://www.tandfonline.com/journals/umyc20)\n",
    "  Mycologia is the main journal of the Mycological Society of America. It is a mix of open access and traditional access articles. The connector for this journal will need to identify the open access articles.\n",
    "* Persoonia (all issues)\n",
    "  Persoonia is no longer published.\n",
    "* Mycotaxon (back issues)\n",
    "  Mycotaxon is no longer published.\n",
    "\n",
    "### Journals that need connectors\n",
    "\n",
    "These are journals we're aware that include open access articles.\n",
    "\n",
    "* [Amanitaceae.org](http://www.tullabs.com/amanita/?home)\n",
    "* [Mycosphere](https://mycosphere.org/)\n",
    "* [Mycoscience](https://mycoscience.org/)\n",
    "* [Journal of Fungi](https://www.mdpi.com/journal/jof)\n",
    "* [Mycology](https://www.tandfonline.com/journals/tmyc20)\n",
    "* [Open Access Journal of Mycology & Mycological Sciences](https://www.medwinpublishers.com/OAJMMS/)\n",
    "* [Mycokeys](https://mycokeys.pensoft.net/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a4213-adb5-49e5-b973-570a75cc2cce",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Each journal or other data source gets an ingester that puts PDFs into our document store along with any metadata we can collect. The metadata is sufficient to create citations for each issue, book, or article. If bibtex citations are available we prefer to store these verbatim.\n",
    "\n",
    "### Ingenta RSS ingestion\n",
    "\n",
    "Ingenta Connect is an electronic publisher that holds two Mycology journals. New articles are available via RSS (Really Simple Syndication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d303cc7-f01c-4e15-87ff-6c259d010591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_bibtex(\n",
    "        db: couchdb.Database,\n",
    "        content: bytes,\n",
    "        bibtex_link: str,\n",
    "        meta: Dict[str, Any],\n",
    "        rp\n",
    "        ) -> None:\n",
    "    \"\"\"Load documents referenced in an Ingenta BibTeX database.\"\"\"\n",
    "    bib_database = bibtexparser.parse_string(content)\n",
    "\n",
    "    bibtex_data = {\n",
    "        'link': bibtex_link,\n",
    "        'bibtex': bibtexparser.write_string(bib_database),\n",
    "    }\n",
    "\n",
    "    for bib_entry in bib_database.entries:\n",
    "        doc = {\n",
    "            '_id': uuid4().hex,\n",
    "            'meta': meta,\n",
    "            'pdf_url': f\"{bib_entry['url']}?crawler=true\",\n",
    "        }\n",
    "\n",
    "        # Do not fetch if we already have an entry.\n",
    "        selector = {'selector': {'pdf_url': doc['pdf_url']}}\n",
    "        found = False\n",
    "        for e in db.find(selector):\n",
    "            found = True\n",
    "        if found:\n",
    "            print(f\"Skipping {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        if not rp.can_fetch(user_agent, doc['pdf_url']):\n",
    "            # TODO(piggy): We should probably log blocked URLs.\n",
    "            print(f\"Robot permission denied {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Adding {doc['pdf_url']}\")\n",
    "        for k in bib_entry.fields_dict.keys():\n",
    "            doc[k] = bib_entry[k]\n",
    "\n",
    "        doc_id, doc_rev = db.save(doc)\n",
    "        with requests.get(doc['pdf_url'], stream=False) as pdf_f:\n",
    "            pdf_f.raise_for_status()\n",
    "            pdf_doc = pdf_f.content\n",
    "\n",
    "        attachment_filename = 'article.pdf'\n",
    "        attachment_content_type = 'application/pdf'\n",
    "        attachment_file = BytesIO(pdf_doc)\n",
    "\n",
    "        db.put_attachment(doc, attachment_file, attachment_filename, attachment_content_type)\n",
    "\n",
    "        print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b323f98-5bd8-4106-b79c-2ff9ac1ac74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_ingenta(\n",
    "        db: couchdb.Database,\n",
    "        rss_url: str,\n",
    "        rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest documents from an Ingenta RSS feed.\"\"\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    feed_meta = {\n",
    "        'url': rss_url,_utils/#/_al\n",
    "        'title': feed.feed.title,\n",
    "        'link': feed.feed.link,\n",
    "        'description': feed.feed.description,\n",
    "    }\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        entry_meta = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "        }\n",
    "        if hasattr(entry, 'summary'):\n",
    "            entry_meta['summary'] = entry.summary\n",
    "        if hasattr(entry, 'description'):\n",
    "            entry_meta['description'] = entry.description\n",
    "\n",
    "        bibtex_link = f'{entry.link}?format=bib'\n",
    "        print(f\"bibtex_link: {bibtex_link}\")\n",
    "\n",
    "        if not rp.can_fetch(user_agent, bibtex_link):\n",
    "            print(f\"Robot permission denied {bibtex_link}\")\n",
    "            continue\n",
    "\n",
    "        with requests.get(bibtex_link, stream=False) as bibtex_f:\n",
    "            bibtex_f.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            ingest_from_bibtex(\n",
    "                db=db,\n",
    "                content=bibtex_f.content\\\n",
    "                    .replace(b\"\\\"\\nparent\", b\"\\\",\\nparent\")\\\n",
    "                    .replace(b\"\\n\", b\"\"),\n",
    "                bibtex_link=bibtex_link,\n",
    "                meta={\n",
    "                    'feed': feed_meta,\n",
    "                    'entry': entry_meta,\n",
    "                },\n",
    "                rp=rp\n",
    "            )\n",
    "        print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234f9cc5-9b7e-4559-ad10-f24ad269e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_local_bibtex(\n",
    "    db: couchdb.Database,\n",
    "    root: Path,\n",
    "    rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest from a local directory with Ingenta bibtext files in it.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if not filename.endswith('format=bib'):\n",
    "                continue\n",
    "            full_filepath = os.path.join(dirpath, filename)\n",
    "            bibtex_link = f\"https://www.ingentaconnect.com/{full_filepath[len(str(root)):]}\"\n",
    "            with open(full_filepath) as f:\n",
    "                # Paper over a syntax problem in Ingenta Connect Bibtex files.\n",
    "                content = f.read()\\\n",
    "                    .replace(\"\\\"\\nparent\", \"\\\",\\nparent\")\\\n",
    "                    .replace(\"\\n\", \"\")\n",
    "                ingest_from_bibtex(db, content, bibtex_link, meta={}, rp=rp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58829ec0-93f7-44dd-87bc-511ab2d238d0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mycotaxon\n",
    "\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/mtax/mt?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40602cc3-e231-497d-adc7-d6d4471514cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Studies in Mycology\n",
    "\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/wfbi/sim?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "651f82ea-b3b8-4f0d-adf0-f5550d7ed6d5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ingest_from_local_bibtex(\n",
    "    db=db,\n",
    "    root=Path(\"/data/skol/www/www.ingentaconnect.com\"),\n",
    "    rp=ingenta_rp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf05ad-d2bf-4716-b253-07455cfefcd0",
   "metadata": {},
   "source": [
    "### Text extraction\n",
    "\n",
    "We extract the text, optionally with OCR. Add as an additional attachment on the source record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5e9725-0c3a-4c41-8ebf-2929c4dc4334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\n",
    "    format=\"org.apache.bahir.cloudant\",\n",
    "    database=ingest_db_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e75c64e-95cb-478a-9a85-46d1297e4225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _id: string, _rev: string, abstract: string, author: string, doi: string, eissn: string, issn: string, itemtype: string, journal: string, number: string, pages: string, parent_itemid: string, pdf_url: string, publication date: string, publishercode: string, title: string, url: string, volume: string, year: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9befa546-a895-4a13-8744-d512587d9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Type: text/html; charset=UTF-8\n",
    "\n",
    "def pdf_to_text(pdf_contents: bytes) -> bytes:\n",
    "    doc = fitz.open(stream=BytesIO(pdf_contents), filetype=\"pdf\")\n",
    "\n",
    "    full_text = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        # Possibly perform OCR on the page\n",
    "        text = page.get_text(\"text\", flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_DEHYPHENATE)\n",
    "        # full_text += f\"\\n--- PDF Page {page_num+1} ---\\n\"  # TODO(piggy): Introduce PDF page tracking in line-by-line and paragraph parsers.\n",
    "        full_text += text\n",
    "\n",
    "    return full_text.encode(\"utf-8\")\n",
    "\n",
    "def add_text_to_partition(iterator) -> None:\n",
    "    couch = couchdb.Server(couchdb_url)\n",
    "    couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "    local_db = couch[ingest_db_name]\n",
    "    for row in iterator:\n",
    "        if not row:\n",
    "            continue\n",
    "        if not row._attachments:\n",
    "            continue\n",
    "        row_dict = row.asDict()\n",
    "        attachment_dict = row._attachments.asDict()\n",
    "        for pdf_filename in attachment_dict:\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            if pdf_path.suffix != '.pdf':\n",
    "                continue\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            txt_path_str = pdf_path.stem + '.txt'\n",
    "            # if txt_path_str in attachment_dict:\n",
    "            #     # TODO(piggy): Recalculate text if text is terrible. Too much noise vocabulary?\n",
    "            #     print(f\"Already have text for {row.pdf_url}\")\n",
    "            #     continue\n",
    "            print(f\"{row._id}, {row.pdf_url}\")\n",
    "            pdf_file = local_db.get_attachment(row._id, str(pdf_path)).read()\n",
    "            txt_file = pdf_to_text(pdf_file)\n",
    "            attachment_content_type = 'text/simple; charset=UTF-8'\n",
    "            attachment_file = BytesIO(txt_file)\n",
    "            local_db.put_attachment(row_dict, attachment_file, txt_path_str, attachment_content_type)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74366376-ec05-474f-b172-216093c62865",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df.select(\"*\").foreachPartition(add_text_to_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11805c62-ae42-4959-9673-1e09261a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to skol_classifier.CouchDBConnection.\n",
    "\n",
    "class CouchDBConnection(CDBC):\n",
    "    \"\"\"\n",
    "    Manages CouchDB connection and provides I/O operations.\n",
    "\n",
    "    This class encapsulates connection parameters and provides an idempotent\n",
    "    connection method that can be safely called multiple times.\n",
    "    \"\"\"\n",
    "    # Shared schema definitions (DRY principle)\n",
    "    LOAD_SCHEMA = StructType([\n",
    "        StructField(\"doc_id\", StringType(), False),\n",
    "        StructField(\"human_url\", StringType(), False),\n",
    "        StructField(\"attachment_name\", StringType(), False),\n",
    "        StructField(\"value\", StringType(), False),\n",
    "    ])\n",
    "\n",
    "    SAVE_SCHEMA = StructType([\n",
    "        StructField(\"doc_id\", StringType(), False),\n",
    "        StructField(\"attachment_name\", StringType(), False),\n",
    "        StructField(\"success\", BooleanType(), False),\n",
    "    ])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize CouchDB connection parameters.\n",
    "\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL (e.g., \"http://localhost:5984\")\n",
    "            database: Database name\n",
    "            username: Optional username for authentication\n",
    "            password: Optional password for authentication\n",
    "        \"\"\"\n",
    "        self.couchdb_url = couchdb_url\n",
    "        self.database = database\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self._server = None\n",
    "        self._db = None\n",
    "\n",
    "    def _connect(self):\n",
    "        \"\"\"\n",
    "        Idempotent connection method that returns a CouchDB server object.\n",
    "\n",
    "        This method can be called multiple times safely - it will only create\n",
    "        a connection if one doesn't already exist.\n",
    "\n",
    "        Returns:\n",
    "            couchdb.Server: Connected CouchDB server object\n",
    "        \"\"\"\n",
    "        if self._server is None:\n",
    "            self._server = couchdb.Server(self.couchdb_url)\n",
    "            if self.username and self.password:\n",
    "                self._server.resource.credentials = (self.username, self.password)\n",
    "\n",
    "        if self._db is None:\n",
    "            self._db = self._server[self.database]\n",
    "\n",
    "        return self._server\n",
    "\n",
    "    @property\n",
    "    def db(self):\n",
    "        \"\"\"Get the database object, connecting if necessary.\"\"\"\n",
    "        if self._db is None:\n",
    "            self._connect()\n",
    "        return self._db\n",
    "\n",
    "    def get_all_doc_ids(self, pattern: str = \"*\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of document IDs matching the pattern from CouchDB.\n",
    "\n",
    "        Args:\n",
    "            pattern: Pattern for document IDs (e.g., \"taxon_*\", \"*\")\n",
    "                    - \"*\" matches all non-design documents\n",
    "                    - \"prefix*\" matches documents starting with prefix\n",
    "                    - \"exact\" matches exactly\n",
    "\n",
    "        Returns:\n",
    "            List of matching document IDs\n",
    "        \"\"\"\n",
    "        db = self.db\n",
    "\n",
    "        # Get all document IDs (excluding design documents)\n",
    "        all_doc_ids = [doc_id for doc_id in list(db) if not doc_id.startswith('_design/')]\n",
    "\n",
    "        # Filter by pattern\n",
    "        if pattern == \"*\":\n",
    "            # Return all non-design documents\n",
    "            return all_doc_ids\n",
    "        elif pattern.endswith('*'):\n",
    "            # Prefix matching\n",
    "            prefix = pattern[:-1]\n",
    "            return [doc_id for doc_id in all_doc_ids if doc_id.startswith(prefix)]\n",
    "        else:\n",
    "            # Exact match\n",
    "            return [doc_id for doc_id in all_doc_ids if doc_id == pattern]\n",
    "\n",
    "    def get_document_list(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Get a list of documents with text attachments from CouchDB.\n",
    "\n",
    "        This only fetches document metadata (not content) to create a DataFrame\n",
    "        that can be processed in parallel. Creates ONE ROW per attachment, so if\n",
    "        a document has multiple attachments matching the pattern, it will have\n",
    "        multiple rows in the resulting DataFrame.\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names (e.g., \"*.txt\")\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, attachment_name\n",
    "            One row per (doc_id, attachment_name) pair\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB (driver only)\n",
    "        db = self.db\n",
    "\n",
    "        # Get all documents with attachments matching pattern\n",
    "        doc_list = []\n",
    "        for doc_id in db:\n",
    "            try:\n",
    "                doc = db[doc_id]\n",
    "                attachments = doc.get('_attachments', {})\n",
    "\n",
    "                # Loop through ALL attachments in the document\n",
    "                for att_name in attachments.keys():\n",
    "                    # Check if attachment matches pattern\n",
    "                    # Pattern matching: \"*.txt\" matches files ending with .txt\n",
    "                    if pattern == \"*.txt\" and att_name.endswith('.txt'):\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern == \"*.*\" or pattern == \"*\":\n",
    "                        # Match all attachments\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern.startswith(\"*.\") and att_name.endswith(pattern[1:]):\n",
    "                        # Generic pattern matching for *.ext\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "            except Exception:\n",
    "                # Skip documents we can't read\n",
    "                continue\n",
    "\n",
    "        # Create DataFrame with document IDs and attachment names\n",
    "        schema = StructType([\n",
    "            StructField(\"doc_id\", StringType(), False),\n",
    "            StructField(\"attachment_name\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        return spark.createDataFrame(doc_list, schema)\n",
    "\n",
    "    def fetch_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row]\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Fetch CouchDB attachments for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id and attachment_name\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, human_url, attachment_name, and value (content).\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document has multiple .txt attachments, there will be multiple rows\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Get the specific attachment for this row\n",
    "                    if row.attachment_name in doc.get('_attachments', {}):\n",
    "                        attachment = db.get_attachment(doc, row.attachment_name)\n",
    "                        if attachment:\n",
    "                            content = attachment.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "                            yield Row(\n",
    "                                doc_id=row.doc_id,\n",
    "                                human_url=doc.get('url', 'missing_human_url'),\n",
    "                                attachment_name=row.attachment_name,\n",
    "                                value=content\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    # Log error but continue processing\n",
    "                    print(f\"Error fetching {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            return\n",
    "\n",
    "    def save_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row],\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Save annotated content to CouchDB for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id, attachment_name, final_aggregated_pg\n",
    "                       and optionally human_url\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, attachment_name, and success status.\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document had multiple .txt files, we save multiple .ann files\n",
    "            for row in partition:\n",
    "                success = False\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Update human_url field if provided\n",
    "                    if hasattr(row, 'human_url') and row.human_url:\n",
    "                        doc['url'] = row.human_url\n",
    "                        db.save(doc)\n",
    "                        # Reload doc to get updated _rev\n",
    "                        doc = db[row.doc_id]\n",
    "\n",
    "                    # Create new attachment name by appending suffix\n",
    "                    # e.g., \"article.txt\" becomes \"article.txt.ann\"\n",
    "                    new_attachment_name = f\"{row.attachment_name}{suffix}\"\n",
    "\n",
    "                    # Save the annotated content as a new attachment\n",
    "                    db.put_attachment(\n",
    "                        doc,\n",
    "                        row.final_aggregated_pg.encode('utf-8'),\n",
    "                        filename=new_attachment_name,\n",
    "                        content_type='text/plain'\n",
    "                    )\n",
    "\n",
    "                    success = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=success\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            # Yield failures for all rows\n",
    "            for row in partition:\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n",
    "    def load_distributed(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Load text attachments from CouchDB using foreachPartition.\n",
    "\n",
    "        This function:\n",
    "        1. Gets list of documents (on driver)\n",
    "        2. Creates a DataFrame with doc IDs\n",
    "        3. Uses mapPartitions to fetch content efficiently (one connection per partition)\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, attachment_name, and value.\n",
    "        \"\"\"\n",
    "        # Get document list\n",
    "        doc_df = self.get_document_list(spark, pattern)\n",
    "\n",
    "        # Use mapPartitions for efficient batch fetching\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def fetch_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.fetch_partition(partition)\n",
    "\n",
    "        # Apply mapPartitions using shared schema\n",
    "        result_df = doc_df.rdd.mapPartitions(fetch_partition).toDF(self.LOAD_SCHEMA)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def save_distributed(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Save annotated predictions to CouchDB using foreachPartition.\n",
    "\n",
    "        This function uses mapPartitions where each partition creates a single\n",
    "        CouchDB connection and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with columns: doc_id, attachment_name, final_aggregated_pg\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with doc_id, attachment_name, and success columns\n",
    "        \"\"\"\n",
    "        # Use mapPartitions for efficient batch saving\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def save_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.save_partition(partition, suffix)\n",
    "\n",
    "        # Apply mapPartitions using shared schema\n",
    "        result_df = df.rdd.mapPartitions(save_partition).toDF(self.SAVE_SCHEMA)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def process_partition_with_func(\n",
    "        self,\n",
    "        partition: Iterator[Row],\n",
    "        processor_func,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Generic function to read, process, and save in one partition operation.\n",
    "\n",
    "        This allows custom processing logic while maintaining single connection per partition.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows\n",
    "            processor_func: Function to process content (takes content string, returns processed string)\n",
    "            suffix: Suffix for output attachment\n",
    "\n",
    "        Yields:\n",
    "            Rows with processing results, including success status for logging.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Fetch\n",
    "                    if row.attachment_name in doc.get('_attachments', {}):\n",
    "                        attachment = db.get_attachment(doc, row.attachment_name)\n",
    "                        if attachment:\n",
    "                            content = attachment.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "                            # Process\n",
    "                            processed = processor_func(content)\n",
    "\n",
    "                            # Save\n",
    "                            new_attachment_name = f\"{row.attachment_name}{suffix}\"\n",
    "                            db.put_attachment(\n",
    "                                doc,\n",
    "                                processed.encode('utf-8'),\n",
    "                                filename=new_attachment_name,\n",
    "                                content_type='text/plain'\n",
    "                            )\n",
    "\n",
    "                            yield Row(\n",
    "                                doc_id=row.doc_id,\n",
    "                                attachment_name=row.attachment_name,\n",
    "                                success=True\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            for row in partition:\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "689d4df6-59c4-4ac4-9f9a-e849bd3b21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDBOutputWriter(CDBOW):\n",
    "    \"\"\"\n",
    "    Writes predictions back to CouchDB as attachments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: str,\n",
    "        password: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the writer.\n",
    "\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL\n",
    "            database: Database name\n",
    "            username: CouchDB username\n",
    "            password: CouchDB password\n",
    "        \"\"\"\n",
    "        self.conn = CouchDBConnection(\n",
    "            couchdb_url=couchdb_url,\n",
    "            database=database,\n",
    "            username=username,\n",
    "            password=password\n",
    "        )\n",
    "\n",
    "    def save_annotated(\n",
    "        self,\n",
    "        predictions: DataFrame,\n",
    "        suffix: str = \".ann\",\n",
    "        coalesce_labels: bool = False,\n",
    "        line_level: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save predictions to CouchDB as attachments.\n",
    "\n",
    "        Args:\n",
    "            predictions: DataFrame with predictions\n",
    "            suffix: Suffix for attachment names\n",
    "            coalesce_labels: Whether to coalesce consecutive labels\n",
    "            line_level: Whether data is line-level\n",
    "        \"\"\"\n",
    "        # Format predictions\n",
    "        if \"annotated_value\" not in predictions.columns:\n",
    "            predictions = YeddaFormatter.format_predictions(predictions)\n",
    "\n",
    "        # Coalesce if requested\n",
    "        if coalesce_labels and line_level:\n",
    "            predictions = YeddaFormatter.coalesce_consecutive_labels(\n",
    "                predictions, line_level=True\n",
    "            )\n",
    "            # For coalesced output, we have coalesced_annotations column\n",
    "            # We need to join them into final_aggregated_pg\n",
    "            from pyspark.sql.functions import expr\n",
    "            predictions = predictions.withColumn(\n",
    "                \"final_aggregated_pg\",\n",
    "                expr(\"array_join(coalesced_annotations, '\\n')\")\n",
    "            )\n",
    "        else:\n",
    "            # Aggregate annotated values by document\n",
    "            groupby_col = \"doc_id\" if \"doc_id\" in predictions.columns else \"filename\"\n",
    "            attachment_col = \"attachment_name\" if \"attachment_name\" in predictions.columns else \"filename\"\n",
    "\n",
    "            # Check if we have line_number for ordering\n",
    "            if \"line_number\" in predictions.columns:\n",
    "                from pyspark.sql.functions import expr, first\n",
    "                # Preserve human_url if it exists\n",
    "                if \"human_url\" in predictions.columns:\n",
    "                    predictions = (\n",
    "                        predictions.groupBy(groupby_col, attachment_col)\n",
    "                        .agg(\n",
    "                            expr(\"sort_array(collect_list(struct(line_number, annotated_value)))\").alias(\"sorted_list\"),\n",
    "                            first(\"human_url\").alias(\"human_url\")\n",
    "                        )\n",
    "                        .withColumn(\"annotated_value_ordered\", expr(\"transform(sorted_list, x -> x.annotated_value)\"))\n",
    "                        .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotated_value_ordered, '\\n')\"))\n",
    "                        .select(groupby_col, \"human_url\", attachment_col, \"final_aggregated_pg\")\n",
    "                    )\n",
    "                else:\n",
    "                    predictions = (\n",
    "                        predictions.groupBy(groupby_col, attachment_col)\n",
    "                        .agg(\n",
    "                            expr(\"sort_array(collect_list(struct(line_number, annotated_value)))\").alias(\"sorted_list\")\n",
    "                        )\n",
    "                        .withColumn(\"annotated_value_ordered\", expr(\"transform(sorted_list, x -> x.annotated_value)\"))\n",
    "                        .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotated_value_ordered, '\\n')\"))\n",
    "                        .select(groupby_col, attachment_col, \"final_aggregated_pg\")\n",
    "                    )\n",
    "            else:\n",
    "                from pyspark.sql.functions import collect_list, expr, first\n",
    "                # Preserve human_url if it exists\n",
    "                if \"human_url\" in predictions.columns:\n",
    "                    predictions = (\n",
    "                        predictions.groupBy(groupby_col, attachment_col)\n",
    "                        .agg(\n",
    "                            collect_list(\"annotated_value\").alias(\"annotations\"),\n",
    "                            first(\"human_url\").alias(\"human_url\")\n",
    "                        )\n",
    "                        .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotations, '\\n')\"))\n",
    "                        .select(groupby_col, \"human_url\", attachment_col, \"final_aggregated_pg\")\n",
    "                    )\n",
    "                else:\n",
    "                    predictions = (\n",
    "                        predictions.groupBy(groupby_col, attachment_col)\n",
    "                        .agg(\n",
    "                            collect_list(\"annotated_value\").alias(\"annotations\")\n",
    "                        )\n",
    "                        .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotations, '\\n')\"))\n",
    "                        .select(groupby_col, attachment_col, \"final_aggregated_pg\")\n",
    "                    )\n",
    "\n",
    "            # Rename columns for CouchDB save\n",
    "            if groupby_col != \"doc_id\":\n",
    "                predictions = predictions.withColumnRenamed(groupby_col, \"doc_id\")\n",
    "            if attachment_col != \"attachment_name\":\n",
    "                predictions = predictions.withColumnRenamed(attachment_col, \"attachment_name\")\n",
    "\n",
    "        # Use CouchDB connection to save\n",
    "        self.conn.save_distributed(predictions, suffix=suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a0fa58-e693-4978-b33e-67010e066948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main classifier module for SKOL text classification\n",
    "\"\"\"\n",
    "class SkolClassifierV2(SC):\n",
    "    \"\"\"\n",
    "    Text classifier for taxonomic literature.\n",
    "\n",
    "    This version only includes the redis and couchdb I/O methods.\n",
    "    All other methods are in SC.\n",
    "\n",
    "    Supports multiple classification models (Logistic Regression, Random Forest, RNN)\n",
    "    and feature types (word TF-IDF, suffix TF-IDF, combined).\n",
    "    \"\"\"\n",
    "\n",
    "    def _load_raw_from_couchdb(self) -> DataFrame:\n",
    "        \"\"\"Load raw text from CouchDB.\"\"\"\n",
    "        conn = CouchDBConnection(\n",
    "            self.couchdb_url,\n",
    "            self.couchdb_database,\n",
    "            self.couchdb_username,\n",
    "            self.couchdb_password\n",
    "        )\n",
    "\n",
    "        df = conn.load_distributed(self.spark, self.couchdb_pattern)\n",
    "\n",
    "        # Split into lines if line_level mode\n",
    "        if self.line_level:\n",
    "            from pyspark.sql.functions import explode, split, col, trim, row_number, lit\n",
    "            from pyspark.sql.window import Window\n",
    "\n",
    "            df = df.withColumn(\"value\", explode(split(col(\"value\"), \"\\\\n\")))\n",
    "            df = df.filter(trim(col(\"value\")) != \"\")\n",
    "\n",
    "            # Add line numbers\n",
    "            window_spec = Window.partitionBy(\"doc_id\", \"attachment_name\").orderBy(lit(1))\n",
    "            df = df.withColumn(\"line_number\", row_number().over(window_spec) - 1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _load_annotated_from_couchdb(self) -> DataFrame:\n",
    "        \"\"\"Load annotated data from CouchDB.\"\"\"\n",
    "        # Load raw annotations from CouchDB\n",
    "        conn = CouchDBConnection(\n",
    "            self.couchdb_url,\n",
    "            self.couchdb_database,\n",
    "            self.couchdb_username,\n",
    "            self.couchdb_password\n",
    "        )\n",
    "\n",
    "        # Look for .ann files for training\n",
    "        pattern = self.couchdb_pattern\n",
    "        if not pattern.endswith('.ann'):\n",
    "            pattern = pattern.replace('.txt', '.txt.ann')\n",
    "\n",
    "        df = conn.load_distributed(self.spark, pattern)\n",
    "\n",
    "        # Parse annotations\n",
    "        from .preprocessing import AnnotatedTextParser\n",
    "\n",
    "        parser = AnnotatedTextParser(line_level=self.line_level)\n",
    "        return parser.parse(df)\n",
    "\n",
    "    def _save_to_couchdb(self, predictions: DataFrame) -> None:\n",
    "        \"\"\"Save predictions to CouchDB.\"\"\"\n",
    "        from skol_classifier.output_formatters import CouchDBOutputWriter\n",
    "\n",
    "        writer = CouchDBOutputWriter(\n",
    "            couchdb_url=self.couchdb_url,\n",
    "            database=self.couchdb_database,\n",
    "            username=self.couchdb_username,\n",
    "            password=self.couchdb_password\n",
    "        )\n",
    "\n",
    "        writer.save_annotated(\n",
    "            predictions,\n",
    "            suffix=self.output_couchdb_suffix,\n",
    "            coalesce_labels=self.coalesce_labels,\n",
    "            line_level=self.line_level\n",
    "        )\n",
    "\n",
    "    def _save_to_couchdb(self, predictions: DataFrame) -> None:\n",
    "        \"\"\"Save predictions to CouchDB.\"\"\"\n",
    "        from skol_classifier.output_formatters import CouchDBOutputWriter\n",
    "\n",
    "        writer = CouchDBOutputWriter(\n",
    "            couchdb_url=self.couchdb_url,\n",
    "            database=self.couchdb_database,\n",
    "            username=self.couchdb_username,\n",
    "            password=self.couchdb_password\n",
    "        )\n",
    "\n",
    "        writer.save_annotated(\n",
    "            predictions,\n",
    "            suffix=self.output_couchdb_suffix,\n",
    "            coalesce_labels=self.coalesce_labels,\n",
    "            line_level=self.line_level\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab2d58-0ea7-4288-889c-b1bf9c360743",
   "metadata": {},
   "source": [
    "## Build a classifier to identify paragraph types.\n",
    "\n",
    "We save this to redis so that we don't need to train the model every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "m4m45o9n97i",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping generation of model skol:classifier:model:rnn-v1.0.\n"
     ]
    }
   ],
   "source": [
    "# Train classifier on annotated data and save to Redis using SkolClassifierV2\n",
    "model_config = {\n",
    "    \"name\": \"RNN BiLSTM (line-level, advanced config)\",\n",
    "    \"model_type\": \"rnn\",\n",
    "    \"use_suffixes\": True,\n",
    "    \"line_level\": True,\n",
    "    \"input_size\": 1000,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": 3,\n",
    "    \"dropout\": 0.3,\n",
    "    \"window_size\": 20,\n",
    "    # \"prediction_stride\": 15,  # 25% overlap\n",
    "    \"prediction_stride\": 20,  # 0 overlap\n",
    "    \"prediction_batch_size\": 32,\n",
    "    # \"batch_size\": 16,  # 442MiB footprint\n",
    "    # \"batch_size\": 128,  # 570MiB footprint\n",
    "    # \"batch_size\": 512,  # 1370MiB footprint\n",
    "    # \"batch_size\": 1024,  # 2394MiB footprint\n",
    "    # \"batch_size\": 2048,  # 4442MiB footprint, 5s per step\n",
    "    # \"batch_size\": 4096,  #  4442MiB footprint, 8s-11s per step\n",
    "    # \"batch_size\": 8192,  # 8538MiB footprint, 36s per step\n",
    "    \"batch_size\": 16384,  # 16730MiB footprint, (3s) 38s-40s per step\n",
    "\n",
    "    # \"epochs\": 4,\n",
    "    # ==================================================\n",
    "    # RNN Model Evaluation Statistics (Line-Level)\n",
    "    # ==================================================\n",
    "    # Test Accuracy:  0.7990\n",
    "    # Test Precision: 0.7990\n",
    "    # Test Recall:    1.0000\n",
    "    # Test F1 Score:  0.7098\n",
    "    # ==================================================    \n",
    "    \"epochs\": 10,\n",
    "    # ==================================================\n",
    "    # RNN Model Evaluation Statistics (Line-Level)\n",
    "    # ==================================================\n",
    "    # Test Accuracy:  0.7990\n",
    "    # Test Precision: 0.7990\n",
    "    # Test Recall:    1.0000\n",
    "    # Test F1 Score:  0.7098\n",
    "    # ==================================================\n",
    "\n",
    "    \"num_workers\": cores,\n",
    "    \"verbosity\": 2,\n",
    "}\n",
    "# model_config =  {\n",
    "#     \"name\": \"Logistic Regression (line-level, words + suffixes)\",\n",
    "#     \"model_type\": \"logistic\",\n",
    "#     \"use_suffixes\": True,\n",
    "#     \"maxIter\": 10,\n",
    "#     \"regParam\": 0.01,\n",
    "#     \"line_level\": True\n",
    "# }\n",
    "\n",
    "if not redis_client.exists(classifier_model_name):\n",
    "    # Get annotated training files\n",
    "    annotated_path = Path.cwd().parent / \"data\" / \"annotated\"\n",
    "    print(f\"Loading annotated files from: {annotated_path}\")\n",
    "\n",
    "    if annotated_path.exists():\n",
    "        annotated_files = get_file_list(str(annotated_path), pattern=\"**/*.ann\")\n",
    "\n",
    "        if len(annotated_files) > 0:\n",
    "            print(f\"Found {len(annotated_files)} annotated files\")\n",
    "\n",
    "            # Train using SkolClassifierV2 with unified API\n",
    "            print(\"Training classifier with SkolClassifierV2...\")\n",
    "            classifier = SkolClassifierV2(\n",
    "                spark=spark,\n",
    "\n",
    "                # Input\n",
    "                input_source='files',\n",
    "                file_paths=annotated_files,\n",
    "\n",
    "                # Model I/O\n",
    "                auto_load_model=False,  # Fit a new model.\n",
    "                model_storage='redis',\n",
    "                redis_client=redis_client,\n",
    "                redis_key=classifier_model_name,\n",
    "                redis_expire=classifier_model_expire,\n",
    "\n",
    "\n",
    "                # Output options\n",
    "                output_dest='couchdb',\n",
    "                couchdb_url=couchdb_url,\n",
    "                couchdb_database=ingest_db_name,\n",
    "                output_couchdb_suffix='.ann',\n",
    "                \n",
    "                # Model and preprocssing options\n",
    "                **model_config\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            results = classifier.fit()\n",
    "\n",
    "            print(f\"Training complete!\")\n",
    "            print(f\"  Accuracy: {results.get('accuracy', 0):.4f}\")\n",
    "            print(f\"  F1 Score: {results.get('f1_score', 0):.4f}\")\n",
    "\n",
    "            classifier.save_model()\n",
    "            print(f\" Model saved to Redis with key: {classifier_model_name}\")\n",
    "        else:\n",
    "            print(f\"No annotated files found in {annotated_path}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {annotated_path}\")\n",
    "        print(\"Please ensure annotated training data is available.\")\n",
    "else:\n",
    "    print(f\"Skipping generation of model {classifier_model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fe9eb-1f1b-4aa8-9796-169faeeaf432",
   "metadata": {},
   "source": [
    "## Extract the taxa names and descriptions\n",
    "\n",
    "We use a classifier to extract taxa names and descriptions from articles, issues, and books. The YEDDA annotated texts are written back to CouchDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d42f1-76de-4900-a2d8-7c3001e7ea52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing classifier with unified V2 API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1765660119.247798 2546505 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765660119.250872 2546505 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765660119.252767 2546505 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765660119.252770 2546505 gpu_device.cc:2456] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0a. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1765660119.257954 2546505 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765660119.259851 2546505 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765660119.261784 2546505 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765660119.261787 2546505 gpu_device.cc:2456] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0a. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1765660119.369118 2546505 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765660119.370519 2546505 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765660119.371720 2546505 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "I0000 00:00:1765660119.371734 2546505 gpu_device.cc:2040] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21907 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5090 Laptop GPU, pci bus id: 0000:02:00.0, compute capability: 12.0a\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load Model] Overriding prediction_stride: 20 -> 20\n",
      "[Load Model] Overriding prediction_batch_size: None -> 32\n",
      "[Load Model] Overriding batch_size: 16384 -> 16384\n",
      "[Load Model] Overriding num_workers: 4 -> 4\n",
      "[Load Model] Overriding verbosity: 2 -> 2\n",
      "Detected GPUs: ['/physical_device:GPU:0']\n",
      "GPU configuration failed: Physical devices cannot be modified after being initialized. Attempting to use CPU...\n",
      "Could not force CPU mode: Visible devices cannot be modified after being initialized\n",
      "[RNN set_model] Model set, weights count: 14\n",
      "Model loaded from Redis: skol:classifier:model:rnn-v1.0\n",
      "\n",
      "Loading and classifying documents from CouchDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1160796 text documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+--------------------+-----------+\n",
      "|              doc_id|           human_url|attachment_name|               value|line_number|\n",
      "+--------------------+--------------------+---------------+--------------------+-----------+\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|           MYCOTAXON|          0|\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|Volume 108, pp. 2...|          1|\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|     AprilJune 2009|          2|\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|Rattania setulife...|          3|\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|an undescribed en...|          4|\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|on rattans from W...|          5|\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|Ashish Prabhugaon...|          6|\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|*ashishprabhugaon...|          7|\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|Department of Bot...|          8|\n",
      "|006b331e284e4dc8b...|https://www.ingen...|    article.txt|Goa  403 206, In...|          9|\n",
      "+--------------------+--------------------+---------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Making predictions...\n",
      "[RNN Predict] Starting prediction [CODE VERSION 2025-12-11-17:45]\n",
      "[RNN Predict] Input data columns: ['doc_id', 'human_url', 'attachment_name', 'value', 'line_number', 'words', 'word_tf', 'word_idf', 'suffixes', 'suffix_tf', 'suffix_idf', 'combined_idf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RNN Predict] Input data count: 1160796\n",
      "[RNN Predict] Verbosity level: 2\n",
      "[RNN Predict] Transforming data into sequences\n",
      "[RNN Predict] Preparing model config and weights for UDF\n",
      "[RNN Predict] model_weights is None: False\n",
      "[RNN Predict] model_weights count: 14\n",
      "[RNN Predict] Using sliding window: window_size=20, stride=20\n",
      "[RNN Predict] Applying prediction UDF to sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660143.919902 2548309 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660143.958672 2548313 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660143.972759 2548317 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660144.045830 2548305 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660144.634914 2548309 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660144.672912 2548313 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660144.688554 2548317 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765660144.758398 2548305 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Detected GPUs: ['/physical_device:GPU:0']\n",
      "Configured 1 GPU(s) with memory growth enabled\n",
      "Detected GPUs: ['/physical_device:GPU:0']\n",
      "Configured 1 GPU(s) with memory growth enabled\n",
      "Detected GPUs: ['/physical_device:GPU:0']\n",
      "Configured 1 GPU(s) with memory growth enabled\n",
      "Detected GPUs: ['/physical_device:GPU:0']\n",
      "Configured 1 GPU(s) with memory growth enabled\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 30 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 30 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 30 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 30 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "[UDF START] Processing 183 sequences\n",
      "[UDF START] Config: input_size=1000, window_size=20, features_col=combined_idf\n",
      "[UDF START] Log file: /tmp/rnn_udf_debug_2548309_1765660148.log\n",
      "[UDF] Set CUDA_VISIBLE_DEVICES to empty string\n",
      "[UDF WARNING] TensorFlow config issue: Visible devices cannot be modified after being initialized\n",
      "[UDF START] Processing 220 sequences\n",
      "[UDF START] Config: input_size=1000, window_size=20, features_col=combined_idf\n",
      "[UDF START] Log file: /tmp/rnn_udf_debug_2548313_1765660148.log\n",
      "[UDF] Set CUDA_VISIBLE_DEVICES to empty string\n",
      "[UDF WARNING] TensorFlow config issue: Visible devices cannot be modified after being initialized\n",
      "[UDF] Model rebuilt successfully\n",
      "[UDF] Initialization complete, starting prediction loop\n",
      "[UDF PHASE 1] Extracting features from 183 sequences\n",
      "[UDF START] Processing 205 sequences\n",
      "[UDF START] Config: input_size=1000, window_size=20, features_col=combined_idf\n",
      "[UDF START] Log file: /tmp/rnn_udf_debug_2548317_1765660148.log\n",
      "[UDF] Set CUDA_VISIBLE_DEVICES to empty string\n",
      "[UDF WARNING] TensorFlow config issue: Visible devices cannot be modified after being initialized\n",
      "[UDF] Model rebuilt successfully\n",
      "[UDF] Initialization complete, starting prediction loop\n",
      "[UDF PHASE 1] Extracting features from 220 sequences\n",
      "[UDF] Model rebuilt successfully\n",
      "[UDF] Initialization complete, starting prediction loop\n",
      "[UDF PHASE 1] Extracting features from 205 sequences\n",
      "[UDF START] Processing 139 sequences\n",
      "[UDF START] Config: input_size=1000, window_size=20, features_col=combined_idf\n",
      "[UDF START] Log file: /tmp/rnn_udf_debug_2548305_1765660148.log\n",
      "[UDF] Set CUDA_VISIBLE_DEVICES to empty string\n",
      "[UDF WARNING] TensorFlow config issue: Visible devices cannot be modified after being initialized\n",
      "[UDF] Model rebuilt successfully\n",
      "[UDF] Initialization complete, starting prediction loop\n",
      "[UDF PHASE 1] Extracting features from 139 sequences\n",
      "[UDF PHASE 2] Predicting on 4400 windows from 183 sequences\n",
      "[UDF PHASE 2] Using 69 batches of max size 64\n",
      "[UDF PHASE 2] Batch 1/69: shape (64, 20, 1000)\n",
      "[UDF PHASE 2] Predicting on 4781 windows from 220 sequences\n",
      "[UDF PHASE 2] Using 75 batches of max size 64\n",
      "[UDF PHASE 2] Batch 1/75: shape (64, 20, 1000)\n",
      "[UDF PHASE 2] Predicting on 5149 windows from 205 sequences\n",
      "[UDF PHASE 2] Using 81 batches of max size 64\n",
      "[UDF PHASE 2] Batch 1/81: shape (64, 20, 1000)\n",
      "[UDF PHASE 2] Predicting on 5533 windows from 139 sequences\n",
      "[UDF PHASE 2] Using 87 batches of max size 64\n",
      "[UDF PHASE 2] Batch 1/87: shape (64, 20, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Predict from CouchDB and save back to CouchDB using SkolClassifierV2\n",
    "print(\"Initializing classifier with unified V2 API...\")\n",
    "\n",
    "model_config2 = model_config.copy()\n",
    "model_config2.update({\n",
    "    \"num_workers\": 4,\n",
    "    \"prediction_batch_size\": 32,\n",
    "    \"verbosity\": 2,\n",
    "})\n",
    "\n",
    "classifier = SkolClassifierV2(\n",
    "    spark=spark,\n",
    "    input_source='couchdb',\n",
    "    couchdb_url=couchdb_url,\n",
    "    couchdb_database=ingest_db_name,\n",
    "    couchdb_username=couchdb_username,\n",
    "    couchdb_password=couchdb_password,\n",
    "    couchdb_pattern='*.txt',\n",
    "    output_dest='couchdb',\n",
    "    output_couchdb_suffix='.ann',\n",
    "    model_storage='redis',\n",
    "    redis_client=redis_client,\n",
    "    redis_key=classifier_model_name,\n",
    "    auto_load_model=True,\n",
    "    coalesce_labels=True,\n",
    "    output_format='annotated',\n",
    "    **model_config2\n",
    ")\n",
    "\n",
    "print(f\"Model loaded from Redis: {classifier_model_name}\")\n",
    "\n",
    "# Load, predict, and save in a streamlined workflow\n",
    "print(\"\\nLoading and classifying documents from CouchDB...\")\n",
    "raw_df = classifier.load_raw()\n",
    "print(f\"Loaded {raw_df.count()} text documents\")\n",
    "raw_df.show(10)\n",
    "\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = classifier.predict(raw_df)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "predictions.select(\n",
    "    \"doc_id\", \"attachment_name\", \"predicted_label\"\n",
    ").show(5, truncate=50)\n",
    "\n",
    "# Save results back to CouchDB\n",
    "print(\"\\nSaving predictions back to CouchDB...\")\n",
    "classifier.save_annotated(predictions)\n",
    "\n",
    "print(f\"\\n Predictions saved to CouchDB as .ann attachments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1701a1c-864f-494a-a393-7ec77ac14f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"predicted_label\", \"annotated_value\").where('predicted_label = \"Nomenclature\"').show()\n",
    "predictions.groupBy(\"predicted_label\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002ae67-9dc9-4335-adf4-d249905d2ad9",
   "metadata": {},
   "source": [
    "Here we estimate an approximation for the number of Taxon structures we'd like to find. The abbreviation \"nov.\" (\"novum\") indicates a new taxon in the current article. This should be a lower bound, as it is not unusual to redescribe a species, e.g. in a survey article or monograph on a genus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9de42c-8b11-4dfe-aa4b-faa838f3dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"*\").filter(col(\"annotated_value\").contains(\"nov.\")).where(\"predicted_label = 'Nomenclature'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca929b6a-75e5-4dcb-ace2-8d420f42ce41",
   "metadata": {},
   "source": [
    "## Build the Taxon objects and store them in CouchDB\n",
    "We use CouchDB to store a full record for each taxon. We copy all metadata to the taxon records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e285b7-0afa-486e-94f6-41bcb1d7524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDBFile(CDBF):\n",
    "    \"\"\"\n",
    "    File-like object that reads from CouchDB attachment content.\n",
    "\n",
    "    This class extends FileObject to support reading text from CouchDB\n",
    "    attachments while preserving database metadata (doc_id, attachment_name,\n",
    "    and database name).\n",
    "    \"\"\"\n",
    "\n",
    "    _doc_id: str\n",
    "    _attachment_name: str\n",
    "    _db_name: str\n",
    "    _human_url: Optional[str]\n",
    "    _content_lines: List[str]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        content: str,\n",
    "        doc_id: str,\n",
    "        attachment_name: str,\n",
    "        db_name: str,\n",
    "        human_url: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize CouchDBFile from attachment content.\n",
    "\n",
    "        Args:\n",
    "            content: Text content from CouchDB attachment\n",
    "            doc_id: CouchDB document ID\n",
    "            attachment_name: Name of the attachment (e.g., \"article.txt.ann\")\n",
    "            db_name: Database name where document is stored (ingest_db_name)\n",
    "            url: Optional URL from the CouchDB row\n",
    "        \"\"\"\n",
    "        self._doc_id = doc_id\n",
    "        self._attachment_name = attachment_name\n",
    "        self._db_name = db_name\n",
    "        self._human_url = human_url\n",
    "        self._line_number = 0\n",
    "        self._page_number = 1\n",
    "        self._empirical_page_number = None\n",
    "\n",
    "        # Split content into lines\n",
    "        self._content_lines = content.split('\\n')\n",
    "\n",
    "    def _get_content_iterator(self) -> Iterator[str]:\n",
    "        \"\"\"Get iterator over content lines.\"\"\"\n",
    "        return iter(self._content_lines)\n",
    "\n",
    "    @property\n",
    "    def filename(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a composite identifier for CouchDB documents.\n",
    "\n",
    "        Format: db_name/doc_id/attachment_name\n",
    "        This allows tracking the source of each line.\n",
    "        \"\"\"\n",
    "        return f\"{self._db_name}/{self._doc_id}/{self._attachment_name}\"\n",
    "\n",
    "    @property\n",
    "    def doc_id(self) -> str:\n",
    "        \"\"\"CouchDB document ID.\"\"\"\n",
    "        return self._doc_id\n",
    "\n",
    "    @property\n",
    "    def attachment_name(self) -> str:\n",
    "        \"\"\"Attachment filename.\"\"\"\n",
    "        return self._attachment_name\n",
    "\n",
    "    @property\n",
    "    def db_name(self) -> str:\n",
    "        \"\"\"Database name (ingest_db_name).\"\"\"\n",
    "        return self._db_name\n",
    "\n",
    "    @property\n",
    "    def human_url(self) -> Optional[str]:\n",
    "        \"\"\"URL from the CouchDB row.\"\"\"\n",
    "        return self._human_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22648996-4d9f-47f1-ab4f-6b5fd54f97e9",
   "metadata": {},
   "source": [
    "## Build Taxon objects\n",
    "\n",
    "Here we extract the Taxon objects from the annotated attachments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08069896-722b-45c7-8b95-78cbb89e8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_couchdb_url = couchdb_url\n",
    "ingest_username = couchdb_username\n",
    "ingest_password = couchdb_password\n",
    "taxon_couchdb_url = couchdb_url\n",
    "taxon_username = couchdb_username\n",
    "taxon_password = couchdb_password\n",
    "pattern = '*.txt.ann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed264e3-3e3c-4c32-bb99-2205edaa4bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create TaxonExtractor instance with database configuration\n",
    "extractor = TaxonExtractor(\n",
    "    spark=spark,\n",
    "    ingest_couchdb_url=ingest_couchdb_url,\n",
    "    ingest_db_name=ingest_db_name,\n",
    "    taxon_db_name=taxon_db_name,\n",
    "    ingest_username=ingest_username,\n",
    "    ingest_password=ingest_password,\n",
    "    taxon_username=taxon_username,\n",
    "    taxon_password=taxon_password\n",
    ")\n",
    "\n",
    "print(\"TaxonExtractor initialized\")\n",
    "print(f\"  Ingest DB: {ingest_db_name}\")\n",
    "print(f\"  Taxon DB:  {taxon_db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5acdf9-f40f-4ef7-b18b-b5f8236b248b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Load annotated documents\n",
    "print(\"\\nStep 1: Loading annotated documents from CouchDB...\")\n",
    "annotated_df = extractor.load_annotated_documents(pattern='*.txt.ann')\n",
    "print(f\"Loaded {annotated_df.count()} annotated documents\")\n",
    "annotated_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952917da-82e8-4ed0-8876-e1705f4631ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Extract taxa to DataFrame\n",
    "print(\"\\nStep 2: Extracting taxa from annotated documents...\")\n",
    "taxa_df = extractor.extract_taxa(annotated_df)\n",
    "print(f\"Extracted {taxa_df.count()} taxa\")\n",
    "taxa_df.printSchema()\n",
    "taxa_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f01347-0543-4499-a2e7-065a54e61938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Inspect actual Taxon objects from the RDD (optional debugging)\n",
    "print(\"\\n=== Sample Taxon Objects ===\")\n",
    "taxa_rdd = annotated_df.rdd.mapPartitions(\n",
    "    lambda partition: extract_taxa_from_partition(iter(partition), ingest_db_name)  # type: ignore[reportUnknownArgumentType]\n",
    ")\n",
    "for i, taxon in enumerate(taxa_rdd.take(3)):\n",
    "    print(f\"\\nTaxon {i+1}:\")\n",
    "    print(f\"  Type: {type(taxon)}\")\n",
    "    print(f\"  Has nomenclature: {taxon.has_nomenclature()}\")\n",
    "    taxon_row = taxon.as_row()\n",
    "    print(f\"  Taxon name: {taxon_row['taxon'][:80]}...\")\n",
    "    print(f\"  Source: {taxon_row['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7af36b-f190-4dd7-8f29-a9ed11f64c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Save taxa to CouchDB\n",
    "print(\"\\nStep 4: Saving taxa to CouchDB...\")\n",
    "results_df = extractor.save_taxa(taxa_df)\n",
    "\n",
    "# Show detailed results\n",
    "results_df.groupBy(\"success\").count().show(truncate=False)\n",
    "\n",
    "# If there are failures, show error messages\n",
    "print(\"\\nError messages:\")\n",
    "results_df.filter(\"success = false\").select(\"error_message\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ewsk11g9lpl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run the complete pipeline in one step\n",
    "# Uncomment to use the simplified one-step approach:\n",
    "\n",
    "# print(\"\\nRunning complete pipeline...\")\n",
    "# results = extractor.run_pipeline(pattern='*.txt.ann')\n",
    "#\n",
    "# successful = results.filter(\"success = true\").count()\n",
    "# failed = results.filter(\"success = false\").count()\n",
    "#\n",
    "# print(f\"\\nPipeline Results:\")\n",
    "# print(f\"  Successful: {successful}\")\n",
    "# print(f\"  Failed:     {failed}\")\n",
    "#\n",
    "# results.groupBy(\"success\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769c280-8139-4228-b5a7-d21dab100910",
   "metadata": {},
   "source": [
    "### Observations on the classification models\n",
    "\n",
    "The line-by-line classification model is classifying many Description lines as Misc-exposition. It works reasonably well for Nomenclature.\n",
    "\n",
    "The problem with the paragraph classification model is that the heuristic paragrph parser does not generalize well to the more modern journals.\n",
    "\n",
    "One possible approach to investigate is adding heuristics to the label-merging code to convert some Misc-exposition lines to Description if they are surrounded by Description paragraphs.\n",
    "\n",
    "A more sophisticated approach is to use a completely new model that has some memory, such as an RNN, or a two pass model that adds the label of the previous line(s) as added features for each line.\n",
    "\n",
    "It may become necessary to hand annotate some of the more modern journals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ee719-eb73-425b-ad44-6f30158c8d5e",
   "metadata": {},
   "source": [
    "## Dr. Drafts document embedding\n",
    "\n",
    "Dr. Drafts is the framework we use to embed all the descriptions into a searchable space. SBERT is a model that can embed sentences into a semantic space such that sentences with similar meaning are near each other. The data structure that we build here is central to the eventual function of the SKOL web site.\n",
    "\n",
    "Dr. Drafts loads taxon documents from the CouchDB, and builds an embedding which it saves to redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c550d-15f4-426c-b11e-d7ac6691e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dr_drafts_mycosearch.data import SKOL_TAXA as STX\n",
    "from dr_drafts_mycosearch.compute_embeddings import EmbeddingsComputer as EC\n",
    "\n",
    "class SKOL_TAXA(STX):\n",
    "    \"\"\"Data interface for Synopotic Key of Life Taxa in CouchDB.\"\"\"\n",
    "    def load_data(self):\n",
    "        \"\"\"Load taxon data from CouchDB into a pandas DataFrame.\"\"\"\n",
    "        # Connect to CouchDB\n",
    "        server = couchdb.Server(self.couchdb_url)\n",
    "        if self.username and self.password:\n",
    "            server.resource.credentials = (self.username, self.password)\n",
    "\n",
    "        # Access the database\n",
    "        if self.db_name not in server:\n",
    "            raise ValueError(f\"Database '{self.db_name}' not found in CouchDB server\")\n",
    "\n",
    "        db = server[self.db_name]\n",
    "\n",
    "        # Fetch all documents from the database\n",
    "        records = []\n",
    "        for doc_id in db:\n",
    "            # Skip design documents\n",
    "            if doc_id.startswith('_design/'):\n",
    "                continue\n",
    "\n",
    "            doc = db[doc_id]\n",
    "            print(f\"DEBUG: doc: {doc}\")  # Debugging line to inspect document structure\n",
    "            records.append(doc)\n",
    "\n",
    "        if not records:\n",
    "            # Create empty DataFrame if no records found\n",
    "            self.df = pd.DataFrame()\n",
    "            print(f\"Warning: No taxon records found in database '{self.db_name}'\")\n",
    "            return\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        self.df = pd.DataFrame(records)\n",
    "        assert self.df.iloc[0]['source']['human_url'].startswith('http'), \"Expected 'source.url' to start with 'http'\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a79b3c-4ac4-4208-9346-0a5bc90e0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsComputer(EC):\n",
    "    \"\"\"Class for computing and storing embeddings from narrative data.\"\"\"\n",
    "    \n",
    "    def write_embeddings_to_redis(self):\n",
    "        \"\"\"Write embeddings to Redis using instance configuration.\"\"\"\n",
    "        if self.redis_username and self.redis_password:\n",
    "            r = redis.from_url(self.redis_url, username=self.redis_username, password=self.redis_password, db=self.redis_db)\n",
    "        else:\n",
    "            r = redis.from_url(self.redis_url, db=self.redis_db)\n",
    "\n",
    "        pickled_data = pickle.dumps(self.result)\n",
    "        r.set(self.embedding_name, pickled_data)\n",
    "        if self.redist_expire is not None:\n",
    "            r.expire(self.embedding_name, self.redist_expire)\n",
    "        print(f'Embeddings written to Redis (db={self.redis_db}) with key: {self.embedding_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345caed-a301-49ca-a182-ce0ace7e839b",
   "metadata": {},
   "source": [
    "## Compute Embeddings\n",
    "\n",
    "We use SBERT to embed the taxa into a search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c806139-15d0-4904-9ec9-8f7e6ada5b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skol_taxa = SKOL_TAXA(\n",
    "    couchdb_url=\"http://localhost:5984\",\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password,\n",
    "    db_name=taxon_db_name\n",
    ")\n",
    "descriptions = skol_taxa.get_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e661cb-fc85-4c1a-9617-b61ca7d1199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not redis_client.exists(embedding_name):\n",
    "\n",
    "    embedder = EmbeddingsComputer(\n",
    "        idir='/dev/null',\n",
    "        redis_url='redis://localhost:6379',\n",
    "        redis_expire=embedding_expire,\n",
    "        embedding_name=embedding_name,\n",
    "    )\n",
    "\n",
    "    embedding_result = embedder.run(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1498755-a114-4c46-985d-47f47e7abee8",
   "metadata": {},
   "source": [
    "## Compute JSON versions of all descriptions\n",
    "\n",
    "There is an anticipated need for the details of each description to be available as a nested JSON structure, which can be used to build menus with features, subfeatures, and values.\n",
    "\n",
    "The TaxaJSONTranslator reads taxa from the CouchDB and writes annotated taxa back out to CouchDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83954366-44a3-430f-92b9-526ac08637d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxaJSONTranslator(TJT):\n",
    "    \"\"\"\n",
    "    Translates taxa descriptions to structured JSON using a fine-tuned Mistral model.\n",
    "\n",
    "    This class is optimized for processing PySpark DataFrames created by\n",
    "    TaxonExtractor.load_taxa(), adding a new column with JSON-formatted features.\n",
    "    \"\"\"\n",
    "    def load_taxa(\n",
    "        self,\n",
    "        db_name: str,\n",
    "        pattern: str = \"*\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Load taxa from CouchDB taxon database.\n",
    "\n",
    "        This method loads taxa documents saved by TaxonExtractor.save_taxa()\n",
    "        and returns them as a DataFrame compatible with translate_descriptions().\n",
    "\n",
    "        Args:\n",
    "            db_name: Name of taxon database\n",
    "            pattern: Pattern for document IDs to load (default: \"*\")\n",
    "                    Use \"*\" to load all documents\n",
    "                    Use \"taxon_abc*\" to load specific subset\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns:\n",
    "                - _id: CouchDB document ID (for joining results)\n",
    "                - taxon: String of concatenated nomenclature paragraphs\n",
    "                - description: String of concatenated description paragraphs\n",
    "                - source: Dict with keys doc_id, url, db_name\n",
    "                - line_number: Line number of first nomenclature paragraph\n",
    "                - paragraph_number: Paragraph number of first nomenclature paragraph\n",
    "                - page_number: Page number of first nomenclature paragraph\n",
    "                - empirical_page_number: Empirical page number of first nomenclature paragraph\n",
    "\n",
    "        Example:\n",
    "            >>> translator = TaxaJSONTranslator(\n",
    "            ...     spark=spark,\n",
    "            ...     couchdb_url=\"http://localhost:5984\",\n",
    "            ...     username=\"admin\",\n",
    "            ...     password=\"secret\",\n",
    "            ...     checkpoint_path=\"...\"\n",
    "            ... )\n",
    "            >>> taxa_df = translator.load_taxa(db_name=\"mycobank_taxa\")\n",
    "            >>> print(f\"Loaded {taxa_df.count()} taxa\")\n",
    "        \"\"\"\n",
    "        from skol_classifier.couchdb_io import CouchDBConnection\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, MapType, IntegerType\n",
    "\n",
    "        # Define schema with _id for joining results\n",
    "        schema = StructType([\n",
    "            StructField(\"_id\", StringType(), False),\n",
    "            StructField(\"taxon\", StringType(), False),\n",
    "            StructField(\"description\", StringType(), False),\n",
    "            StructField(\"source\", MapType(StringType(), StringType(), valueContainsNull=True), False),\n",
    "            StructField(\"line_number\", IntegerType(), True),\n",
    "            StructField(\"paragraph_number\", IntegerType(), True),\n",
    "            StructField(\"page_number\", IntegerType(), True),\n",
    "            StructField(\"empirical_page_number\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        # Use CouchDBConnection to load data\n",
    "        conn = CouchDBConnection(self.couchdb_url, db_name, username=self.username, password=self.password)\n",
    "\n",
    "        # Get matching document IDs\n",
    "        doc_ids = conn.get_all_doc_ids(pattern)\n",
    "\n",
    "        if not doc_ids:\n",
    "            print(f\"No documents found matching pattern '{pattern}'\")\n",
    "            return self.spark.createDataFrame([], schema)\n",
    "\n",
    "        print(f\"Loading {len(doc_ids)} taxa from {db_name}...\")\n",
    "\n",
    "        # Create DataFrame with doc_ids for parallel processing\n",
    "        doc_ids_rdd = self.spark.sparkContext.parallelize(doc_ids)\n",
    "        doc_ids_df = doc_ids_rdd.map(lambda x: (x,)).toDF([\"doc_id\"])\n",
    "\n",
    "        # Prepare connection parameters for workers\n",
    "        couchdb_url = self.couchdb_url\n",
    "        username = self.username\n",
    "        password = self.password\n",
    "\n",
    "        # Load taxa using mapPartitions\n",
    "        def load_partition(partition):\n",
    "            \"\"\"Load taxa from CouchDB for an entire partition.\"\"\"\n",
    "            from skol_classifier.couchdb_io import CouchDBConnection\n",
    "            from pyspark.sql import Row\n",
    "\n",
    "            # Create connection using CouchDBConnection API\n",
    "            conn = CouchDBConnection(couchdb_url, db_name, username, password)\n",
    "\n",
    "            try:\n",
    "                db = conn.db\n",
    "\n",
    "                # Process each row (which contains doc_id)\n",
    "                for row in partition:\n",
    "                    try:\n",
    "                        doc_id = row.doc_id if hasattr(row, 'doc_id') else str(row[0])\n",
    "\n",
    "                        # Load document from CouchDB\n",
    "                        if doc_id in db:\n",
    "                            doc = db[doc_id]\n",
    "\n",
    "                            # Convert CouchDB document to Row (include _id for joining)\n",
    "                            taxon_data = {\n",
    "                                '_id': doc.get('_id', doc_id),\n",
    "                                'taxon': doc.get('taxon', ''),\n",
    "                                'description': doc.get('description', ''),\n",
    "                                'source': doc.get('source', {}),\n",
    "                                'line_number': doc.get('line_number'),\n",
    "                                'paragraph_number': doc.get('paragraph_number'),\n",
    "                                'page_number': doc.get('page_number'),\n",
    "                                'empirical_page_number': doc.get('empirical_page_number')\n",
    "                            }\n",
    "\n",
    "                            yield Row(**taxon_data)\n",
    "                        else:\n",
    "                            print(f\"Document {doc_id} not found in database\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading taxon {doc_id}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error connecting to CouchDB: {e}\")\n",
    "\n",
    "        taxa_rdd = doc_ids_df.rdd.mapPartitions(load_partition)\n",
    "        taxa_df = self.spark.createDataFrame(taxa_rdd, schema)\n",
    "\n",
    "        count = taxa_df.count()\n",
    "        print(f\" Loaded {count} taxa\")\n",
    "\n",
    "        return taxa_df\n",
    "\n",
    "    def save_taxa(\n",
    "        self,\n",
    "        taxa_df: DataFrame,\n",
    "        db_name: str,\n",
    "        json_annotated_col: str = \"features_json\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Save taxa DataFrame to CouchDB, including the json_annotated field.\n",
    "\n",
    "        This method saves taxa with the translated JSON features back to CouchDB.\n",
    "        It handles arbitrary JSON in the json_annotated_col by parsing it before storage.\n",
    "        The save operation is idempotent - documents with the same composite key\n",
    "        (source.doc_id, source.url, line_number) will be updated rather than duplicated.\n",
    "\n",
    "        Uses credentials from self.username and self.password.\n",
    "\n",
    "        Args:\n",
    "            taxa_df: DataFrame with taxa and translations (must include json_annotated_col)\n",
    "            db_name: Name of taxon database\n",
    "            json_annotated_col: Name of column containing JSON features (default: \"features_json\")\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with save results (doc_id, success, error_message)\n",
    "\n",
    "        Example:\n",
    "            >>> # Load taxa and translate\n",
    "            >>> taxa_df = translator.load_taxa(db_name=\"mycobank_taxa\")\n",
    "            >>> enriched_df = translator.translate_descriptions(taxa_df)\n",
    "            >>>\n",
    "            >>> # Save back to CouchDB\n",
    "            >>> results = translator.save_taxa(enriched_df, db_name=\"mycobank_taxa\")\n",
    "            >>> print(f\"Saved: {results.filter('success = true').count()}\")\n",
    "        \"\"\"\n",
    "        from pyspark.sql import Row\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "        # Get credentials from self\n",
    "        couchdb_url = self.couchdb_url\n",
    "        username = self.username\n",
    "        password = self.password\n",
    "\n",
    "        # Schema for save results\n",
    "        save_schema = StructType([\n",
    "            StructField(\"doc_id\", StringType(), False),\n",
    "            StructField(\"success\", BooleanType(), False),\n",
    "            StructField(\"error_message\", StringType(), False),\n",
    "        ])\n",
    "\n",
    "        def save_partition(partition):\n",
    "            \"\"\"Save taxa to CouchDB for an entire partition (idempotent).\"\"\"\n",
    "            from skol_classifier.couchdb_io import CouchDBConnection\n",
    "            import hashlib\n",
    "\n",
    "            def generate_taxon_doc_id(doc_id: str, url: Optional[str], line_number: int) -> str:\n",
    "                \"\"\"Generate deterministic document ID for idempotent saves.\"\"\"\n",
    "                key_parts = [\n",
    "                    doc_id,\n",
    "                    url if url else \"no_url\",\n",
    "                    str(line_number)\n",
    "                ]\n",
    "                composite_key = \":\".join(key_parts)\n",
    "                hash_obj = hashlib.sha256(composite_key.encode('utf-8'))\n",
    "                doc_hash = hash_obj.hexdigest()\n",
    "                return f\"taxon_{doc_hash}\"\n",
    "\n",
    "            # Create connection using CouchDBConnection API\n",
    "            conn = CouchDBConnection(couchdb_url, db_name, username, password)\n",
    "\n",
    "            # Connect to CouchDB once per partition\n",
    "            try:\n",
    "                # Try to get database, create if it doesn't exist\n",
    "                import couchdb\n",
    "                server = couchdb.Server(couchdb_url)\n",
    "                if username and password:\n",
    "                    server.resource.credentials = (username, password)\n",
    "\n",
    "                if db_name not in server:\n",
    "                    server.create(db_name)\n",
    "\n",
    "                db = conn.db\n",
    "\n",
    "                # Process each taxon in the partition\n",
    "                for row in partition:\n",
    "                    success = False\n",
    "                    error_msg = \"\"\n",
    "                    doc_id = \"unknown\"\n",
    "\n",
    "                    try:\n",
    "                        # Extract source metadata from row\n",
    "                        source_dict = row.source if hasattr(row, 'source') else {}\n",
    "                        source = dict(source_dict) if isinstance(source_dict, dict) else {}\n",
    "                        source_doc_id = str(source.get('doc_id', 'unknown'))\n",
    "                        source_url = source.get('url')\n",
    "                        line_number = row.line_number if hasattr(row, 'line_number') else 0\n",
    "\n",
    "                        # Generate deterministic document ID\n",
    "                        doc_id = generate_taxon_doc_id(\n",
    "                            source_doc_id,\n",
    "                            source_url if isinstance(source_url, str) else None,\n",
    "                            int(line_number) if line_number else 0\n",
    "                        )\n",
    "\n",
    "                        # Convert row to dict for CouchDB storage\n",
    "                        taxon_doc = row.asDict()\n",
    "\n",
    "                        # Handle json_annotated field: parse JSON string to dict\n",
    "                        if json_annotated_col in taxon_doc and taxon_doc[json_annotated_col]:\n",
    "                            json_str = taxon_doc[json_annotated_col]\n",
    "                            if isinstance(json_str, str):\n",
    "                                try:\n",
    "                                    # Parse JSON string to dict for storage\n",
    "                                    taxon_doc['json_annotated'] = json.loads(json_str)\n",
    "                                except json.JSONDecodeError:\n",
    "                                    print(f\"Warning: Invalid JSON in {json_annotated_col} for doc {doc_id}\")\n",
    "                                    taxon_doc['json_annotated'] = {}\n",
    "                            else:\n",
    "                                # Already a dict, just rename the field\n",
    "                                taxon_doc['json_annotated'] = json_str\n",
    "                            # Remove the original column if it has a different name\n",
    "                            if json_annotated_col != 'json_annotated':\n",
    "                                del taxon_doc[json_annotated_col]\n",
    "\n",
    "                        # Check if document already exists (idempotent)\n",
    "                        if doc_id in db:\n",
    "                            # Document exists - update it\n",
    "                            existing_doc = db[doc_id]\n",
    "                            taxon_doc['_id'] = doc_id\n",
    "                            taxon_doc['_rev'] = existing_doc['_rev']\n",
    "                        else:\n",
    "                            # New document - create it\n",
    "                            taxon_doc['_id'] = doc_id\n",
    "\n",
    "                        db.save(taxon_doc)\n",
    "                        success = True\n",
    "\n",
    "                    except Exception as e:\n",
    "                        error_msg = str(e)\n",
    "                        print(f\"Error saving taxon {doc_id}: {e}\")\n",
    "\n",
    "                    yield Row(\n",
    "                        doc_id=doc_id,\n",
    "                        success=success,\n",
    "                        error_message=error_msg\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error connecting to CouchDB: {e}\")\n",
    "                # Yield failures for all rows\n",
    "                for row in partition:\n",
    "                    yield Row(\n",
    "                        doc_id=\"unknown_connection_error\",\n",
    "                        success=False,\n",
    "                        error_message=str(e)\n",
    "                    )\n",
    "\n",
    "        print(f\"Saving taxa to {db_name}...\")\n",
    "        results_df = taxa_df.rdd.mapPartitions(save_partition).toDF(save_schema)\n",
    "\n",
    "        total = results_df.count()\n",
    "        successes = results_df.filter(\"success = true\").count()\n",
    "        failures = total - successes\n",
    "\n",
    "        print(f\" Save complete:\")\n",
    "        print(f\"  Total: {total}\")\n",
    "        print(f\"  Successful: {successes}\")\n",
    "        print(f\"  Failed: {failures}\")\n",
    "\n",
    "        return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ef2d8-ce08-400a-91d6-f089d0f2e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TaxaJSONTranslator(\n",
    "    spark=spark,\n",
    "    base_model_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    max_length=2048,\n",
    "    max_new_tokens=1024,\n",
    "    device=\"cuda\",\n",
    "    load_in_4bit=True,\n",
    "    use_auth_token=True,\n",
    "    couchdb_url=couchdb_url,\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b0a40-6839-47c9-ac9e-409b2803f93e",
   "metadata": {},
   "source": [
    "### Run the mistral model to generate JSON from each Taxon description."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d39221ce-9e84-41fc-b741-0917afd8ecf6",
   "metadata": {},
   "source": [
    "descriptions_df = translator.load_taxa(db_name=taxon_db_name).limit(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "334f6d67-fef5-4d4c-b98c-71014a74e4ea",
   "metadata": {},
   "source": [
    "descriptions_df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14202225-e1be-4e68-8b7a-46f3002695b0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "json_annotated_df = translator.translate_descriptions_batch(\n",
    "    taxa_df=descriptions_df,\n",
    "    batch_size=10,\n",
    "    description_col=\"description\",\n",
    "    output_col=\"json_annotated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e317f-b02a-417e-a03a-cfcb27ec2e87",
   "metadata": {},
   "source": [
    "### Add the generated fields as a field on the objects generated by save_taxa."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c528c46-3018-408e-b2d2-6407456b3862",
   "metadata": {},
   "source": [
    "results_df = translator.save_taxa(json_annotated_df, db_name=json_taxon_db_name)\n",
    "\n",
    "results_df.groupBy(\"success\").count().show(truncate=False)\n",
    "\n",
    "print(\"\\nError messages:\")\n",
    "results_df.filter(\"success = false\").select(\"error_message\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526beef-5503-4fac-a9a5-427367d625a5",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "We use Agglomerative Clustering to group the taxa into \"clades\" based in cosine similarity of their SBERT embeddings. We then load them into neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e0d92-b22b-40ec-ad75-1aab43355c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxon_clusterer import TaxonClusterer as TC\n",
    "\n",
    "class TaxonClusterer(TC):\n",
    "    \n",
    "    def load_embeddings(self, embedding_key: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Load embeddings from Redis.\n",
    "\n",
    "        Args:\n",
    "            embedding_key: Redis key containing pickled embeddings\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (embeddings array, taxon names list, taxon metadata list)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If key doesn't exist or data is invalid\n",
    "        \"\"\"\n",
    "        print(f\"Loading embeddings from Redis key: {embedding_key}\")\n",
    "\n",
    "        if not self.redis_client.exists(embedding_key):\n",
    "            raise ValueError(f\"Redis key '{embedding_key}' does not exist\")\n",
    "\n",
    "        # Load pickled data from Redis\n",
    "        pickled_data = self.redis_client.get(embedding_key)\n",
    "        data = pickle.loads(pickled_data)\n",
    "\n",
    "        # Assume it's a pandas DataFrame from EmbeddingsComputer\n",
    "        try:\n",
    "            assert isinstance(data, pd.DataFrame)\n",
    "            # Extract embedding columns (F0, F1, F2, ...)\n",
    "            embedding_cols = [col for col in data.columns if col.startswith('F')]\n",
    "            self.embeddings = data[embedding_cols].values\n",
    "\n",
    "            # Extract taxon names from 'taxon' field (nomenclature)\n",
    "            # If 'taxon' column doesn't exist, fall back to 'description'\n",
    "            if 'taxon' in data.columns:\n",
    "                self.taxon_names = data['taxon'].tolist()\n",
    "            else:\n",
    "                self.taxon_names = data['description'].tolist()\n",
    "\n",
    "            # Extract metadata from other columns\n",
    "            self.taxon_metadata = []\n",
    "            for _, row in data.iterrows():\n",
    "                metadata = {}\n",
    "\n",
    "                # Flatten source dict for neo4j storage.\n",
    "                if 'source' in data.columns:\n",
    "                    source = row['source']\n",
    "                    assert isinstance(source, dict), \"Source field must be dict\"\n",
    "                    for key in source.keys():\n",
    "                        metadata[f'source_{key}'] = source[key]\n",
    "\n",
    "                # Add other metadata fields\n",
    "                if 'filename' in data.columns:\n",
    "                    metadata['filename'] = row.get('filename')\n",
    "                if 'row' in data.columns:\n",
    "                    metadata['row'] = row.get('row')\n",
    "                if 'line_number' in data.columns:\n",
    "                    metadata['line_number'] = row.get('line_number')\n",
    "                if 'paragraph_number' in data.columns:\n",
    "                    metadata['paragraph_number'] = row.get('paragraph_number')\n",
    "                if 'page_number' in data.columns:\n",
    "                    metadata['page_number'] = row.get('page_number')\n",
    "                if 'empirical_page_number' in data.columns:\n",
    "                    metadata['empirical_page_number'] = row.get('empirical_page_number')\n",
    "\n",
    "                # Always include description\n",
    "                metadata['description'] = row.get('description', '')\n",
    "\n",
    "                self.taxon_metadata.append(metadata)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse data from Redis: {e}\")\n",
    "\n",
    "        print(f\" Loaded {len(self.taxon_names)} taxa with {self.embeddings.shape[1]}-dimensional embeddings\")\n",
    "        \n",
    "        return self.embeddings, self.taxon_names, self.taxon_metadata\n",
    "\n",
    "    def store_in_neo4j(\n",
    "        self,\n",
    "        root_name: str = \"Fungi\",\n",
    "        clear_existing: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Store the clustering tree in Neo4j.\n",
    "\n",
    "        Creates:\n",
    "        - Taxon nodes (leaf nodes) with properties: name, node_id\n",
    "        - Pseudoclade nodes (internal nodes) with properties: name, node_id, count\n",
    "        - PARENT_OF relationships with property: distance (cosine similarity)\n",
    "\n",
    "        Args:\n",
    "            root_name: Name for the root pseudoclade\n",
    "            clear_existing: Whether to clear existing Taxon and Pseudoclade nodes\n",
    "        \"\"\"\n",
    "        if self.root_node is None:\n",
    "            raise ValueError(\"No clustering tree available. Call cluster() first.\")\n",
    "\n",
    "        print(f\"Storing tree in Neo4j...\")\n",
    "        print(f\"  Root name: {root_name}\")\n",
    "\n",
    "        with self.neo4j_driver.session() as session:\n",
    "            # Optionally clear existing data\n",
    "            if clear_existing:\n",
    "                print(\"  Clearing existing Taxon and Pseudoclade nodes...\")\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (n)\n",
    "                    WHERE n:Taxon OR n:Pseudoclade\n",
    "                    DETACH DELETE n\n",
    "                \"\"\")\n",
    "\n",
    "            # Create indexes for performance\n",
    "            session.run(\"CREATE INDEX taxon_node_id IF NOT EXISTS FOR (t:Taxon) ON (t.node_id)\")\n",
    "            session.run(\"CREATE INDEX pseudoclade_node_id IF NOT EXISTS FOR (p:Pseudoclade) ON (p.node_id)\")\n",
    "\n",
    "            # Store tree recursively\n",
    "            pseudoclade_counter = [0]  # Use list for mutability in nested function\n",
    "\n",
    "            def store_node(node: ClusterNode, parent_id: Optional[int] = None, is_root: bool = False):\n",
    "                \"\"\"Recursively store nodes in Neo4j.\"\"\"\n",
    "                if node.is_leaf:\n",
    "                    # Create Taxon node with metadata\n",
    "                    taxon_props = {\n",
    "                        'name': node.taxon_name,\n",
    "                        'node_id': node.node_id\n",
    "                    }\n",
    "\n",
    "                    # Add metadata fields if available\n",
    "                    if node.metadata:\n",
    "                        for key, value in node.metadata.items():\n",
    "                            # Convert values to Neo4j-compatible types\n",
    "                            if value is not None and not isinstance(value, (bool, int, float, str)):\n",
    "                                taxon_props[key] = str(value)\n",
    "                            else:\n",
    "                                taxon_props[key] = value\n",
    "\n",
    "                    session.run(\"\"\"\n",
    "                        CREATE (t:Taxon $props)\n",
    "                    \"\"\", props=taxon_props)\n",
    "\n",
    "                    # Create relationship to parent if exists\n",
    "                    if parent_id is not None:\n",
    "                        session.run(\"\"\"\n",
    "                            MATCH (parent:Pseudoclade {node_id: $parent_id})\n",
    "                            MATCH (child:Taxon {node_id: $child_id})\n",
    "                            CREATE (parent)-[:PARENT_OF {distance: $distance}]->(child)\n",
    "                        \"\"\", parent_id=parent_id, child_id=node.node_id, distance=node.distance)\n",
    "                else:\n",
    "                    # Create Pseudoclade node\n",
    "                    if is_root:\n",
    "                        pseudoclade_name = root_name\n",
    "                    else:\n",
    "                        pseudoclade_counter[0] += 1\n",
    "                        pseudoclade_name = f\"Pseudoclade_{pseudoclade_counter[0]}\"\n",
    "\n",
    "                    session.run(\"\"\"\n",
    "                        CREATE (p:Pseudoclade {\n",
    "                            name: $name,\n",
    "                            node_id: $node_id,\n",
    "                            count: $count\n",
    "                        })\n",
    "                    \"\"\", name=pseudoclade_name, node_id=node.node_id, count=node.count)\n",
    "\n",
    "                    # Create relationship to parent if exists\n",
    "                    if parent_id is not None:\n",
    "                        session.run(\"\"\"\n",
    "                            MATCH (parent:Pseudoclade {node_id: $parent_id})\n",
    "                            MATCH (child:Pseudoclade {node_id: $child_id})\n",
    "                            CREATE (parent)-[:PARENT_OF {distance: $distance}]->(child)\n",
    "                        \"\"\", parent_id=parent_id, child_id=node.node_id, distance=node.distance)\n",
    "\n",
    "                    # Recursively store children\n",
    "                    if node.left_child:\n",
    "                        store_node(node.left_child, node.node_id, False)\n",
    "                    if node.right_child:\n",
    "                        store_node(node.right_child, node.node_id, False)\n",
    "\n",
    "            # Start from root\n",
    "            store_node(self.root_node, None, True)\n",
    "\n",
    "        print(f\" Tree stored in Neo4j\")\n",
    "\n",
    "        # Print summary statistics\n",
    "        self._print_neo4j_stats()\n",
    "\n",
    "    def _print_neo4j_stats(self):\n",
    "        \"\"\"Print statistics about stored data.\"\"\"\n",
    "        with self.neo4j_driver.session() as session:\n",
    "            # Count taxa\n",
    "            result = session.run(\"MATCH (t:Taxon) RETURN count(t) as count\")\n",
    "            taxon_count = result.single()['count']\n",
    "\n",
    "            # Count pseudoclades\n",
    "            result = session.run(\"MATCH (p:Pseudoclade) RETURN count(p) as count\")\n",
    "            pseudoclade_count = result.single()['count']\n",
    "\n",
    "            # Count relationships\n",
    "            result = session.run(\"MATCH ()-[r:PARENT_OF]->() RETURN count(r) as count\")\n",
    "            relationship_count = result.single()['count']\n",
    "\n",
    "            print(f\"  Taxon nodes: {taxon_count}\")\n",
    "            print(f\"  Pseudoclade nodes: {pseudoclade_count}\")\n",
    "            print(f\"  PARENT_OF relationships: {relationship_count}\")\n",
    "\n",
    "    def get_subtree(self, clade_name: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get all taxa descendant from a given clade.\n",
    "\n",
    "        Args:\n",
    "            clade_name: Name of the clade (Pseudoclade or Taxon)\n",
    "\n",
    "        Returns:\n",
    "            List of taxon names in the subtree\n",
    "        \"\"\"\n",
    "        with self.neo4j_driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (root {name: $clade_name})-[:PARENT_OF*]->(t:Taxon)\n",
    "                RETURN t.name as taxon_name\n",
    "                ORDER BY taxon_name\n",
    "            \"\"\", clade_name=clade_name)\n",
    "\n",
    "            return [record['taxon_name'] for record in result]\n",
    "\n",
    "    def get_tree_path(self, taxon_name: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get the path from root to a specific taxon.\n",
    "\n",
    "        Args:\n",
    "            taxon_name: Name of the taxon\n",
    "\n",
    "        Returns:\n",
    "            List of (node_name, distance) tuples from root to taxon\n",
    "        \"\"\"\n",
    "        with self.neo4j_driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH path = (root:Pseudoclade)-[:PARENT_OF*]->(t:Taxon {name: $taxon_name})\n",
    "                WHERE NOT (root)<-[:PARENT_OF]-()\n",
    "                UNWIND nodes(path) as node\n",
    "                RETURN node.name as name,\n",
    "                       CASE WHEN node:Taxon THEN 0.0\n",
    "                            ELSE relationships(path)[0].distance\n",
    "                       END as distance\n",
    "            \"\"\", taxon_name=taxon_name)\n",
    "\n",
    "            return [(record['name'], record['distance']) for record in result]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f97734-e9d9-4f2e-96f0-3f298d4a93a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterer = TaxonClusterer(\n",
    "    redis_host=\"localhost\",\n",
    "    redis_port=6379,\n",
    "    redis_db=0,\n",
    "    neo4j_uri=neo4j_uri,\n",
    ")\n",
    "\n",
    "# Load embeddings from Redis\n",
    "(embeddings, taxon_names, metadata) = clusterer.load_embeddings(embedding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9004117-e280-448e-ab39-b7f4aa449c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26683a0-6879-44be-869a-a87748aac888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "linkage_matrix = clusterer.cluster(method=\"average\", metric=\"cosine\")\n",
    "\n",
    "# Store in Neo4j with root named \"Fungi\"\n",
    "clusterer.store_in_neo4j(root_name=\"Fungi\", clear_existing=True)\n",
    "\n",
    "print(\" Clustering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f1457-b1bf-4e02-9b0c-ce194f33969d",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* doi Foundation, \"DOI Citation Formatter HTTP API\", https://citation.doi.org/api-docs.html, accessed 2025-11-12.\n",
    "* Yang, Jie and Zhang, Yue and Li, Linwei and Li, Xingxuan, 2018, \"YEDDA: A Lightweight Collaborative Text Span Annotation Tool\", Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, http://aclweb.org/anthology/P18-4006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82412f-0fd7-4526-a949-3732c2b26624",
   "metadata": {},
   "source": [
    "## Appendix: On the use of an AI Coder\n",
    "\n",
    "Portions of this work were completed with the aid of Claude Code Pro. I wish to give a clarifying example of how I've used this very powerful tool, and reveal why I am comfortable with claiming authorship of the resulting code.\n",
    "\n",
    "For this project I needed results from an earlier class project in which a trio of students built and evaluated models for classifying paragraphs. The earlier work was built as a iPython Notebook, with many examples and inline code. Just copying the earlier notebook would have introduced many irrelevant details and would not further the overall project.\n",
    "\n",
    "I asked Claude Code to translate the notebook into a module that I could import. It did a pretty good job. Without being told, it made a submodule, extract the illustrative code as examples, wrote reasonable documentation and created packaging for the module.\n",
    "\n",
    "The skill level of the coding was roughly that of a highly disciplined average junior programmer. The architecture was simplistic and violated several design constraints such as DRY. I requested specific refactorings, such as asking for a group of functions to be converted into an object that shared duplicated parameters.\n",
    "\n",
    "The initial code used REST interfaces directly, and read all the data into a single machine, not using pyspark correctly. Through a series of refactorings, I asked that the code use appropriate libraries I named, and create correct udf functions to execute transformations in parallel.\n",
    "\n",
    "I walked the AI through creating an object that I could use to illustrate my use of redis and couchdb interfaces, while leaving the irrelevant details in a separate library.\n",
    "\n",
    "In short, I still have to understand good design principles. I have to be able to recognize where appropriate libraries were applicable. I still have to understand the frameworks I am working with.\n",
    "\n",
    "I now have a strong understanding of the difference between \"vibe coding\" and AI-assisted software engineering. In my first 4 hours with Claude Code, I was able to produce roughly 4 days' worth of professional-grade working code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
