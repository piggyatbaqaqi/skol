{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a626f7e-4127-4227-9906-dc08dd9135ce",
   "metadata": {},
   "source": [
    "# SKOL IV: All the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ae6b20-7abd-4ef6-b2cf-8d221a40d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/26 09:55:04 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 10.1.10.58 instead (on interface wlp130s0f0)\n",
      "25/11/26 09:55:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-86518068-3597-45c9-b4f7-2b944db6b6ce;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 207ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-86518068-3597-45c9-b4f7-2b944db6b6ce\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/8ms)\n",
      "25/11/26 09:55:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Spark context Web UI available at http://10.1.10.58:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1764150907465).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 21.0.9)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> :quit\n"
     ]
    }
   ],
   "source": [
    "bahir_package = 'org.apache.bahir:spark-sql-cloudant_2.12:2.4.0'\n",
    "!spark-shell --packages $bahir_package < /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9615b87-4962-47ca-b10f-98558713196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import pickle\n",
    "import requests\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# Be sure to get version 2: https://simple-repository.app.cern.ch/project/bibtexparser/2.0.0b8/description\n",
    "import bibtexparser\n",
    "import couchdb\n",
    "import feedparser\n",
    "import fitz # PyMuPDF\n",
    "\n",
    "import pandas as pd  # TODO(piggy): Remove this dependency in favor of pure pyspark DataFrames.\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, CountVectorizer, IDF, StringIndexer, VectorAssembler, IndexToString\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name, collect_list, regexp_extract, col, udf,\n",
    "    explode, trim, row_number, min, expr, concat, lit\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, BooleanType, IntegerType, MapType, NullType,\n",
    "    StringType, StructType, StructField \n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import redis\n",
    "import torch\n",
    "from uuid import uuid4\n",
    "\n",
    "# Local modules\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "parent_path = Path(parent_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from couchdb_file import CouchDBFile as CDBF\n",
    "from fileobj import FileObject\n",
    "from finder import parse_annotated, remove_interstitials\n",
    "import line\n",
    "from line import Line\n",
    "\n",
    "# Import the SKOL classifier jupyter/ist769_skol.ipynb\n",
    "from skol_classifier import SkolClassifier as SC, get_file_list\n",
    "from skol_classifier.preprocessing import SuffixTransformer, ParagraphExtractor\n",
    "from skol_classifier.utils import calculate_stats\n",
    "\n",
    "from taxon import group_paragraphs, Taxon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9570ec3-8bb3-41af-99df-d96907293a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: cloudant.protocol\n",
      "Warning: Ignoring non-Spark config property: cloudant.password\n",
      "Warning: Ignoring non-Spark config property: cloudant.host\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    }
   ],
   "source": [
    "couchdb_host = \"127.0.0.1:5984\" # e.g., \"ACCOUNT.cloudant.com\" or \"localhost\"\n",
    "couchdb_username = \"admin\"\n",
    "couchdb_password = \"SU2orange!\"\n",
    "ingest_db_name = \"skol_dev\"\n",
    "taxon_db_name = \"skol_taxa_dev\"\n",
    "\n",
    "couchdb_url = f'http://{couchdb_host}'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CouchDB Spark SQL Example in Python using dataframes\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"cloudant.protocol\", \"http\") \\\n",
    "    .config(\"cloudant.host\", couchdb_host) \\\n",
    "    .config(\"cloudant.username\", couchdb_username) \\\n",
    "    .config(\"cloudant.password\", couchdb_password) \\\n",
    "    .config(\"spark.jars.packages\", bahir_package) \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .config(\"spark.submit.pyFiles\",\n",
    "            f'{parent_path / \"line.py\"},{parent_path / \"fileobj.py\"},'\n",
    "            f'{parent_path / \"couchdb_file.py\"},{parent_path / \"finder.py\"},'\n",
    "            f'{parent_path / \"taxon.py\"},{parent_path / \"paragraph.py\"},'\n",
    "            f'{parent_path / \"label.py\"},{parent_path / \"file.py\"},'\n",
    "            f'{parent_path / \"extract_taxa_to_couchdb.py\"}'\n",
    "           ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!\n",
    "\n",
    "couch = couchdb.Server(couchdb_url)\n",
    "couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "\n",
    "if ingest_db_name not in couch:\n",
    "    db = couch.create(ingest_db_name)\n",
    "else:\n",
    "    db = couch[ingest_db_name]\n",
    "\n",
    "user_agent = \"synoptickeyof.life\"\n",
    "\n",
    "ingenta_rp = RobotFileParser()\n",
    "ingenta_rp.set_url(\"https://www.ingentaconnect.com/robots.txt\")\n",
    "ingenta_rp.read() # Reads and parses the robots.txt file from the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e7f93-bb66-4922-b3f9-c9429bc304ac",
   "metadata": {},
   "source": [
    "## The Data Sources\n",
    "\n",
    "The goal is to collect all the open access taxonomic literature in Mycology. Most of the sources below mainly cover macro-fungi and slime molds.\n",
    "\n",
    "### Ingested Data Sources\n",
    "\n",
    "* [Mycotaxon at Ingenta Connect](https://www.ingentaconnect.com/content/mtax/mt)\n",
    "* [Studies in Mycology at Ingenta Connect](https://www.studiesinmycology.org/)\n",
    "\n",
    "### Source of many older public domain and open access works\n",
    "\n",
    "Mycoweb includes scans of many older works in mycology. I have local copies but need to ingest them.\n",
    "\n",
    "* [Mycoweb](https://mykoweb.com/)\n",
    "\n",
    "### Journals in hand\n",
    "\n",
    "These are journals we've collected over the years. The initial annotated issues are from early years of Mycotaxon. We still need to ingest all of these.\n",
    "\n",
    "* Mycologia (back issues)\n",
    "* [Mycologia at Taylor and Francis](https://www.tandfonline.com/journals/umyc20)\n",
    "  Mycologia is the main journal of the Mycological Society of America. It is a mix of open access and traditional access articles. The connector for this journal will need to identify the open access articles.\n",
    "* Persoonia (all issues)\n",
    "  Persoonia is no longer published.\n",
    "* Mycotaxon (back issues)\n",
    "  Mycotaxon is no longer published.\n",
    "\n",
    "### Journals that need connectors\n",
    "\n",
    "These are journals we're aware that include open access articles.\n",
    "\n",
    "* [Amanitaceae.org](http://www.tullabs.com/amanita/?home)\n",
    "* [Mycosphere](https://mycosphere.org/)\n",
    "* [Mycoscience](https://mycoscience.org/)\n",
    "* [Journal of Fungi](https://www.mdpi.com/journal/jof)\n",
    "* [Mycology](https://www.tandfonline.com/journals/tmyc20)\n",
    "* [Open Access Journal of Mycology & Mycological Sciences](https://www.medwinpublishers.com/OAJMMS/)\n",
    "* [Mycokeys](https://mycokeys.pensoft.net/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a4213-adb5-49e5-b973-570a75cc2cce",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Each journal or other data source gets an ingester that puts PDFs into our document store along with any metadata we can collect. The metadata is sufficient to create citations for each issue, book, or article. If bibtex citations are available we prefer to store these verbatim.\n",
    "\n",
    "### Ingenta RSS ingestion\n",
    "\n",
    "Ingenta Connect is an electronic publisher that holds two Mycology journals. New articles are available via RSS (Really Simple Syndication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d303cc7-f01c-4e15-87ff-6c259d010591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_bibtex(\n",
    "        db: couchdb.Database,\n",
    "        content: bytes,\n",
    "        bibtex_link: str,\n",
    "        meta: Dict[str, Any],\n",
    "        rp\n",
    "        ) -> None:\n",
    "    \"\"\"Load documents referenced in an Ingenta BibTeX database.\"\"\"\n",
    "    bib_database = bibtexparser.parse_string(content)\n",
    "\n",
    "    bibtex_data = {\n",
    "        'link': bibtex_link,\n",
    "        'bibtex': bibtexparser.write_string(bib_database),\n",
    "    }\n",
    "    \n",
    "    for bib_entry in bib_database.entries:\n",
    "        doc = {\n",
    "            '_id': uuid4().hex,\n",
    "            'meta': meta,\n",
    "            'pdf_url': f\"{bib_entry['url']}?crawler=true\",\n",
    "        }\n",
    "\n",
    "        # Do not fetch if we already have an entry.\n",
    "        selector = {'selector': {'pdf_url': doc['pdf_url']}}\n",
    "        found = False\n",
    "        for e in db.find(selector):\n",
    "            found = True\n",
    "        if found:\n",
    "            print(f\"Skipping {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        if not rp.can_fetch(user_agent, doc['pdf_url']):\n",
    "            # TODO(piggy): We should probably record blocked URLs.\n",
    "            print(f\"Robot permission denied {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Adding {doc['pdf_url']}\")\n",
    "        for k in bib_entry.fields_dict.keys():\n",
    "            doc[k] = bib_entry[k]\n",
    "        \n",
    "        doc_id, doc_rev = db.save(doc)\n",
    "        with requests.get(doc['pdf_url'], stream=False) as pdf_f:\n",
    "            pdf_f.raise_for_status()\n",
    "            pdf_doc = pdf_f.content\n",
    "        \n",
    "        attachment_filename = 'article.pdf'\n",
    "        attachment_content_type = 'application/pdf'\n",
    "        attachment_file = BytesIO(pdf_doc)\n",
    "\n",
    "        db.put_attachment(doc, attachment_file, attachment_filename, attachment_content_type)\n",
    "\n",
    "        print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b323f98-5bd8-4106-b79c-2ff9ac1ac74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_ingenta(\n",
    "        db: couchdb.Database,\n",
    "        rss_url: str,\n",
    "        rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest documents from an Ingenta RSS feed.\"\"\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    \n",
    "    feed_meta = {\n",
    "        'url': rss_url,\n",
    "        'title': feed.feed.title,\n",
    "        'link': feed.feed.link,\n",
    "        'description': feed.feed.description,\n",
    "    }\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        entry_meta = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "        }\n",
    "        if hasattr(entry, 'summary'):\n",
    "            entry_meta['summary'] = entry.summary\n",
    "        if hasattr(entry, 'description'):\n",
    "            entry_meta['description'] = entry.description\n",
    "\n",
    "        bibtex_link = f'{entry.link}?format=bib'\n",
    "        print(f\"bibtex_link: {bibtex_link}\")\n",
    "\n",
    "        if not rp.can_fetch(user_agent, bibtex_link):\n",
    "            print(f\"Robot permission denied {bibtex_link}\")\n",
    "            continue\n",
    "\n",
    "        with requests.get(bibtex_link, stream=False) as bibtex_f:\n",
    "            bibtex_f.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            ingest_from_bibtex(\n",
    "                db=db,\n",
    "                content=bibtex_f.content\\\n",
    "                    .replace(b\"\\\"\\nparent\", b\"\\\",\\nparent\")\\\n",
    "                    .replace(b\"\\n\", b\"\"),\n",
    "                bibtex_link=bibtex_link,\n",
    "                meta={\n",
    "                    'feed': feed_meta,\n",
    "                    'entry': entry_meta,\n",
    "                },\n",
    "                rp=rp\n",
    "            )\n",
    "        print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234f9cc5-9b7e-4559-ad10-f24ad269e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_local_bibtex(\n",
    "    db: couchdb.Database,\n",
    "    root: Path,\n",
    "    rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest from a local directory with Ingenta bibtext files in it.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if not filename.endswith('format=bib'):\n",
    "                continue\n",
    "            full_filepath = os.path.join(dirpath, filename)\n",
    "            bibtex_link = f\"https://www.ingentaconnect.com/{full_filepath[len(str(root)):]}\"\n",
    "            with open(full_filepath) as f:\n",
    "                content = f.read()\\\n",
    "                    .replace(\"\\\"\\nparent\", \"\\\",\\nparent\")\\\n",
    "                    .replace(\"\\n\", \"\")\n",
    "                ingest_from_bibtex(db, content, bibtex_link, meta={}, rp=rp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58829ec0-93f7-44dd-87bc-511ab2d238d0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mycotaxon\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/mtax/mt?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40602cc3-e231-497d-adc7-d6d4471514cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Studies in Mycology\n",
    "\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/wfbi/sim?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "651f82ea-b3b8-4f0d-adf0-f5550d7ed6d5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ingest_from_local_bibtex(\n",
    "    db=db,\n",
    "    root=Path(\"/data/skol/www/www.ingentaconnect.com\"),\n",
    "    rp=ingenta_rp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf05ad-d2bf-4716-b253-07455cfefcd0",
   "metadata": {},
   "source": [
    "Download the RSS\n",
    "\n",
    "Read bibtex files and create records for each article.\n",
    "\n",
    "Download the PDFs at the URLs in the bibtex entries.\n",
    "\n",
    "Create a JSON record with the PDF as an attachment.\n",
    "\n",
    "### Text extraction\n",
    "\n",
    "We extract the text, optionally with OCR. Add as an additional attachment on the source record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5e9725-0c3a-4c41-8ebf-2929c4dc4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\n",
    "    format=\"org.apache.bahir.cloudant\",\n",
    "    database=ingest_db_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e75c64e-95cb-478a-9a85-46d1297e4225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _id: string, _rev: string, abstract: string, author: string, doi: string, eissn: string, issn: string, itemtype: string, journal: string, number: string, pages: string, parent_itemid: string, pdf_url: string, publication date: string, publishercode: string, title: string, url: string, volume: string, year: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9befa546-a895-4a13-8744-d512587d9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Type: text/html; charset=UTF-8\n",
    "\n",
    "def pdf_to_text(pdf_contents: bytes) -> bytes:\n",
    "    doc = fitz.open(stream=BytesIO(pdf_contents), filetype=\"pdf\")\n",
    "\n",
    "    full_text = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        # Possibly perform OCR on the page\n",
    "        text = page.get_text(\"text\", flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_DEHYPHENATE)\n",
    "        # full_text += f\"\\n--- PDF Page {page_num+1} ---\\n\"\n",
    "        full_text += text\n",
    "\n",
    "    return full_text.encode(\"utf-8\")\n",
    "\n",
    "def add_text_to_partition(iterator) -> None:\n",
    "    couch = couchdb.Server(couchdb_url)\n",
    "    couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "    local_db = couch[ingest_db_name]\n",
    "    for row in iterator:\n",
    "        if not row:\n",
    "            continue\n",
    "        if not row._attachments:\n",
    "            continue\n",
    "        row_dict = row.asDict()\n",
    "        attachment_dict = row._attachments.asDict()\n",
    "        for pdf_filename in attachment_dict:\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            if pdf_path.suffix != '.pdf':\n",
    "                continue\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            txt_path_str = pdf_path.stem + '.txt'\n",
    "            # if txt_path_str in attachment_dict:\n",
    "            #     # TODO(piggy): Recalculate text if text is terrible. Too much noise vocabulary?\n",
    "            #     print(f\"Already have text for {row.pdf_url}\")\n",
    "            #     continue\n",
    "            print(f\"{row._id}, {row.pdf_url}\")\n",
    "            pdf_file = local_db.get_attachment(row._id, str(pdf_path)).read()\n",
    "            txt_file = pdf_to_text(pdf_file)\n",
    "            attachment_content_type = 'text/simple; charset=UTF-8'\n",
    "            attachment_file = BytesIO(txt_file)\n",
    "            local_db.put_attachment(row_dict, attachment_file, txt_path_str, attachment_content_type)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a51014-ee58-449b-8ce4-8d0a986e68c1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df.select(\"*\").foreachPartition(add_text_to_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11805c62-ae42-4959-9673-1e09261a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to skol_classifier.CouchDBConnection.\n",
    "from skol_classifier import CouchDBConnection as CDBC\n",
    "\n",
    "class CouchDBConnection(CDBC):\n",
    "    \"\"\"\n",
    "    Manages CouchDB connection and provides I/O operations.\n",
    "\n",
    "    This class encapsulates connection parameters and provides an idempotent\n",
    "    connection method that can be safely called multiple times.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08a0fa58-e693-4978-b33e-67010e066948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main classifier module for SKOL text classification\n",
    "\"\"\"\n",
    "class SkolClassifier(SC):\n",
    "    \"\"\"\n",
    "    Text classifier for taxonomic literature.\n",
    "\n",
    "    This version only includes the redis and couchdb I/O methods.\n",
    "    All other methods are in SC.\n",
    "\n",
    "    Supports multiple classification models (Logistic Regression, Random Forest)\n",
    "    and feature types (word TF-IDF, suffix TF-IDF, combined).\n",
    "    \"\"\"\n",
    "\n",
    "    # def save_to_redis(self) -> bool:\n",
    "\n",
    "    # def load_from_redis(self) -> bool:\n",
    "    # def load_from_couchdb(self, pattern: str = \"*.txt\") -> DataFrame:\n",
    "\n",
    "    # def predict_from_couchdb(\n",
    "\n",
    "    # def save_to_couchdb(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab2d58-0ea7-4288-889c-b1bf9c360743",
   "metadata": {},
   "source": [
    "## Build a classifier to identify paragraph types.\n",
    "\n",
    "We save this to redis so that we don't need to train the model every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "m4m45o9n97i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping generation of model skol:classifier:model:v1.2.\n"
     ]
    }
   ],
   "source": [
    "# Train classifier on annotated data and save to Redis\n",
    "# Connect to Redis\n",
    "redis_client = redis.Redis(\n",
    "    host='localhost',\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=False\n",
    ")\n",
    "classifier_model_name = \"skol:classifier:model:v1.2\"\n",
    "\n",
    "if not redis_client.exists(classifier_model_name):\n",
    "\n",
    "    # Initialize classifier with Redis connection\n",
    "    classifier = SkolClassifier(\n",
    "        spark=spark,\n",
    "        redis_client=redis_client,\n",
    "        redis_key=classifier_model_name,\n",
    "        auto_load=False,  # Don't auto-load, we want to train fresh\n",
    "        couchdb_url=couchdb_url,\n",
    "        database=ingest_db_name,\n",
    "        username=couchdb_username,\n",
    "        password=couchdb_password\n",
    "    )\n",
    "    \n",
    "    # Get annotated training files\n",
    "    annotated_path = Path.cwd().parent / \"data\" / \"annotated\"\n",
    "    print(f\"Loading annotated files from: {annotated_path}\")\n",
    "    \n",
    "    if annotated_path.exists():\n",
    "        annotated_files = get_file_list(str(annotated_path), pattern=\"**/*.ann\")\n",
    "        \n",
    "        if len(annotated_files) > 0:\n",
    "            print(f\"Found {len(annotated_files)} annotated files\")\n",
    "            \n",
    "            # Train the classifier\n",
    "            print(\"Training classifier...\")\n",
    "            results = classifier.fit(\n",
    "                annotated_files,\n",
    "                model_type='logistic',\n",
    "                use_suffixes=False,\n",
    "                line_level=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Training complete!\")\n",
    "            print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "            print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
    "            print(f\"  Labels: {classifier.labels}\")\n",
    "            \n",
    "            # Save model to Redis\n",
    "            print(\"\\nSaving model to Redis...\")\n",
    "            if classifier.save_to_redis():\n",
    "                print(f\"✓ Model successfully saved to Redis with key: {classifier_model_name}.\")\n",
    "            else:\n",
    "                print(\"✗ Failed to save model to Redis\")\n",
    "        else:\n",
    "            print(f\"No annotated files found in {annotated_path}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {annotated_path}\")\n",
    "        print(\"Please ensure annotated training data is available.\")\n",
    "else:\n",
    "    print(f\"Skipping generation of model {classifier_model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fe9eb-1f1b-4aa8-9796-169faeeaf432",
   "metadata": {},
   "source": [
    "## Extract the taxa names and descriptions\n",
    "\n",
    "We use a classifier to extract taxa names and descriptions from articles, issues, and books. The YEDDA annotated texts are written back to CouchDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd6b1621-bb26-4cf1-84cb-3b74453741aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with labels: ['Misc-exposition', 'Description', 'Nomenclature']\n",
      "\n",
      "Loading and classifying documents from CouchDB...\n",
      "\n",
      "Sample predictions:\n",
      "+--------------------------------+---------------+---------------+\n",
      "|                          doc_id|attachment_name|predicted_label|\n",
      "+--------------------------------+---------------+---------------+\n",
      "|0020c88329ed456a95a18e0c219269f4|    article.txt|Misc-exposition|\n",
      "|0020c88329ed456a95a18e0c219269f4|    article.txt|Misc-exposition|\n",
      "|0020c88329ed456a95a18e0c219269f4|    article.txt|Misc-exposition|\n",
      "|0020c88329ed456a95a18e0c219269f4|    article.txt|Misc-exposition|\n",
      "|0020c88329ed456a95a18e0c219269f4|    article.txt|Misc-exposition|\n",
      "+--------------------------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Saving predictions back to CouchDB...\n",
      "(Each partition writes its documents using a single connection)\n",
      "\n",
      "Saved 2099 annotated files to CouchDB\n"
     ]
    }
   ],
   "source": [
    "classifier = SkolClassifier(\n",
    "    spark=spark,\n",
    "    redis_client=redis_client,\n",
    "    redis_key=classifier_model_name,\n",
    "    auto_load=True,\n",
    "    couchdb_url=couchdb_url,\n",
    "    database=ingest_db_name,\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password\n",
    ")\n",
    "\n",
    "if classifier.labels is None:\n",
    "    raise ValueError(\"No model found in Redis. Please train a model first.\")\n",
    "\n",
    "print(f\"Model loaded with labels: {classifier.labels}\")\n",
    "\n",
    "print(\"\\nLoading and classifying documents from CouchDB...\")\n",
    "predictions = classifier.predict_from_couchdb(pattern=\"*.txt\", line_level=True)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "predictions.select(\n",
    "    \"doc_id\", \"attachment_name\", \"predicted_label\"\n",
    ").show(5, truncate=50)\n",
    "\n",
    "# Save results back to CouchDB using distributed writes\n",
    "print(\"\\nSaving predictions back to CouchDB...\")\n",
    "print(\"(Each partition writes its documents using a single connection)\")\n",
    "results = classifier.save_to_couchdb(predictions=predictions, suffix=\".ann\", coalesce_labels=True)\n",
    "\n",
    "# Report results\n",
    "successful = sum(1 for r in results if r['success'])\n",
    "failed = len(results) - successful\n",
    "\n",
    "print(f\"\\nSaved {successful} annotated files to CouchDB\")\n",
    "if failed > 0:\n",
    "    print(f\"Failed to save {failed} files\")\n",
    "    for r in results:\n",
    "        if not r['success']:\n",
    "            print(f\"  {r['doc_id']}/{r['attachment_name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ecd132-4946-47cf-a67b-364335eb768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|predicted_label|        annotated_pg|\n",
      "+---------------+--------------------+\n",
      "|   Nomenclature|[@ 2. Caloplaca b...|\n",
      "|   Nomenclature|[@ 5. Caloplaca g...|\n",
      "|   Nomenclature|[@ Handlingar, Sj...|\n",
      "|   Nomenclature|[@ 7. Caloplaca l...|\n",
      "|   Nomenclature|[@ nom. nud.#Nome...|\n",
      "|   Nomenclature|[@ wiss. Kl. Wien...|\n",
      "|   Nomenclature|[@ Boom & Etayo (...|\n",
      "|   Nomenclature|[@ 13. Caloplaca ...|\n",
      "|   Nomenclature|[@ 14. Caloplaca ...|\n",
      "|   Nomenclature|[@ 16. Gasparrini...|\n",
      "|   Nomenclature|[@ Iran, 1937. An...|\n",
      "|   Nomenclature|[@ Aegeischen Mee...|\n",
      "|   Nomenclature|[@ Szatala Ö. 195...|\n",
      "|   Nomenclature|[@ Marasmius pseu...|\n",
      "|   Nomenclature|[@ Rattania Prabh...|\n",
      "|   Nomenclature|[@ M. acerina (R....|\n",
      "|   Nomenclature|[@ Rattania setul...|\n",
      "|   Nomenclature|[@ Masseeella flu...|\n",
      "|   Nomenclature|[@ 70. 1907.\\t#No...|\n",
      "|   Nomenclature|[@ Phellodon tome...|\n",
      "+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-------+\n",
      "|predicted_label|  count|\n",
      "+---------------+-------+\n",
      "|   Nomenclature|  13277|\n",
      "|    Description|  99375|\n",
      "|Misc-exposition|1048144|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"predicted_label\", \"annotated_pg\").where('predicted_label = \"Nomenclature\"').show()\n",
    "predictions.groupBy(\"predicted_label\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002ae67-9dc9-4335-adf4-d249905d2ad9",
   "metadata": {},
   "source": [
    "Here we estimate an upper bound for the Taxon structures we'd like to find. The abbreviation \"nov.\" (\"novum\") indicates a new taxon in the current article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db747262-98fb-4166-abb0-e55966e538c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4014"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"*\").filter(col(\"annotated_pg\").contains(\"nov.\")).where(\"predicted_label = 'Nomenclature'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca929b6a-75e5-4dcb-ace2-8d420f42ce41",
   "metadata": {},
   "source": [
    "## Build the Taxon objects and store them in CouchDB\n",
    "We use CouchDB to store a full record for each taxon. We copy all metadata to the taxon records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e285b7-0afa-486e-94f6-41bcb1d7524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDBFile(CDBF):\n",
    "    \"\"\"\n",
    "    File-like object that reads from CouchDB attachment content.\n",
    "\n",
    "    This class extends FileObject to support reading text from CouchDB\n",
    "    attachments while preserving database metadata (doc_id, attachment_name,\n",
    "    and database name).\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3de964f-332f-470e-a32e-ffef1a338047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from couchdb_file import read_couchdb_partition, read_couchdb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7201b4fb-3b89-414e-b0d7-d62367bd05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_taxa_to_couchdb import (\n",
    "    generate_taxon_doc_id,\n",
    "    extract_taxa_from_partition,\n",
    "    convert_taxa_to_rows,\n",
    "    extract_and_save_taxa_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22648996-4d9f-47f1-ab4f-6b5fd54f97e9",
   "metadata": {},
   "source": [
    "## Build Taxon objects\n",
    "\n",
    "Here we extract the Taxon objects from the annotated attachments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08069896-722b-45c7-8b95-78cbb89e8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_couchdb_url = couchdb_url\n",
    "ingest_username = couchdb_username\n",
    "ingest_password = couchdb_password\n",
    "taxon_couchdb_url = couchdb_url\n",
    "taxon_username = couchdb_username\n",
    "taxon_password = couchdb_password\n",
    "pattern = '*.txt.ann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bc7e0-884c-444f-81b3-a658b87c0f38",
   "metadata": {},
   "outputs": [],
   "source": "from extract_taxa_to_couchdb import TaxonExtractor, extract_taxa_from_partition"
  },
  {
   "cell_type": "markdown",
   "id": "33c1d6c7-e5a2-479f-9f79-194c146c1952",
   "metadata": {},
   "source": [
    "# START REFACTOR HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed264e3-3e3c-4c32-bb99-2205edaa4bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# Create TaxonExtractor instance with database configuration\nextractor = TaxonExtractor(\n    spark=spark,\n    ingest_couchdb_url=ingest_couchdb_url,\n    ingest_db_name=ingest_db_name,\n    taxon_db_name=taxon_db_name,\n    ingest_username=ingest_username,\n    ingest_password=ingest_password,\n    taxon_username=taxon_username,\n    taxon_password=taxon_password\n)\n\nprint(\"TaxonExtractor initialized\")\nprint(f\"  Ingest DB: {ingest_db_name}\")\nprint(f\"  Taxon DB:  {taxon_db_name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649df24-6575-4c25-8f07-687f8cf60710",
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Load annotated documents\nprint(\"\\nStep 1: Loading annotated documents from CouchDB...\")\nannotated_df = extractor.load_annotated_documents(pattern='*.txt.ann')\nprint(f\"Loaded {annotated_df.count()} annotated documents\")\nannotated_df.show(5, truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae516e78-3faf-4f43-8ff3-7dcca348355a",
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Extract taxa to DataFrame\nprint(\"\\nStep 2: Extracting taxa from annotated documents...\")\ntaxa_df = extractor.extract_taxa(annotated_df)\nprint(f\"Extracted {taxa_df.count()} taxa\")\ntaxa_df.printSchema()\ntaxa_df.show(10, truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7eac7-fefa-4664-a4be-c66f7e84f674",
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Inspect actual Taxon objects from the RDD (optional debugging)\nprint(\"\\n=== Sample Taxon Objects ===\")\ntaxa_rdd = annotated_df.rdd.mapPartitions(\n    lambda partition: extract_taxa_from_partition(iter(partition), ingest_db_name)  # type: ignore[reportUnknownArgumentType]\n)\nfor i, taxon in enumerate(taxa_rdd.take(3)):\n    print(f\"\\nTaxon {i+1}:\")\n    print(f\"  Type: {type(taxon)}\")\n    print(f\"  Has nomenclature: {taxon.has_nomenclature()}\")\n    taxon_row = taxon.as_row()\n    print(f\"  Taxon name: {taxon_row['taxon'][:80]}...\")\n    print(f\"  Source: {taxon_row['source']}\")"
  },
  {
   "cell_type": "code",
   "id": "0t94fhxca6tp",
   "source": "# Step 4: Save taxa to CouchDB\nprint(\"\\nStep 4: Saving taxa to CouchDB...\")\nresults_df = extractor.save_taxa(taxa_df)\n\n# Show results summary\nsuccessful = results_df.filter(\"success = true\").count()\nfailed = results_df.filter(\"success = false\").count()\n\nprint(f\"\\nResults:\")\nprint(f\"  Successful: {successful}\")\nprint(f\"  Failed:     {failed}\")\n\n# Show detailed results\nresults_df.groupBy(\"success\").count().show(truncate=False)\n\n# If there are failures, show error messages\nif failed > 0:\n    print(\"\\nError messages:\")\n    results_df.filter(\"success = false\").select(\"error_message\").distinct().show(truncate=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ewsk11g9lpl",
   "source": "# Alternative: Run the complete pipeline in one step\n# Uncomment to use the simplified one-step approach:\n\n# print(\"\\nRunning complete pipeline...\")\n# results = extractor.run_pipeline(pattern='*.txt.ann')\n#\n# successful = results.filter(\"success = true\").count()\n# failed = results.filter(\"success = false\").count()\n#\n# print(f\"\\nPipeline Results:\")\n# print(f\"  Successful: {successful}\")\n# print(f\"  Failed:     {failed}\")\n#\n# results.groupBy(\"success\").count().show(truncate=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c95ee719-eb73-425b-ad44-6f30158c8d5e",
   "metadata": {},
   "source": [
    "## Dr. Drafts document embedding\n",
    "\n",
    "Dr. Drafts loads documents from CouchDB. Save the embedding to redis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c550d-15f4-426c-b11e-d7ac6691e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dr_drafts_mycosearch.data import SKOL_TAXA as STX\n",
    "from dr_drafts_mycosearch.compute_embeddings import EmbeddingsComputer as EC\n",
    "\n",
    "class SKOL_TAXA(STX):\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load taxon data from CouchDB into a pandas DataFrame.\"\"\"\n",
    "        # TODO(piggy): Convert whole package to pyspark DataFrame for better scaling.\n",
    "        # Connect to CouchDB\n",
    "        server = couchdb.Server(self.couchdb_url)\n",
    "        if self.username and self.password:\n",
    "            server.resource.credentials = (self.username, self.password)\n",
    "\n",
    "        # Access the database\n",
    "        if self.db_name not in server:\n",
    "            raise ValueError(f\"Database '{self.db_name}' not found in CouchDB server\")\n",
    "\n",
    "        db = server[self.db_name]\n",
    "\n",
    "        # Fetch all documents from the database\n",
    "        records = []\n",
    "        for doc_id in db:\n",
    "            # Skip design documents\n",
    "            if doc_id.startswith('_design/'):\n",
    "                continue\n",
    "\n",
    "            doc = db[doc_id]\n",
    "            records.append(doc)\n",
    "\n",
    "        if not records:\n",
    "            # Create empty DataFrame if no records found\n",
    "            self.df = pd.DataFrame()\n",
    "            print(f\"Warning: No taxon records found in database '{self.db_name}'\")\n",
    "            return\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        self.df = pd.DataFrame(records)\n",
    "        print(f\"Loaded {len(self.df)} taxon records from CouchDB database '{self.db_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a79b3c-4ac4-4208-9346-0a5bc90e0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsComputer(EC):\n",
    "    \"\"\"Class for computing and storing embeddings from narrative data.\"\"\"\n",
    "\n",
    "    def write_embeddings_to_redis(self):\n",
    "        \"\"\"Write embeddings to Redis using instance configuration.\"\"\"\n",
    "        import redis\n",
    "\n",
    "        if self.redis_username and self.redis_password:\n",
    "            r = redis.from_url(self.redis_url, username=self.redis_username, password=self.redis_password, db=self.redis_db)\n",
    "        else:\n",
    "            r = redis.from_url(self.redis_url, db=self.redis_db)\n",
    "\n",
    "        pickled_data = pickle.dumps(self.result)\n",
    "        r.set(self.embedding_name, pickled_data)\n",
    "        print(f'Embeddings written to Redis (db={self.redis_db}) with key: {self.embedding_name}')\n",
    "\n",
    "    def run(self, descriptions):\n",
    "        \"\"\"Run the full embeddings computation pipeline.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The computed embeddings\n",
    "        \"\"\"\n",
    "        df = descriptions.drop_duplicates(\n",
    "            subset=['description'],\n",
    "            keep='last',\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            print('Warning: No GPU detected. Using CPU.')\n",
    "\n",
    "        embeddings = self.encode_narratives(df.description.astype(str))\n",
    "        self.result = pd.concat([df, embeddings], axis=1)\n",
    "\n",
    "        # Write to Redis if embedding name is specified\n",
    "        if self.embedding_name:\n",
    "            if not self.redis_url:\n",
    "                raise ValueError(\"redis_url must be provided when embedding_name is specified\")\n",
    "            self.write_embeddings_to_redis()\n",
    "        else:\n",
    "            # Write to local filesystem\n",
    "            self.write_embeddings_to_file()\n",
    "\n",
    "        return self.result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345caed-a301-49ca-a182-ce0ace7e839b",
   "metadata": {},
   "source": [
    "## Compute Embeddings\n",
    "\n",
    "We use SBERT to embed the taxa into a search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8060c0d8-53d6-4501-a312-53522c8ae25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skol_taxa = SKOL_TAXA(\n",
    "    couchdb_url=\"http://localhost:5984\",\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password,\n",
    "    db_name=taxon_db_name\n",
    ")\n",
    "descriptions = skol_taxa.get_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383fbb75-4f00-4cea-b557-fe2cbafbc1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = EmbeddingsComputer(\n",
    "    idir='/dev/null',\n",
    "    redis_url='redis://localhost:6379',\n",
    "    embedding_name='skol:embedding:v0.1',\n",
    ")\n",
    "\n",
    "embedding_result = embedder.run(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f1457-b1bf-4e02-9b0c-ce194f33969d",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* doi Foundation, \"DOI Citation Formatter HTTP API\", https://citation.doi.org/api-docs.html, accessed 2025-11-12.\n",
    "* Yang, Jie and Zhang, Yue and Li, Linwei and Li, Xingxuan, 2018, \"YEDDA: A Lightweight Collaborative Text Span Annotation Tool\", Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, http://aclweb.org/anthology/P18-4006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82412f-0fd7-4526-a949-3732c2b26624",
   "metadata": {},
   "source": [
    "## Appendix: On the use of an AI Coder\n",
    "\n",
    "Portions of this work were completed with the aid of Claude Code Pro. I wish to give a clarifying example of how I've used this very powerful tool, and reveal why I am comfortable with claiming authorship of the resulting code.\n",
    "\n",
    "For this project I needed results from an earlier class project in which a trio of students built and evaluated models for classifying paragraphs. The earlier work was built as a iPython Notebook, with many examples and inline code. Just copying the earlier notebook would have introduced many irrelevant details and would not further the overall project.\n",
    "\n",
    "I asked Claude Code to translate the notebook into a module that I could import. It did a pretty good job. Without being told, it made a submodule, extract the illustrative code as examples, wrote reasonable documentation and created packaging for the module.\n",
    "\n",
    "The skill level of the coding was roughly that of a highly disciplined average junior programmer. The architecture was simplistic and violated several design constraints such as DRY. I requested specific refactorings, such as asking for a group of functions to be converted into an object that shared duplicated parameters.\n",
    "\n",
    "The initial code used REST interfaces directly, and read all the data into a single machine, not using pyspark correctly. Through a series of refactorings, I asked that the code use appropriate libraries I named, and create correct udf functions to execute transformations in parallel.\n",
    "\n",
    "I walked the AI through creating an object that I could use to illustrate my use of redis and couchdb interfaces, while leaving the irrelevant details in a separate library.\n",
    "\n",
    "In short, I still have to understand good design principles. I have to be able to recognize where appropriate libraries were applicable. I still have to understand the frameworks I am working with.\n",
    "\n",
    "I now have a strong understanding of the difference between \"vibe coding\" and AI-assisted software engineering. In my first 4 hours with Claude Code, I was able to produce roughly 4 days' worth of professional-grade working code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}