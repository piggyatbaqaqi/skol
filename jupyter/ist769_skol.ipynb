{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a626f7e-4127-4227-9906-dc08dd9135ce",
   "metadata": {},
   "source": [
    "# SKOL IV: All the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ae6b20-7abd-4ef6-b2cf-8d221a40d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/16 23:48:58 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 172.16.227.68 instead (on interface wlp130s0f0)\n",
      "25/11/16 23:48:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8fb8350f-3fcb-45b7-9a56-19eab6e17792;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 197ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8fb8350f-3fcb-45b7-9a56-19eab6e17792\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/6ms)\n",
      "25/11/16 23:48:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Spark context Web UI available at http://172.16.227.68:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1763336942133).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 21.0.8)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> :quit\n"
     ]
    }
   ],
   "source": [
    "bahir_package = 'org.apache.bahir:spark-sql-cloudant_2.12:2.4.0'\n",
    "!spark-shell --packages $bahir_package < /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9615b87-4962-47ca-b10f-98558713196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import requests\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# Be sure to get version 2: https://simple-repository.app.cern.ch/project/bibtexparser/2.0.0b8/description\n",
    "import bibtexparser\n",
    "import couchdb\n",
    "import feedparser\n",
    "import fitz # PyMuPDF\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType, IntegerType, NullType, StringType, StructType, StructField \n",
    ")\n",
    "import redis\n",
    "from uuid import uuid4\n",
    "\n",
    "# Import the SKOL classifier jupyter/ist769_skol.ipynb\n",
    "from skol_classifier import SkolClassifier as SC, get_file_list\n",
    "from skol_classifier.preprocessing import SuffixTransformer, ParagraphExtractor\n",
    "from skol_classifier.utils import calculate_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9570ec3-8bb3-41af-99df-d96907293a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: cloudant.protocol\n",
      "Warning: Ignoring non-Spark config property: cloudant.password\n",
      "Warning: Ignoring non-Spark config property: cloudant.host\n",
      "Warning: Ignoring non-Spark config property: cloudant.username\n",
      "25/11/16 23:49:04 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 172.16.227.68 instead (on interface wlp130s0f0)\n",
      "25/11/16 23:49:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-83fbe488-4259-4aa8-b01f-5929c91ed69d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 212ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-83fbe488-4259-4aa8-b01f-5929c91ed69d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/6ms)\n",
      "25/11/16 23:49:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "couchdb_host = \"127.0.0.1:5984\" # e.g., \"ACCOUNT.cloudant.com\" or \"localhost\"\n",
    "couchdb_username = \"admin\"\n",
    "couchdb_password = \"SU2orange!\"\n",
    "ingest_db_name = \"skol_dev\"\n",
    "taxon_db_name = \"skol_taxa_dev\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CouchDB Spark SQL Example in Python using dataframes\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"cloudant.protocol\", \"http\") \\\n",
    "    .config(\"cloudant.host\", couchdb_host) \\\n",
    "    .config(\"cloudant.username\", couchdb_username) \\\n",
    "    .config(\"cloudant.password\", couchdb_password) \\\n",
    "    .config(\"spark.jars.packages\", bahir_package) \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!\n",
    "\n",
    "couch = couchdb.Server(f'http://{couchdb_username}:{couchdb_password}@{couchdb_host}')\n",
    "if ingest_db_name not in couch:\n",
    "    db = couch.create(ingest_db_name)\n",
    "else:\n",
    "    db = couch[ingest_db_name]\n",
    "\n",
    "user_agent = \"synoptickeyof.life\"\n",
    "\n",
    "ingenta_rp = RobotFileParser()\n",
    "ingenta_rp.set_url(\"https://www.ingentaconnect.com/robots.txt\")\n",
    "ingenta_rp.read() # Reads and parses the robots.txt file from the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e7f93-bb66-4922-b3f9-c9429bc304ac",
   "metadata": {},
   "source": [
    "## The Data Sources\n",
    "\n",
    "The goal is to collect all the open access taxonomic literature in Mycology. Most of the sources below mainly cover macro-fungi and slime molds.\n",
    "\n",
    "### Ingested Data Sources\n",
    "\n",
    "* [Mycotaxon at Ingenta Connect](https://www.ingentaconnect.com/content/mtax/mt)\n",
    "* [Studies in Mycology at Ingenta Connect](https://www.studiesinmycology.org/)\n",
    "\n",
    "### Source of many older public domain and open access works\n",
    "\n",
    "Mycoweb includes scans of many older works in mycology. I have local copies but need to ingest them.\n",
    "\n",
    "* [Mycoweb](https://mykoweb.com/)\n",
    "\n",
    "### Journals in hand\n",
    "\n",
    "These are journals we've collected over the years. The initial annotated issues are from early years of Mycotaxon. We still need to ingest all of these.\n",
    "\n",
    "* Mycologia (back issues)\n",
    "* [Mycologia at Taylor and Francis](https://www.tandfonline.com/journals/umyc20)\n",
    "  Mycologia is the main journal of the Mycological Society of America. It is a mix of open access and traditional access articles. The connector for this journal will need to identify the open access articles.\n",
    "* Persoonia (all issues)\n",
    "  Persoonia is no longer published.\n",
    "* Mycotaxon (back issues)\n",
    "  Mycotaxon is no longer published.\n",
    "\n",
    "### Journals that need connectors\n",
    "\n",
    "These are journals we're aware that include open access articles.\n",
    "\n",
    "* [Amanitaceae.org](http://www.tullabs.com/amanita/?home)\n",
    "* [Mycosphere](https://mycosphere.org/)\n",
    "* [Mycoscience](https://mycoscience.org/)\n",
    "* [Journal of Fungi](https://www.mdpi.com/journal/jof)\n",
    "* [Mycology](https://www.tandfonline.com/journals/tmyc20)\n",
    "* [Open Access Journal of Mycology & Mycological Sciences](https://www.medwinpublishers.com/OAJMMS/)\n",
    "* [Mycokeys](https://mycokeys.pensoft.net/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a4213-adb5-49e5-b973-570a75cc2cce",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Each journal or other data source gets an ingester that puts PDFs into our document store along with any metadata we can collect. The metadata is sufficient to create citations for each issue, book, or article. If bibtex citations are available we prefer to store these verbatim.\n",
    "\n",
    "### Ingenta RSS ingestion\n",
    "\n",
    "Ingenta Connect is an electronic publisher that holds two Mycology journals. New articles are available via RSS (Really Simple Syndication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d303cc7-f01c-4e15-87ff-6c259d010591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_bibtex(\n",
    "        db: couchdb.Database,\n",
    "        content: bytes,\n",
    "        bibtex_link: str,\n",
    "        meta: Dict[str, Any],\n",
    "        rp\n",
    "        ) -> None:\n",
    "    \"\"\"Load documents referenced in an Ingenta BibTeX database.\"\"\"\n",
    "    bib_database = bibtexparser.parse_string(content)\n",
    "\n",
    "    bibtex_data = {\n",
    "        'link': bibtex_link,\n",
    "        'bibtex': bibtexparser.write_string(bib_database),\n",
    "    }\n",
    "    \n",
    "    for bib_entry in bib_database.entries:\n",
    "        doc = {\n",
    "            '_id': uuid4().hex,\n",
    "            'meta': meta,\n",
    "            'pdf_url': f\"{bib_entry['url']}?crawler=true\",\n",
    "        }\n",
    "\n",
    "        # Do not fetch if we already have an entry.\n",
    "        selector = {'selector': {'pdf_url': doc['pdf_url']}}\n",
    "        found = False\n",
    "        for e in db.find(selector):\n",
    "            found = True\n",
    "        if found:\n",
    "            print(f\"Skipping {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        if not rp.can_fetch(user_agent, doc['pdf_url']):\n",
    "            # TODO(piggy): We should probably record blocked URLs.\n",
    "            print(f\"Robot permission denied {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Adding {doc['pdf_url']}\")\n",
    "        for k in bib_entry.fields_dict.keys():\n",
    "            doc[k] = bib_entry[k]\n",
    "        \n",
    "        doc_id, doc_rev = db.save(doc)\n",
    "        with requests.get(doc['pdf_url'], stream=False) as pdf_f:\n",
    "            pdf_f.raise_for_status()\n",
    "            pdf_doc = pdf_f.content\n",
    "        \n",
    "        attachment_filename = 'article.pdf'\n",
    "        attachment_content_type = 'application/pdf'\n",
    "        attachment_file = BytesIO(pdf_doc)\n",
    "\n",
    "        db.put_attachment(doc, attachment_file, attachment_filename, attachment_content_type)\n",
    "\n",
    "        print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b323f98-5bd8-4106-b79c-2ff9ac1ac74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_ingenta(\n",
    "        db: couchdb.Database,\n",
    "        rss_url: str,\n",
    "        rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest documents from an Ingenta RSS feed.\"\"\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    \n",
    "    feed_meta = {\n",
    "        'url': rss_url,\n",
    "        'title': feed.feed.title,\n",
    "        'link': feed.feed.link,\n",
    "        'description': feed.feed.description,\n",
    "    }\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        entry_meta = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "        }\n",
    "        if hasattr(entry, 'summary'):\n",
    "            entry_meta['summary'] = entry.summary\n",
    "        if hasattr(entry, 'description'):\n",
    "            entry_meta['description'] = entry.description\n",
    "\n",
    "        bibtex_link = f'{entry.link}?format=bib'\n",
    "        print(f\"bibtex_link: {bibtex_link}\")\n",
    "\n",
    "        if not rp.can_fetch(user_agent, bibtex_link):\n",
    "            print(f\"Robot permission denied {bibtex_link}\")\n",
    "            continue\n",
    "\n",
    "        with requests.get(bibtex_link, stream=False) as bibtex_f:\n",
    "            bibtex_f.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            ingest_from_bibtex(\n",
    "                db=db,\n",
    "                content=bibtex_f.content\\\n",
    "                    .replace(b\"\\\"\\nparent\", b\"\\\",\\nparent\")\\\n",
    "                    .replace(b\"\\n\", b\"\"),\n",
    "                bibtex_link=bibtex_link,\n",
    "                meta={\n",
    "                    'feed': feed_meta,\n",
    "                    'entry': entry_meta,\n",
    "                },\n",
    "                rp=rp\n",
    "            )\n",
    "        print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234f9cc5-9b7e-4559-ad10-f24ad269e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_local_bibtex(\n",
    "    db: couchdb.Database,\n",
    "    root: Path,\n",
    "    rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest from a local directory with Ingenta bibtext files in it.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if not filename.endswith('format=bib'):\n",
    "                continue\n",
    "            full_filepath = os.path.join(dirpath, filename)\n",
    "            bibtex_link = f\"https://www.ingentaconnect.com/{full_filepath[len(str(root)):]}\"\n",
    "            with open(full_filepath) as f:\n",
    "                content = f.read()\\\n",
    "                    .replace(\"\\\"\\nparent\", \"\\\",\\nparent\")\\\n",
    "                    .replace(\"\\n\", \"\")\n",
    "                ingest_from_bibtex(db, content, bibtex_link, meta={}, rp=rp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c336a4-1539-4200-930d-9597fc5bfee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mycotaxon\n",
    "# ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/mtax/mt?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aaf127b-5d8d-4631-a9e1-726636d5f1c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Studies in Mycology\n",
    "# ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/wfbi/sim?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cbd30e5-2b62-4252-baec-1fa4bc7c2c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ingest_from_local_bibtex(\n",
    "#     db=db,\n",
    "#     root=Path(\"/data/skol/www/www.ingentaconnect.com\"),\n",
    "#     rp=ingenta_rp\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf05ad-d2bf-4716-b253-07455cfefcd0",
   "metadata": {},
   "source": [
    "Download the RSS\n",
    "\n",
    "Read bibtex files and create records for each article.\n",
    "\n",
    "Download the PDFs at the URLs in the bibtex entries.\n",
    "\n",
    "Create a JSON record with the PDF as an attachment.\n",
    "\n",
    "### Text extraction\n",
    "\n",
    "We extract \n",
    "Extract the text, optionally with OCR. Add as an additional attachment on the source record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e5e9725-0c3a-4c41-8ebf-2929c4dc4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.load(\n",
    "#     format=\"org.apache.bahir.cloudant\",\n",
    "#     database=ingest_db_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e75c64e-95cb-478a-9a85-46d1297e4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9befa546-a895-4a13-8744-d512587d9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Type: text/html; charset=UTF-8\n",
    "\n",
    "def pdf_to_text(pdf_contents: bytes) -> bytes:\n",
    "    doc = fitz.open(stream=BytesIO(pdf_contents), filetype=\"pdf\")\n",
    "\n",
    "    full_text = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        # Possibly perform OCR on the page\n",
    "        text = page.get_text(\"text\", flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_DEHYPHENATE)\n",
    "        full_text += f\"\\n--- PDF Page {page_num+1} ---\\n\"\n",
    "        full_text += text\n",
    "\n",
    "    return full_text.encode(\"utf-8\")\n",
    "\n",
    "def add_text_to_partition(iterator) -> None:\n",
    "    couch = couchdb.Server(f'http://{couchdb_username}:{couchdb_password}@{couchdb_host}')\n",
    "    local_db = couch[ingest_db_name]\n",
    "    for row in iterator:\n",
    "        if not row:\n",
    "            continue\n",
    "        if not row._attachments:\n",
    "            continue\n",
    "        row_dict = row.asDict()\n",
    "        attachment_dict = row._attachments.asDict()\n",
    "        for pdf_filename in attachment_dict:\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            if pdf_path.suffix != '.pdf':\n",
    "                continue\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            txt_path_str = pdf_path.stem + '.txt'\n",
    "            if txt_path_str in attachment_dict:\n",
    "                # TODO(piggy): Recalculate text if text is terrible. Too much noise vocabulary?\n",
    "                print(f\"Already have text for {row.pdf_url}\")\n",
    "                continue\n",
    "            print(f\"{row._id}, {row.pdf_url}\")\n",
    "            pdf_file = local_db.get_attachment(row._id, str(pdf_path)).read()\n",
    "            txt_file = pdf_to_text(pdf_file)\n",
    "            attachment_content_type = 'text/simple; charset=UTF-8'\n",
    "            attachment_file = BytesIO(txt_file)\n",
    "            local_db.put_attachment(row_dict, attachment_file, txt_path_str, attachment_content_type)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5ef81f-a537-4a15-a001-49a7e6432271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"*\").foreachPartition(add_text_to_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11805c62-ae42-4959-9673-1e09261a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to skol_classifier.CouchDBConnection.\n",
    "from skol_classifier import CouchDBConnection as CDBC\n",
    "\n",
    "class CouchDBConnection(CDBC):\n",
    "    \"\"\"\n",
    "    Manages CouchDB connection and provides I/O operations.\n",
    "\n",
    "    This class encapsulates connection parameters and provides an idempotent\n",
    "    connection method that can be safely called multiple times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize CouchDB connection parameters.\n",
    "\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL (e.g., \"http://localhost:5984\")\n",
    "            database: Database name\n",
    "            username: Optional username for authentication\n",
    "            password: Optional password for authentication\n",
    "        \"\"\"\n",
    "        self.couchdb_url = couchdb_url\n",
    "        self.database = database\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self._server = None\n",
    "        self._db = None\n",
    "\n",
    "    def _connect(self):\n",
    "        \"\"\"\n",
    "        Idempotent connection method that returns a CouchDB server object.\n",
    "\n",
    "        This method can be called multiple times safely - it will only create\n",
    "        a connection if one doesn't already exist.\n",
    "\n",
    "        Returns:\n",
    "            couchdb.Server: Connected CouchDB server object\n",
    "        \"\"\"\n",
    "        if self._server is None:\n",
    "            self._server = couchdb.Server(self.couchdb_url)\n",
    "            if self.username and self.password:\n",
    "                self._server.resource.credentials = (self.username, self.password)\n",
    "\n",
    "        if self._db is None:\n",
    "            self._db = self._server[self.database]\n",
    "\n",
    "        return self._server\n",
    "\n",
    "    @property\n",
    "    def db(self):\n",
    "        \"\"\"Get the database object, connecting if necessary.\"\"\"\n",
    "        if self._db is None:\n",
    "            self._connect()\n",
    "        return self._db\n",
    "\n",
    "    def get_document_list(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Get a list of documents with text attachments from CouchDB.\n",
    "\n",
    "        This only fetches document metadata (not content) to create a DataFrame\n",
    "        that can be processed in parallel. Creates ONE ROW per attachment, so if\n",
    "        a document has multiple attachments matching the pattern, it will have\n",
    "        multiple rows in the resulting DataFrame.\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names (e.g., \"*.txt\")\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, attachment_name\n",
    "            One row per (doc_id, attachment_name) pair\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB (driver only)\n",
    "        db = self.db\n",
    "\n",
    "        # Get all documents with attachments matching pattern\n",
    "        doc_list = []\n",
    "        for doc_id in db:\n",
    "            try:\n",
    "                doc = db[doc_id]\n",
    "                attachments = doc.get('_attachments', {})\n",
    "\n",
    "                # Loop through ALL attachments in the document\n",
    "                for att_name in attachments.keys():\n",
    "                    # Check if attachment matches pattern\n",
    "                    # Pattern matching: \"*.txt\" matches files ending with .txt\n",
    "                    if pattern == \"*.txt\" and att_name.endswith('.txt'):\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern == \"*.*\" or pattern == \"*\":\n",
    "                        # Match all attachments\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern.startswith(\"*.\") and att_name.endswith(pattern[1:]):\n",
    "                        # Generic pattern matching for *.ext\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "            except Exception:\n",
    "                # Skip documents we can't read\n",
    "                continue\n",
    "\n",
    "        # Create DataFrame with document IDs and attachment names\n",
    "        schema = StructType([\n",
    "            StructField(\"doc_id\", StringType(), False),\n",
    "            StructField(\"attachment_name\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        return spark.createDataFrame(doc_list, schema)\n",
    "\n",
    "    def fetch_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row]\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Fetch CouchDB attachments for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id and attachment_name\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, attachment_name, and value (content)\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document has multiple .txt attachments, there will be multiple rows\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Get the specific attachment for this row\n",
    "                    if row.attachment_name in doc.get('_attachments', {}):\n",
    "                        attachment = db.get_attachment(doc, row.attachment_name)\n",
    "                        if attachment:\n",
    "                            content = attachment.read().decode('utf-8', errors='ignore')\n",
    "                            yield Row(\n",
    "                                doc_id=row.doc_id,\n",
    "                                attachment_name=row.attachment_name,\n",
    "                                value=content\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    # Log error but continue processing\n",
    "                    print(f\"Error fetching {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            return\n",
    "\n",
    "    def save_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row],\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Save annotated content to CouchDB for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id, attachment_name, final_aggregated_pg\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, attachment_name, and success status\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document had multiple .txt files, we save multiple .ann files\n",
    "            for row in partition:\n",
    "                success = False\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Create new attachment name by appending suffix\n",
    "                    # e.g., \"article.txt\" becomes \"article.txt.ann\"\n",
    "                    new_attachment_name = f\"{row.attachment_name}{suffix}\"\n",
    "\n",
    "                    # Save the annotated content as a new attachment\n",
    "                    db.put_attachment(\n",
    "                        doc,\n",
    "                        row.final_aggregated_pg.encode('utf-8'),\n",
    "                        filename=new_attachment_name,\n",
    "                        content_type='text/plain'\n",
    "                    )\n",
    "\n",
    "                    success = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=success\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            # Yield failures for all rows\n",
    "            for row in partition:\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n",
    "    def load_distributed(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Load text attachments from CouchDB using foreachPartition.\n",
    "\n",
    "        This function:\n",
    "        1. Gets list of documents (on driver)\n",
    "        2. Creates a DataFrame with doc IDs\n",
    "        3. Uses mapPartitions to fetch content efficiently (one connection per partition)\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, attachment_name, value\n",
    "        \"\"\"\n",
    "        # Get document list\n",
    "        doc_df = self.get_document_list(spark, pattern)\n",
    "\n",
    "        # Use mapPartitions for efficient batch fetching\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def fetch_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.fetch_partition(partition)\n",
    "\n",
    "        # Define output schema\n",
    "        schema = StructType([\n",
    "            StructField(\"doc_id\", StringType(), False),\n",
    "            StructField(\"attachment_name\", StringType(), False),\n",
    "            StructField(\"value\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        # Apply mapPartitions\n",
    "        result_df = doc_df.rdd.mapPartitions(fetch_partition).toDF(schema)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def save_distributed(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Save annotated predictions to CouchDB using foreachPartition.\n",
    "\n",
    "        This function uses mapPartitions where each partition creates a single\n",
    "        CouchDB connection and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with columns: doc_id, attachment_name, final_aggregated_pg\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with doc_id, attachment_name, and success columns\n",
    "        \"\"\"\n",
    "        # Use mapPartitions for efficient batch saving\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def save_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.save_partition(partition, suffix)\n",
    "\n",
    "        # Define output schema\n",
    "        schema = StructType([\n",
    "            StructField(\"doc_id\", StringType(), False),\n",
    "            StructField(\"attachment_name\", StringType(), False),\n",
    "            StructField(\"success\", BooleanType(), False)\n",
    "        ])\n",
    "\n",
    "        # Apply mapPartitions\n",
    "        result_df = df.rdd.mapPartitions(save_partition).toDF(schema)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def process_partition_with_func(\n",
    "        self,\n",
    "        partition: Iterator[Row],\n",
    "        processor_func,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Generic function to read, process, and save in one partition operation.\n",
    "\n",
    "        This allows custom processing logic while maintaining single connection per partition.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows\n",
    "            processor_func: Function to process content (takes content string, returns processed string)\n",
    "            suffix: Suffix for output attachment\n",
    "\n",
    "        Yields:\n",
    "            Rows with processing results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Fetch\n",
    "                    if row.attachment_name in doc.get('_attachments', {}):\n",
    "                        attachment = db.get_attachment(doc, row.attachment_name)\n",
    "                        if attachment:\n",
    "                            content = attachment.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "                            # Process\n",
    "                            processed = processor_func(content)\n",
    "\n",
    "                            # Save\n",
    "                            new_attachment_name = f\"{row.attachment_name}{suffix}\"\n",
    "                            db.put_attachment(\n",
    "                                doc,\n",
    "                                processed.encode('utf-8'),\n",
    "                                filename=new_attachment_name,\n",
    "                                content_type='text/plain'\n",
    "                            )\n",
    "\n",
    "                            yield Row(\n",
    "                                doc_id=row.doc_id,\n",
    "                                attachment_name=row.attachment_name,\n",
    "                                success=True\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            for row in partition:\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a0fa58-e693-4978-b33e-67010e066948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main classifier module for SKOL text classification\n",
    "\"\"\"\n",
    "class SkolClassifier(SC):\n",
    "    \"\"\"\n",
    "    Text classifier for taxonomic literature.\n",
    "\n",
    "    Supports multiple classification models (Logistic Regression, Random Forest)\n",
    "    and feature types (word TF-IDF, suffix TF-IDF, combined).\n",
    "    \"\"\"\n",
    "    def save_to_redis(\n",
    "        self,\n",
    "        redis_client: Optional[Any] = None,\n",
    "        redis_key: Optional[str] = None\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Save the trained models to Redis.\n",
    "\n",
    "        The models are saved to a temporary directory, then packaged and stored in Redis\n",
    "        as a compressed binary blob along with metadata.\n",
    "\n",
    "        Args:\n",
    "            redis_client: Redis client (uses self.redis_client if not provided)\n",
    "            redis_key: Redis key name (uses self.redis_key if not provided)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no models are trained or Redis client is not available\n",
    "        \"\"\"\n",
    "        if self.pipeline_model is None or self.classifier_model is None:\n",
    "            raise ValueError(\n",
    "                \"No models to save. Train models using fit() or train_classifier() first.\"\n",
    "            )\n",
    "\n",
    "        client = redis_client or self.redis_client\n",
    "        key = redis_key or self.redis_key\n",
    "\n",
    "        if client is None:\n",
    "            raise ValueError(\n",
    "                \"No Redis client available. Provide redis_client argument or \"\n",
    "                \"initialize classifier with redis_client.\"\n",
    "            )\n",
    "\n",
    "        temp_dir = None\n",
    "        try:\n",
    "            # Create temporary directory for model files\n",
    "            temp_dir = tempfile.mkdtemp(prefix=\"skol_model_\")\n",
    "            temp_path = Path(temp_dir)\n",
    "\n",
    "            # Save pipeline model\n",
    "            pipeline_path = temp_path / \"pipeline_model\"\n",
    "            self.pipeline_model.save(str(pipeline_path))\n",
    "\n",
    "            # Save classifier model\n",
    "            classifier_path = temp_path / \"classifier_model\"\n",
    "            self.classifier_model.save(str(classifier_path))\n",
    "\n",
    "            # Save metadata (labels and model info)\n",
    "            metadata = {\n",
    "                \"labels\": self.labels,\n",
    "                \"version\": \"0.0.1\"\n",
    "            }\n",
    "            metadata_path = temp_path / \"metadata.json\"\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f)\n",
    "\n",
    "            # Create archive in memory\n",
    "            import io\n",
    "            import tarfile\n",
    "\n",
    "            archive_buffer = io.BytesIO()\n",
    "            with tarfile.open(fileobj=archive_buffer, mode='w:gz') as tar:\n",
    "                tar.add(temp_path, arcname='.')\n",
    "\n",
    "            # Get compressed data\n",
    "            archive_data = archive_buffer.getvalue()\n",
    "\n",
    "            # Save to Redis\n",
    "            client.set(key, archive_data)\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Redis: {e}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary directory\n",
    "            if temp_dir and Path(temp_dir).exists():\n",
    "                shutil.rmtree(temp_dir)\n",
    "\n",
    "    def load_from_redis(\n",
    "        self,\n",
    "        redis_client: Optional[Any] = None,\n",
    "        redis_key: Optional[str] = None\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Load trained models from Redis.\n",
    "\n",
    "        Args:\n",
    "            redis_client: Redis client (uses self.redis_client if not provided)\n",
    "            redis_key: Redis key name (uses self.redis_key if not provided)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If Redis client is not available or key doesn't exist\n",
    "        \"\"\"\n",
    "        client = redis_client or self.redis_client\n",
    "        key = redis_key or self.redis_key\n",
    "\n",
    "        if client is None:\n",
    "            raise ValueError(\n",
    "                \"No Redis client available. Provide redis_client argument or \"\n",
    "                \"initialize classifier with redis_client.\"\n",
    "            )\n",
    "\n",
    "        temp_dir = None\n",
    "        try:\n",
    "            # Retrieve from Redis\n",
    "            archive_data = client.get(key)\n",
    "            if archive_data is None:\n",
    "                raise ValueError(f\"No model found in Redis with key: {key}\")\n",
    "\n",
    "            # Create temporary directory for extraction\n",
    "            temp_dir = tempfile.mkdtemp(prefix=\"skol_model_load_\")\n",
    "            temp_path = Path(temp_dir)\n",
    "\n",
    "            # Extract archive\n",
    "            import io\n",
    "            import tarfile\n",
    "\n",
    "            archive_buffer = io.BytesIO(archive_data)\n",
    "            with tarfile.open(fileobj=archive_buffer, mode='r:gz') as tar:\n",
    "                tar.extractall(temp_path)\n",
    "\n",
    "            # Load pipeline model\n",
    "            pipeline_path = temp_path / \"pipeline_model\"\n",
    "            self.pipeline_model = PipelineModel.load(str(pipeline_path))\n",
    "\n",
    "            # Load classifier model\n",
    "            classifier_path = temp_path / \"classifier_model\"\n",
    "            self.classifier_model = PipelineModel.load(str(classifier_path))\n",
    "\n",
    "            # Load metadata\n",
    "            metadata_path = temp_path / \"metadata.json\"\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "                self.labels = metadata.get(\"labels\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading from Redis: {e}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary directory\n",
    "            if temp_dir and Path(temp_dir).exists():\n",
    "                shutil.rmtree(temp_dir)\n",
    "\n",
    "    def load_from_couchdb(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Load raw text from CouchDB attachments using distributed UDFs.\n",
    "\n",
    "        This method uses Spark UDFs to fetch attachments in parallel across workers,\n",
    "        rather than loading all data on the driver.\n",
    "\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL (e.g., \"http://localhost:5984\")\n",
    "            database: Database name\n",
    "            username: Optional username for authentication\n",
    "            password: Optional password for authentication\n",
    "            pattern: Pattern for attachment names (default: \"*.txt\")\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, attachment_name, value\n",
    "        \"\"\"\n",
    "        conn = CouchDBConnection(couchdb_url, database, username, password)\n",
    "        return conn.fetch_partition(self.spark, pattern)\n",
    "    \n",
    "\n",
    "    def predict_from_couchdb(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None,\n",
    "        pattern: str = \"*.txt\",\n",
    "        output_format: str = \"annotated\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Load text from CouchDB, predict labels, and return predictions.\n",
    "\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL\n",
    "            database: Database name\n",
    "            username: Optional username\n",
    "            password: Optional password\n",
    "            pattern: Pattern for attachment names\n",
    "            output_format: Output format ('annotated' or 'simple')\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with predictions, including doc_id and attachment_name\n",
    "        \"\"\"\n",
    "        if self.pipeline_model is None or self.classifier_model is None:\n",
    "            raise ValueError(\n",
    "                \"Models not trained. Call fit_features() and train_classifier() first.\"\n",
    "            )\n",
    "\n",
    "        # Load data from CouchDB\n",
    "        df = self.load_from_couchdb(\n",
    "            couchdb_url, database, username, password, pattern\n",
    "        )\n",
    "\n",
    "        # Process paragraphs\n",
    "        from .preprocessing import ParagraphExtractor\n",
    "        from pyspark.sql.types import ArrayType, StringType\n",
    "        from pyspark.sql.window import Window\n",
    "\n",
    "        heuristic_udf = udf(\n",
    "            ParagraphExtractor.extract_heuristic_paragraphs,\n",
    "            ArrayType(StringType())\n",
    "        )\n",
    "\n",
    "        # Window specification for ordering\n",
    "        window_spec = Window.partitionBy(\"doc_id\", \"attachment_name\").orderBy(\"start_idx\")\n",
    "\n",
    "        # Group and extract paragraphs\n",
    "        grouped_df = (\n",
    "            df.groupBy(\"doc_id\", \"attachment_name\")\n",
    "            .agg(\n",
    "                collect_list(\"value\").alias(\"lines\"),\n",
    "                min(lit(0)).alias(\"start_idx\")\n",
    "            )\n",
    "            .withColumn(\"value\", explode(heuristic_udf(col(\"lines\"))))\n",
    "            .drop(\"lines\")\n",
    "            .filter(trim(col(\"value\")) != \"\")\n",
    "            .withColumn(\"row_number\", row_number().over(window_spec))\n",
    "        )\n",
    "\n",
    "        # Extract features\n",
    "        features = self.pipeline_model.transform(grouped_df)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.classifier_model.transform(features)\n",
    "\n",
    "        # Convert label indices to strings\n",
    "        from pyspark.ml.feature import IndexToString\n",
    "\n",
    "        converter = IndexToString(\n",
    "            inputCol=\"prediction\",\n",
    "            outputCol=\"predicted_label\",\n",
    "            labels=self.labels\n",
    "        )\n",
    "        labeled_predictions = converter.transform(predictions)\n",
    "\n",
    "        # Format output\n",
    "        if output_format == \"annotated\":\n",
    "            labeled_predictions = labeled_predictions.withColumn(\n",
    "                \"annotated_pg\",\n",
    "                concat(\n",
    "                    lit(\"[@ \"),\n",
    "                    col(\"value\"),\n",
    "                    lit(\"#\"),\n",
    "                    col(\"predicted_label\"),\n",
    "                    lit(\"]\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return labeled_predictions\n",
    "\n",
    "    def save_to_couchdb(\n",
    "        self,\n",
    "        predictions: DataFrame,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Save annotated predictions back to CouchDB using distributed UDFs.\n",
    "\n",
    "        This method uses Spark UDFs to save attachments in parallel across workers,\n",
    "        distributing the write operations.\n",
    "\n",
    "        Args:\n",
    "            predictions: DataFrame with predictions (must include annotated_pg column)\n",
    "            couchdb_url: CouchDB server URL\n",
    "            database: Database name\n",
    "            username: Optional username\n",
    "            password: Optional password\n",
    "            suffix: Suffix to append to attachment names (default: \".ann\")\n",
    "\n",
    "        Returns:\n",
    "            List of results from CouchDB operations\n",
    "        \"\"\"\n",
    "        conn = CouchDBConnection(couchdb_url, database, username, password)\n",
    "\n",
    "        # Aggregate paragraphs by document and attachment\n",
    "        aggregated_df = (\n",
    "            predictions.groupBy(\"doc_id\", \"attachment_name\")\n",
    "            .agg(\n",
    "                expr(\"sort_array(collect_list(struct(row_number, annotated_pg))) AS sorted_list\")\n",
    "            )\n",
    "            .withColumn(\"annotated_pg_ordered\", expr(\"transform(sorted_list, x -> x.annotated_pg)\"))\n",
    "            .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotated_pg_ordered, '\\n')\"))\n",
    "            .select(\"doc_id\", \"attachment_name\", \"final_aggregated_pg\")\n",
    "        )\n",
    "\n",
    "        # Save to CouchDB using distributed UDF\n",
    "        result_df = conn.save_distributed(aggregated_df, suffix)\n",
    "\n",
    "        # Collect results\n",
    "        results = []\n",
    "        for row in result_df.collect():\n",
    "            results.append({\n",
    "                'doc_id': row.doc_id,\n",
    "                'attachment_name': f\"{row.attachment_name}{suffix}\",\n",
    "                'success': row.success\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "m4m45o9n97i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotated files from: /data/piggy/src/github.com/piggyatbaqaqi/skol/data/annotated\n",
      "Found 190 annotated files\n",
      "Training classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9410\n",
      "Test Precision: 0.9614\n",
      "Test Recall: 0.9650\n",
      "Test F1 Score: 0.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "  Accuracy: 0.9410\n",
      "  F1 Score: 0.9407\n",
      "  Labels: ['Misc-exposition', 'Description', 'Nomenclature']\n",
      "\n",
      "Saving model to Redis...\n",
      "Error saving to Redis: ('Pipeline write will fail on this pipeline because stage %s of type %s is not MLWritable', 'SuffixTransformer_6aa87b223488', <class 'skol_classifier.preprocessing.SuffixTransformer'>)\n",
      " Failed to save model to Redis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Train classifier on annotated data and save to Redis\n",
    "# Connect to Redis\n",
    "redis_client = redis.Redis(\n",
    "    host='localhost',\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=False\n",
    ")\n",
    "classifier_model_name = \"skol:classifier:model:v1.0\"\n",
    "\n",
    "# Initialize classifier with Redis connection\n",
    "classifier = SkolClassifier(\n",
    "    spark=spark,\n",
    "    redis_client=redis_client,\n",
    "    redis_key=classifier_model_name,\n",
    "    auto_load=False  # Don't auto-load, we want to train fresh\n",
    ")\n",
    "\n",
    "# Get annotated training files\n",
    "annotated_path = Path.cwd().parent / \"data\" / \"annotated\"\n",
    "print(f\"Loading annotated files from: {annotated_path}\")\n",
    "\n",
    "if annotated_path.exists():\n",
    "    annotated_files = get_file_list(str(annotated_path), pattern=\"**/*.ann\")\n",
    "    \n",
    "    if len(annotated_files) > 0:\n",
    "        print(f\"Found {len(annotated_files)} annotated files\")\n",
    "        \n",
    "        # Train the classifier\n",
    "        print(\"Training classifier...\")\n",
    "        results = classifier.fit(annotated_files)\n",
    "        \n",
    "        print(f\"Training complete!\")\n",
    "        print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
    "        print(f\"  Labels: {classifier.labels}\")\n",
    "        \n",
    "        # Save model to Redis\n",
    "        print(\"\\nSaving model to Redis...\")\n",
    "        if classifier.save_to_redis():\n",
    "            print(f\" Model successfully saved to Redis with key: {classifier_model_name}.\")\n",
    "        else:\n",
    "            print(\" Failed to save model to Redis\")\n",
    "    else:\n",
    "        print(f\"No annotated files found in {annotated_path}\")\n",
    "else:\n",
    "    print(f\"Directory does not exist: {annotated_path}\")\n",
    "    print(\"Please ensure annotated training data is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca929b6a-75e5-4dcb-ace2-8d420f42ce41",
   "metadata": {},
   "source": [
    "## Extract the taxa names and descriptions\n",
    "\n",
    "We use a classifier to extract taxa names and descriptions from articles, issues, and books. Earlier versions of the project added YEDDA annotations. New to this project is saving the metadata, taxa names, and their descriptions directly to a database. Also new is the saved model\n",
    "\n",
    "We use CouchDB to store a full record for each taxon. We copy all metadata to the taxon records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f1457-b1bf-4e02-9b0c-ce194f33969d",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* doi Foundation, \"DOI Citation Formatter HTTP API\", https://citation.doi.org/api-docs.html, accessed 2025-11-12.\n",
    "* YEDDA annotation format (look up citation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82412f-0fd7-4526-a949-3732c2b26624",
   "metadata": {},
   "source": [
    "## Appendix: On the use of an AI Coder\n",
    "\n",
    "Portions of this work were completed with the aid of Claude Code Pro. I wish to give a clarifying example of how I've used this very powerful tool, and reveal why I am comfortable with claiming authorship of the resulting code.\n",
    "\n",
    "For this project I needed results from an earlier class project in which a trio of students built and evaluated models for classifying paragraphs. The earlier work was built as a iPython Notebook, with many examples and inline code. Just copying the earlier notebook would have introduced many irrelevant details and would not further the overall project.\n",
    "\n",
    "I asked Claude Code to translate the notebook into a module that I could import. It did a pretty good job. Without being told, it made a submodule, extract the illustrative code as examples, wrote reasonable documentation and created packaging for the module.\n",
    "\n",
    "The skill level of the coding was roughly that of a highly disciplined average junior programmer. The architecture was simplistic and violated several design constraints such as DRY. I requested specific refactorings, such as asking for a group of functions to be converted into an object that shared duplicated parameters.\n",
    "\n",
    "The initial code used REST interfaces directly, and read all the data into a single machine, not using pyspark correctly. Through a series of refactorings, I asked that the code use appropriate libraries I named, and create correct udf functions to execute transformations in parallel.\n",
    "\n",
    "I walked the AI through creating an object that I could use to illustrate my use of redis and couchdb interfaces, while leaving the irrelevant details in a separate library.\n",
    "\n",
    "In short, I still had to understand good design principles. I had to be able to recognize where appropriate libraries were applicable. I still had to understand the frameworks I am working with.\n",
    "\n",
    "I now have a strong understanding of the difference between \"vibe coding\" and AI-assisted software engineering. In my first 4 hours with Claude Code, I was able to produce roughly 4 days' worth of professional-grade working code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
