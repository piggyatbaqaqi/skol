{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a626f7e-4127-4227-9906-dc08dd9135ce",
   "metadata": {},
   "source": [
    "# SKOL IV: All the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ae6b20-7abd-4ef6-b2cf-8d221a40d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/20 22:58:59 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 192.168.100.99 instead (on interface wlp130s0f0)\n",
      "25/11/20 22:58:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0efcdeae-6abf-4ed3-8a6c-3a21da3687c8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 190ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0efcdeae-6abf-4ed3-8a6c-3a21da3687c8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/9ms)\n",
      "25/11/20 22:58:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Spark context Web UI available at http://192.168.100.99:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1763679542216).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 21.0.8)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> :quit\n"
     ]
    }
   ],
   "source": [
    "bahir_package = 'org.apache.bahir:spark-sql-cloudant_2.12:2.4.0'\n",
    "!spark-shell --packages $bahir_package < /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9615b87-4962-47ca-b10f-98558713196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import requests\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# Be sure to get version 2: https://simple-repository.app.cern.ch/project/bibtexparser/2.0.0b8/description\n",
    "import bibtexparser\n",
    "import couchdb\n",
    "import feedparser\n",
    "import fitz # PyMuPDF\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, CountVectorizer, IDF, StringIndexer, VectorAssembler, IndexToString\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name, collect_list, regexp_extract, col, udf,\n",
    "    explode, trim, row_number, min, expr, concat, lit\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, BooleanType, IntegerType, MapType, NullType,\n",
    "    StringType, StructType, StructField \n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import redis\n",
    "from uuid import uuid4\n",
    "\n",
    "# Local modules\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "parent_path = Path(parent_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from couchdb_file import CouchDBFile as CDBF\n",
    "from fileobj import FileObject\n",
    "from finder import parse_annotated, remove_interstitials\n",
    "import line\n",
    "from line import Line\n",
    "\n",
    "# Import the SKOL classifier jupyter/ist769_skol.ipynb\n",
    "from skol_classifier import SkolClassifier as SC, get_file_list\n",
    "from skol_classifier.preprocessing import SuffixTransformer, ParagraphExtractor\n",
    "from skol_classifier.utils import calculate_stats\n",
    "\n",
    "from taxon import group_paragraphs, Taxon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9570ec3-8bb3-41af-99df-d96907293a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: cloudant.protocol\n",
      "Warning: Ignoring non-Spark config property: cloudant.password\n",
      "Warning: Ignoring non-Spark config property: cloudant.host\n",
      "Warning: Ignoring non-Spark config property: cloudant.username\n",
      "25/11/20 22:59:04 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 192.168.100.99 instead (on interface wlp130s0f0)\n",
      "25/11/20 22:59:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-83dea4cf-a31f-437e-aea3-aa08db74ae98;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 201ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-83dea4cf-a31f-437e-aea3-aa08db74ae98\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/8ms)\n",
      "25/11/20 22:59:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "couchdb_host = \"127.0.0.1:5984\" # e.g., \"ACCOUNT.cloudant.com\" or \"localhost\"\n",
    "couchdb_username = \"admin\"\n",
    "couchdb_password = \"SU2orange!\"\n",
    "ingest_db_name = \"skol_dev\"\n",
    "taxon_db_name = \"skol_taxa_dev\"\n",
    "\n",
    "couchdb_url = f'http://{couchdb_host}'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CouchDB Spark SQL Example in Python using dataframes\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"cloudant.protocol\", \"http\") \\\n",
    "    .config(\"cloudant.host\", couchdb_host) \\\n",
    "    .config(\"cloudant.username\", couchdb_username) \\\n",
    "    .config(\"cloudant.password\", couchdb_password) \\\n",
    "    .config(\"spark.jars.packages\", bahir_package) \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .config(\"spark.submit.pyFiles\",\n",
    "            f'{parent_path / \"line.py\"},{parent_path / \"fileobj.py\"},'\n",
    "            f'{parent_path / \"couchdb_file.py\"},{parent_path / \"finder.py\"},'\n",
    "            f'{parent_path / \"taxon.py\"},{parent_path / \"paragraph.py\"},'\n",
    "            f'{parent_path / \"label.py\"},{parent_path / \"file.py\"}'\n",
    "           ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!\n",
    "\n",
    "couch = couchdb.Server(couchdb_url)\n",
    "couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "\n",
    "if ingest_db_name not in couch:\n",
    "    db = couch.create(ingest_db_name)\n",
    "else:\n",
    "    db = couch[ingest_db_name]\n",
    "\n",
    "user_agent = \"synoptickeyof.life\"\n",
    "\n",
    "ingenta_rp = RobotFileParser()\n",
    "ingenta_rp.set_url(\"https://www.ingentaconnect.com/robots.txt\")\n",
    "ingenta_rp.read() # Reads and parses the robots.txt file from the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e7f93-bb66-4922-b3f9-c9429bc304ac",
   "metadata": {},
   "source": [
    "## The Data Sources\n",
    "\n",
    "The goal is to collect all the open access taxonomic literature in Mycology. Most of the sources below mainly cover macro-fungi and slime molds.\n",
    "\n",
    "### Ingested Data Sources\n",
    "\n",
    "* [Mycotaxon at Ingenta Connect](https://www.ingentaconnect.com/content/mtax/mt)\n",
    "* [Studies in Mycology at Ingenta Connect](https://www.studiesinmycology.org/)\n",
    "\n",
    "### Source of many older public domain and open access works\n",
    "\n",
    "Mycoweb includes scans of many older works in mycology. I have local copies but need to ingest them.\n",
    "\n",
    "* [Mycoweb](https://mykoweb.com/)\n",
    "\n",
    "### Journals in hand\n",
    "\n",
    "These are journals we've collected over the years. The initial annotated issues are from early years of Mycotaxon. We still need to ingest all of these.\n",
    "\n",
    "* Mycologia (back issues)\n",
    "* [Mycologia at Taylor and Francis](https://www.tandfonline.com/journals/umyc20)\n",
    "  Mycologia is the main journal of the Mycological Society of America. It is a mix of open access and traditional access articles. The connector for this journal will need to identify the open access articles.\n",
    "* Persoonia (all issues)\n",
    "  Persoonia is no longer published.\n",
    "* Mycotaxon (back issues)\n",
    "  Mycotaxon is no longer published.\n",
    "\n",
    "### Journals that need connectors\n",
    "\n",
    "These are journals we're aware that include open access articles.\n",
    "\n",
    "* [Amanitaceae.org](http://www.tullabs.com/amanita/?home)\n",
    "* [Mycosphere](https://mycosphere.org/)\n",
    "* [Mycoscience](https://mycoscience.org/)\n",
    "* [Journal of Fungi](https://www.mdpi.com/journal/jof)\n",
    "* [Mycology](https://www.tandfonline.com/journals/tmyc20)\n",
    "* [Open Access Journal of Mycology & Mycological Sciences](https://www.medwinpublishers.com/OAJMMS/)\n",
    "* [Mycokeys](https://mycokeys.pensoft.net/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a4213-adb5-49e5-b973-570a75cc2cce",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Each journal or other data source gets an ingester that puts PDFs into our document store along with any metadata we can collect. The metadata is sufficient to create citations for each issue, book, or article. If bibtex citations are available we prefer to store these verbatim.\n",
    "\n",
    "### Ingenta RSS ingestion\n",
    "\n",
    "Ingenta Connect is an electronic publisher that holds two Mycology journals. New articles are available via RSS (Really Simple Syndication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d303cc7-f01c-4e15-87ff-6c259d010591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_bibtex(\n",
    "        db: couchdb.Database,\n",
    "        content: bytes,\n",
    "        bibtex_link: str,\n",
    "        meta: Dict[str, Any],\n",
    "        rp\n",
    "        ) -> None:\n",
    "    \"\"\"Load documents referenced in an Ingenta BibTeX database.\"\"\"\n",
    "    bib_database = bibtexparser.parse_string(content)\n",
    "\n",
    "    bibtex_data = {\n",
    "        'link': bibtex_link,\n",
    "        'bibtex': bibtexparser.write_string(bib_database),\n",
    "    }\n",
    "    \n",
    "    for bib_entry in bib_database.entries:\n",
    "        doc = {\n",
    "            '_id': uuid4().hex,\n",
    "            'meta': meta,\n",
    "            'pdf_url': f\"{bib_entry['url']}?crawler=true\",\n",
    "        }\n",
    "\n",
    "        # Do not fetch if we already have an entry.\n",
    "        selector = {'selector': {'pdf_url': doc['pdf_url']}}\n",
    "        found = False\n",
    "        for e in db.find(selector):\n",
    "            found = True\n",
    "        if found:\n",
    "            print(f\"Skipping {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        if not rp.can_fetch(user_agent, doc['pdf_url']):\n",
    "            # TODO(piggy): We should probably record blocked URLs.\n",
    "            print(f\"Robot permission denied {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Adding {doc['pdf_url']}\")\n",
    "        for k in bib_entry.fields_dict.keys():\n",
    "            doc[k] = bib_entry[k]\n",
    "        \n",
    "        doc_id, doc_rev = db.save(doc)\n",
    "        with requests.get(doc['pdf_url'], stream=False) as pdf_f:\n",
    "            pdf_f.raise_for_status()\n",
    "            pdf_doc = pdf_f.content\n",
    "        \n",
    "        attachment_filename = 'article.pdf'\n",
    "        attachment_content_type = 'application/pdf'\n",
    "        attachment_file = BytesIO(pdf_doc)\n",
    "\n",
    "        db.put_attachment(doc, attachment_file, attachment_filename, attachment_content_type)\n",
    "\n",
    "        print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b323f98-5bd8-4106-b79c-2ff9ac1ac74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_ingenta(\n",
    "        db: couchdb.Database,\n",
    "        rss_url: str,\n",
    "        rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest documents from an Ingenta RSS feed.\"\"\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    \n",
    "    feed_meta = {\n",
    "        'url': rss_url,\n",
    "        'title': feed.feed.title,\n",
    "        'link': feed.feed.link,\n",
    "        'description': feed.feed.description,\n",
    "    }\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        entry_meta = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "        }\n",
    "        if hasattr(entry, 'summary'):\n",
    "            entry_meta['summary'] = entry.summary\n",
    "        if hasattr(entry, 'description'):\n",
    "            entry_meta['description'] = entry.description\n",
    "\n",
    "        bibtex_link = f'{entry.link}?format=bib'\n",
    "        print(f\"bibtex_link: {bibtex_link}\")\n",
    "\n",
    "        if not rp.can_fetch(user_agent, bibtex_link):\n",
    "            print(f\"Robot permission denied {bibtex_link}\")\n",
    "            continue\n",
    "\n",
    "        with requests.get(bibtex_link, stream=False) as bibtex_f:\n",
    "            bibtex_f.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            ingest_from_bibtex(\n",
    "                db=db,\n",
    "                content=bibtex_f.content\\\n",
    "                    .replace(b\"\\\"\\nparent\", b\"\\\",\\nparent\")\\\n",
    "                    .replace(b\"\\n\", b\"\"),\n",
    "                bibtex_link=bibtex_link,\n",
    "                meta={\n",
    "                    'feed': feed_meta,\n",
    "                    'entry': entry_meta,\n",
    "                },\n",
    "                rp=rp\n",
    "            )\n",
    "        print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234f9cc5-9b7e-4559-ad10-f24ad269e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_local_bibtex(\n",
    "    db: couchdb.Database,\n",
    "    root: Path,\n",
    "    rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest from a local directory with Ingenta bibtext files in it.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if not filename.endswith('format=bib'):\n",
    "                continue\n",
    "            full_filepath = os.path.join(dirpath, filename)\n",
    "            bibtex_link = f\"https://www.ingentaconnect.com/{full_filepath[len(str(root)):]}\"\n",
    "            with open(full_filepath) as f:\n",
    "                content = f.read()\\\n",
    "                    .replace(\"\\\"\\nparent\", \"\\\",\\nparent\")\\\n",
    "                    .replace(\"\\n\", \"\")\n",
    "                ingest_from_bibtex(db, content, bibtex_link, meta={}, rp=rp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58829ec0-93f7-44dd-87bc-511ab2d238d0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mycotaxon\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/mtax/mt?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40602cc3-e231-497d-adc7-d6d4471514cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Studies in Mycology\n",
    "\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/wfbi/sim?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "651f82ea-b3b8-4f0d-adf0-f5550d7ed6d5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ingest_from_local_bibtex(\n",
    "    db=db,\n",
    "    root=Path(\"/data/skol/www/www.ingentaconnect.com\"),\n",
    "    rp=ingenta_rp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf05ad-d2bf-4716-b253-07455cfefcd0",
   "metadata": {},
   "source": [
    "Download the RSS\n",
    "\n",
    "Read bibtex files and create records for each article.\n",
    "\n",
    "Download the PDFs at the URLs in the bibtex entries.\n",
    "\n",
    "Create a JSON record with the PDF as an attachment.\n",
    "\n",
    "### Text extraction\n",
    "\n",
    "We extract the text, optionally with OCR. Add as an additional attachment on the source record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5e9725-0c3a-4c41-8ebf-2929c4dc4334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\n",
    "    format=\"org.apache.bahir.cloudant\",\n",
    "    database=ingest_db_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e75c64e-95cb-478a-9a85-46d1297e4225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _id: string, _rev: string, abstract: string, author: string, doi: string, eissn: string, issn: string, itemtype: string, journal: string, number: string, pages: string, parent_itemid: string, pdf_url: string, publication date: string, publishercode: string, title: string, url: string, volume: string, year: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9befa546-a895-4a13-8744-d512587d9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Type: text/html; charset=UTF-8\n",
    "\n",
    "def pdf_to_text(pdf_contents: bytes) -> bytes:\n",
    "    doc = fitz.open(stream=BytesIO(pdf_contents), filetype=\"pdf\")\n",
    "\n",
    "    full_text = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        # Possibly perform OCR on the page\n",
    "        text = page.get_text(\"text\", flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_DEHYPHENATE)\n",
    "        # full_text += f\"\\n--- PDF Page {page_num+1} ---\\n\"\n",
    "        full_text += text\n",
    "\n",
    "    return full_text.encode(\"utf-8\")\n",
    "\n",
    "def add_text_to_partition(iterator) -> None:\n",
    "    couch = couchdb.Server(couchdb_url)\n",
    "    couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "    local_db = couch[ingest_db_name]\n",
    "    for row in iterator:\n",
    "        if not row:\n",
    "            continue\n",
    "        if not row._attachments:\n",
    "            continue\n",
    "        row_dict = row.asDict()\n",
    "        attachment_dict = row._attachments.asDict()\n",
    "        for pdf_filename in attachment_dict:\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            if pdf_path.suffix != '.pdf':\n",
    "                continue\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            txt_path_str = pdf_path.stem + '.txt'\n",
    "            # if txt_path_str in attachment_dict:\n",
    "            #     # TODO(piggy): Recalculate text if text is terrible. Too much noise vocabulary?\n",
    "            #     print(f\"Already have text for {row.pdf_url}\")\n",
    "            #     continue\n",
    "            print(f\"{row._id}, {row.pdf_url}\")\n",
    "            pdf_file = local_db.get_attachment(row._id, str(pdf_path)).read()\n",
    "            txt_file = pdf_to_text(pdf_file)\n",
    "            attachment_content_type = 'text/simple; charset=UTF-8'\n",
    "            attachment_file = BytesIO(txt_file)\n",
    "            local_db.put_attachment(row_dict, attachment_file, txt_path_str, attachment_content_type)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a51014-ee58-449b-8ce4-8d0a986e68c1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df.select(\"*\").foreachPartition(add_text_to_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11805c62-ae42-4959-9673-1e09261a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to skol_classifier.CouchDBConnection.\n",
    "from skol_classifier import CouchDBConnection as CDBC\n",
    "\n",
    "class CouchDBConnection(CDBC):\n",
    "    \"\"\"\n",
    "    Manages CouchDB connection and provides I/O operations.\n",
    "\n",
    "    This class encapsulates connection parameters and provides an idempotent\n",
    "    connection method that can be safely called multiple times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize CouchDB connection parameters.\n",
    "CouchDBFile\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL (e.g., \"http://localhost:5984\")\n",
    "            database: Database name\n",
    "            username: Optional username for authentication\n",
    "            password: Optional password for authentication\n",
    "        \"\"\"\n",
    "        self.couchdb_url = couchdb_url\n",
    "        self.database = database\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self._server = None\n",
    "        self._db = None\n",
    "\n",
    "    def _connect(self):\n",
    "        \"\"\"\n",
    "        Idempotent connection method that returns a CouchDB server object.\n",
    "\n",
    "        This method can be called multiple times safely - it will only create\n",
    "        a connection if one doesn't already exist.\n",
    "\n",
    "        Returns:\n",
    "            couchdb.Server: Connected CouchDB server object\n",
    "        \"\"\"\n",
    "        if self._server is None:\n",
    "            self._server = couchdb.Server(self.couchdb_url)\n",
    "            if self.username and self.password:\n",
    "                self._server.resource.credentials = (self.username, self.password)\n",
    "\n",
    "        if self._db is None:\n",
    "            self._db = self._server[self.database]\n",
    "\n",
    "        return self._server\n",
    "\n",
    "    @property\n",
    "    def db(self):\n",
    "        \"\"\"Get the database object, connecting if necessary.\"\"\"\n",
    "        if self._db is None:\n",
    "            self._connect()\n",
    "        return self._db\n",
    "\n",
    "    def get_document_list(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Get a list of documents with text attachments from CouchDB.\n",
    "\n",
    "        This only fetches document metadata (not content) to create a DataFrame\n",
    "        that can be processed in parallel. Creates ONE ROW per attachment, so if\n",
    "        a document has multiple attachments matching the pattern, it will have\n",
    "        multiple rows in the resulting DataFrame.\n",
    "ist769_skol\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names (e.g., \"*.txt\")\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, attachment_name\n",
    "            One row per (doc_id, attachment_name) pair\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB (driver only)\n",
    "        db = self.db\n",
    "\n",
    "        # Get all documents with attachments matching pattern\n",
    "        doc_list = []\n",
    "        for doc_id in db:\n",
    "            try:\n",
    "                doc = db[doc_id]\n",
    "                attachments = doc.get('_attachments', {})\n",
    "\n",
    "                # Loop through ALL attachments in the document\n",
    "                for att_name in attachments.keys():\n",
    "                    # Check if attachment matches pattern\n",
    "                    # Pattern matching: \"*.txt\" matches files ending with .txt\n",
    "                    if pattern == \"*.txt\" and att_name.endswith('.txt'):\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern == \"*.*\" or pattern == \"*\":\n",
    "                        # Match all attachments\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern.startswith(\"*.\") and att_name.endswith(pattern[1:]):\n",
    "                        # Generic pattern matching for *.ext\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "            except Exception:\n",
    "                # Skip documents we can't read\n",
    "                continue\n",
    "\n",
    "        # Create DataFrame with document IDs and attachment names\n",
    "        schema = StructType([\n",
    "            StructField(\"doc_id\", StringType(), False),\n",
    "            StructField(\"attachment_name\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        return spark.createDataFrame(doc_list, schema)\n",
    "\n",
    "    def fetch_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row]\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Fetch CouchDB attachments for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id and attachment_name\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, doc, attachement_name, and value (content)\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document has multiple .txt attachments, there will be multiple rows\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "                    # Get the specific attachment for this row\n",
    "                    if row.attachment_name in doc.get('_attachments', {}):\n",
    "                        attachment = db.get_attachment(doc, row.attachment_name)\n",
    "                        if attachment:\n",
    "                            content = attachment.read().decode('utf-8', errors='ignore')\n",
    "                            yield Row(\n",
    "                                doc_id=row.doc_id,\n",
    "                                doc=doc,\n",
    "                                attachment_name=row.attachment_name,\n",
    "                                value=content\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    # Log error but continue processing\n",
    "                    print(f\"Error fetching {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            return\n",
    "\n",
    "    def save_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row],\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Save annotated content to CouchDB for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id, attachment_name, final_aggregated_pg\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, attachment_name, and success status\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document had multiple .txt files, we save multiple .ann files\n",
    "            for row in partition:\n",
    "                success = False\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Create new attachment name by appending suffix\n",
    "                    # e.g., \"article.txt\" becomes \"article.txt.ann\"\n",
    "                    new_attachment_name = f\"{row.attachment_name}{suffix}\"\n",
    "\n",
    "                    # Save the annotated content as a new attachment\n",
    "                    db.put_attachment(\n",
    "                        doc,\n",
    "                        row.final_aggregated_pg.encode('utf-8'),\n",
    "                        filename=new_attachment_name,\n",
    "                        content_type='text/plain'\n",
    "                    )\n",
    "\n",
    "                    success = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=success\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            # Yield failures for all rows\n",
    "            for row in partition:\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    doc=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n",
    "    def load_distributed(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Load text attachments from CouchDB using foreachPartition.\n",
    "\n",
    "        This function:\n",
    "        1. Gets list of documents (on driver)\n",
    "        2. Creates a DataFrame with doc IDs\n",
    "        3. Uses mapPartitions to fetch content efficiently (one connection per partition)\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, doc, value\n",
    "        \"\"\"\n",
    "        # Get document list\n",
    "        doc_df = self.get_document_list(spark, pattern)\n",
    "\n",
    "        # Use mapPartitions for efficient batch fetching\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def fetch_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.fetch_partition(partition)\n",
    "\n",
    "        # Apply mapPartitions\n",
    "        result_df = doc_df.rdd.mapPartitions(fetch_partition).toDF()\n",
    "\n",
    "        result_df.printSchema()\n",
    "        return result_df\n",
    "\n",
    "    def save_distributed(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Save annotated predictions to CouchDB using foreachPartition.\n",
    "\n",
    "        This function uses mapPartitions where each partition creates a single\n",
    "        CouchDB connection and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with columns: doc_id, attachment_name, final_aggregated_pg\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with doc_id, attachment_name, and success columns\n",
    "        \"\"\"\n",
    "        # Use mapPartitions for efficient batch saving\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def save_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.save_partition(partition, suffix)\n",
    "\n",
    "        # Define output schema\n",
    "        schema = StructType([\n",
    "            StructField(\"doc_id\", StringType(), False),\n",
    "            StructField(\"attachment_name\", StringType(), False),\n",
    "            StructField(\"success\", BooleanType(), False)\n",
    "        ])\n",
    "\n",
    "        # Apply mapPartitions\n",
    "        result_df = df.rdd.mapPartitions(save_partition).toDF(schema)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def process_partition_with_func(\n",
    "        self,\n",
    "        partition: Iterator[Row],\n",
    "        processor_func,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Generic function to read, process, and save in one partition operation.\n",
    "\n",
    "        This allows custom processing logic while maintaining single connection per partition.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows\n",
    "            processor_func: Function to process content (takes content string, returns processed string)\n",
    "            suffix: Suffix for output attachment\n",
    "\n",
    "        Yields:\n",
    "            Rows with processing results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Fetch\n",
    "                    if row.attachment_name in doc.get('_attachments', {}):\n",
    "                        attachment = db.get_attachment(doc, row.attachment_name)\n",
    "                        if attachment:\n",
    "                            content = attachment.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "                            # Process\n",
    "                            processed = processor_func(content)\n",
    "\n",
    "                            # Save\n",
    "                            new_attachment_name = f\"{row.attachment_name}{suffix}\"\n",
    "                            db.put_attachment(\n",
    "                                doc,\n",
    "                                processed.encode('utf-8'),\n",
    "                                filename=new_attachment_name,\n",
    "                                content_type='text/plain'\n",
    "                            )\n",
    "\n",
    "                            yield Row(\n",
    "                                doc_id=row.doc_id,\n",
    "                                attachment_name=row.attachment_name,\n",
    "                                success=True\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            for row in partition:\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08a0fa58-e693-4978-b33e-67010e066948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main classifier module for SKOL text classification\n",
    "\"\"\"\n",
    "class SkolClassifier(SC):\n",
    "    \"\"\"\n",
    "    Text classifier for taxonomic literature.\n",
    "\n",
    "    This version only includes the redis and couchdb I/O methods.\n",
    "    All other methods are in SC.\n",
    "\n",
    "    Supports multiple classification models (Logistic Regression, Random Forest)\n",
    "    and feature types (word TF-IDF, suffix TF-IDF, combined).\n",
    "    \"\"\"\n",
    "    def save_to_redis(self) -> bool:\n",
    "        \"\"\"\n",
    "        Save the trained models to Redis.\n",
    "\n",
    "        The models are saved to a temporary directory, then packaged and stored in Redis\n",
    "        as a compressed binary blob along with metadata.\n",
    "\n",
    "        Uses the Redis client and key configured in the constructor.\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no models are trained or Redis client is not configured\n",
    "        \"\"\"\n",
    "        if self.pipeline_model is None or self.classifier_model is None:\n",
    "            raise ValueError(\n",
    "                \"No models to save. Train models using fit() or train_classifier() first.\"\n",
    "            )\n",
    "\n",
    "        if self.redis_client is None:\n",
    "            raise ValueError(\n",
    "                \"No Redis client configured. Initialize classifier with redis_client.\"\n",
    "            )\n",
    "\n",
    "        temp_dir = None\n",
    "        try:\n",
    "            # Create temporary directory for model files\n",
    "            temp_dir = tempfile.mkdtemp(prefix=\"skol_model_\")\n",
    "            temp_path = Path(temp_dir)\n",
    "\n",
    "            # Save pipeline model\n",
    "            pipeline_path = temp_path / \"pipeline_model\"\n",
    "            self.pipeline_model.save(str(pipeline_path))\n",
    "\n",
    "            # Save classifier model\n",
    "            classifier_path = temp_path / \"classifier_model\"\n",
    "            self.classifier_model.save(str(classifier_path))\n",
    "\n",
    "            # Save metadata (labels and model info)\n",
    "            metadata = {\n",
    "                \"labels\": self.labels,\n",
    "                \"version\": \"0.0.1\"\n",
    "            }\n",
    "            metadata_path = temp_path / \"metadata.json\"\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f)\n",
    "\n",
    "            # Create archive in memory\n",
    "            import io\n",
    "            import tarfile\n",
    "\n",
    "            archive_buffer = io.BytesIO()\n",
    "            with tarfile.open(fileobj=archive_buffer, mode='w:gz') as tar:\n",
    "                tar.add(temp_path, arcname='.')\n",
    "\n",
    "            # Get compressed data\n",
    "            archive_data = archive_buffer.getvalue()\n",
    "\n",
    "            # Save to Redis\n",
    "            self.redis_client.set(self.redis_key, archive_data)\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Redis: {e}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary directory\n",
    "            if temp_dir and Path(temp_dir).exists():\n",
    "                shutil.rmtree(temp_dir)\n",
    "\n",
    "    def load_from_redis(self) -> bool:\n",
    "        \"\"\"\n",
    "        Load trained models from Redis.\n",
    "\n",
    "        Uses the Redis client and key configured in the constructor.\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If Redis client is not configured or key doesn't exist\n",
    "        \"\"\"\n",
    "        if self.redis_client is None:\n",
    "            raise ValueError(\n",
    "                \"No Redis client configured. Initialize classifier with redis_client.\"\n",
    "            )\n",
    "\n",
    "        temp_dir = None\n",
    "        try:\n",
    "            # Retrieve from Redis\n",
    "            archive_data = self.redis_client.get(self.redis_key)\n",
    "            if archive_data is None:\n",
    "                raise ValueError(f\"No model found in Redis with key: {self.redis_key}\")\n",
    "\n",
    "            # Create temporary directory for extraction\n",
    "            temp_dir = tempfile.mkdtemp(prefix=\"skol_model_load_\")\n",
    "            temp_path = Path(temp_dir)\n",
    "\n",
    "            # Extract archive\n",
    "            import io\n",
    "            import tarfile\n",
    "\n",
    "            archive_buffer = io.BytesIO(archive_data)\n",
    "            with tarfile.open(fileobj=archive_buffer, mode='r:gz') as tar:\n",
    "                tar.extractall(temp_path)\n",
    "\n",
    "            # Load pipeline model\n",
    "            pipeline_path = temp_path / \"pipeline_model\"\n",
    "            self.pipeline_model = PipelineModel.load(str(pipeline_path))\n",
    "\n",
    "            # Load classifier model\n",
    "            classifier_path = temp_path / \"classifier_model\"\n",
    "            self.classifier_model = PipelineModel.load(str(classifier_path))\n",
    "\n",
    "            # Load metadata\n",
    "            metadata_path = temp_path / \"metadata.json\"\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "                self.labels = metadata.get(\"labels\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading from Redis: {e}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary directory\n",
    "            if temp_dir and Path(temp_dir).exists():\n",
    "                shutil.rmtree(temp_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab2d58-0ea7-4288-889c-b1bf9c360743",
   "metadata": {},
   "source": [
    "## Build a classifier to identify paragraph types.\n",
    "\n",
    "We save this to redis so that we don't need to train the model every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "m4m45o9n97i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping generation of model skol:classifier:model:v1.0.\n"
     ]
    }
   ],
   "source": [
    "# Train classifier on annotated data and save to Redis\n",
    "# Connect to Redis\n",
    "redis_client = redis.Redis(\n",
    "    host='localhost',\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=False\n",
    ")\n",
    "classifier_model_name = \"skol:classifier:model:v1.0\"\n",
    "\n",
    "if not redis_client.exists(classifier_model_name):\n",
    "\n",
    "    # Initialize classifier with Redis connection\n",
    "    classifier = SkolClassifier(\n",
    "        spark=spark,\n",
    "        redis_client=redis_client,\n",
    "        redis_key=classifier_model_name,\n",
    "        auto_load=False,  # Don't auto-load, we want to train fresh\n",
    "        couchdb_url=couchdb_url,\n",
    "        database=ingest_db_name,\n",
    "        username=couchdb_username,\n",
    "        password=couchdb_password\n",
    "    )\n",
    "    \n",
    "    # Get annotated training files\n",
    "    annotated_path = Path.cwd().parent / \"data\" / \"annotated\"\n",
    "    print(f\"Loading annotated files from: {annotated_path}\")\n",
    "    \n",
    "    if annotated_path.exists():\n",
    "        annotated_files = get_file_list(str(annotated_path), pattern=\"**/*.ann\")\n",
    "        \n",
    "        if len(annotated_files) > 0:\n",
    "            print(f\"Found {len(annotated_files)} annotated files\")\n",
    "            \n",
    "            # Train the classifier\n",
    "            print(\"Training classifier...\")\n",
    "            results = classifier.fit(annotated_files)\n",
    "            \n",
    "            print(f\"Training complete!\")\n",
    "            print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "            print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
    "            print(f\"  Labels: {classifier.labels}\")\n",
    "            \n",
    "            # Save model to Redis\n",
    "            print(\"\\nSaving model to Redis...\")\n",
    "            if classifier.save_to_redis():\n",
    "                print(f\" Model successfully saved to Redis with key: {classifier_model_name}.\")\n",
    "            else:\n",
    "                print(\" Failed to save model to Redis\")\n",
    "        else:\n",
    "            print(f\"No annotated files found in {annotated_path}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {annotated_path}\")\n",
    "        print(\"Please ensure annotated training data is available.\")\n",
    "else:\n",
    "    print(f\"Skipping generation of model {classifier_model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fe9eb-1f1b-4aa8-9796-169faeeaf432",
   "metadata": {},
   "source": [
    "## Extract the taxa names and descriptions\n",
    "\n",
    "We use a classifier to extract taxa names and descriptions from articles, issues, and books. The YEDDA annotated texts are written back to CouchDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd6b1621-bb26-4cf1-84cb-3b74453741aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_455390/3263727588.py:121: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(temp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with labels: ['Misc-exposition', 'Description', 'Nomenclature']\n",
      "\n",
      "Loading and classifying documents from CouchDB...\n",
      "\n",
      "Sample predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "Exception ignored in: <_io.BufferedWriter name=5>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Exception ignored in: <_io.BufferedWriter name=5>                               \n",
      "Traceback (most recent call last):\n",
      "  File \"/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------+---------------+\n",
      "|                          doc_id|attachment_name|predicted_label|\n",
      "+--------------------------------+---------------+---------------+\n",
      "|006b331e284e4dc8b7ab9e275d00ad22|    article.txt|    Description|\n",
      "|00769b24893c40ea9062aa486f6cebe2|    article.txt|Misc-exposition|\n",
      "|013e630386744976a13eddfc71ea3b97|    article.txt|Misc-exposition|\n",
      "|0149ece76caa4937a07d12406500c198|    article.txt|Misc-exposition|\n",
      "|015e247f133f485984ecaf092b168ca6|    article.txt|Misc-exposition|\n",
      "+--------------------------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Saving predictions back to CouchDB...\n",
      "(Each partition writes its documents using a single connection)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "[Stage 46:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 2005 annotated files to CouchDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "classifier = SkolClassifier(\n",
    "    spark=spark,\n",
    "    redis_client=redis_client,\n",
    "    redis_key=classifier_model_name,\n",
    "    auto_load=True,\n",
    "    couchdb_url=couchdb_url,\n",
    "    database=ingest_db_name,\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password\n",
    ")\n",
    "\n",
    "if classifier.labels is None:\n",
    "    raise ValueError(\"No model found in Redis. Please train a model first.\")\n",
    "\n",
    "print(f\"Model loaded with labels: {classifier.labels}\")\n",
    "\n",
    "print(\"\\nLoading and classifying documents from CouchDB...\")\n",
    "predictions = classifier.predict_from_couchdb(pattern=\"*.txt\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "predictions.select(\n",
    "    \"doc_id\", \"attachment_name\", \"predicted_label\"\n",
    ").show(5, truncate=50)\n",
    "\n",
    "# Save results back to CouchDB using distributed writes\n",
    "print(\"\\nSaving predictions back to CouchDB...\")\n",
    "print(\"(Each partition writes its documents using a single connection)\")\n",
    "results = classifier.save_to_couchdb(predictions=predictions, suffix=\".ann\")\n",
    "\n",
    "# Report results\n",
    "successful = sum(1 for r in results if r['success'])\n",
    "failed = len(results) - successful\n",
    "\n",
    "print(f\"\\nSaved {successful} annotated files to CouchDB\")\n",
    "if failed > 0:\n",
    "    print(f\"Failed to save {failed} files\")\n",
    "    for r in results:\n",
    "        if not r['success']:\n",
    "            print(f\"  {r['doc_id']}/{r['attachment_name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ecd132-4946-47cf-a67b-364335eb768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|predicted_label|        annotated_pg|\n",
      "+---------------+--------------------+\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ MYCOTAXON\\nVol...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ MYCOTAXON\\nVol...|\n",
      "|   Nomenclature|[@ MYCOTAXON \\nIS...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ MYCOTAXON\\nVol...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ 87\\n 2022 Wes...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ MYCOTAXON \\nIS...|\n",
      "|   Nomenclature|[@ MYCOTAXON \\nIS...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ ISSN (print) 0...|\n",
      "|   Nomenclature|[@ Studies in Myc...|\n",
      "|   Nomenclature|[@ MYCOTAXON\\nVol...|\n",
      "+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedWriter name=5>                               \n",
      "Traceback (most recent call last):\n",
      "  File \"/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"predicted_label\", \"annotated_pg\").where('predicted_label = \"Nomenclature\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca929b6a-75e5-4dcb-ace2-8d420f42ce41",
   "metadata": {},
   "source": [
    "## Build the Taxon objects and store them in CouchDB\n",
    "We use CouchDB to store a full record for each taxon. We copy all metadata to the taxon records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e285b7-0afa-486e-94f6-41bcb1d7524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDBFile(CDBF):\n",
    "    \"\"\"\n",
    "    File-like object that reads from CouchDB attachment content.\n",
    "\n",
    "    This class extends FileObject to support reading text from CouchDB\n",
    "    attachments while preserving database metadata (doc_id, attachment_name,\n",
    "    and database name).\n",
    "    \"\"\"\n",
    "\n",
    "    _doc_id: str\n",
    "    _attachment_name: str\n",
    "    _db_name: str\n",
    "    _url: Optional[str]\n",
    "    _content_lines: List[str]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        content: str,\n",
    "        doc_id: str,\n",
    "        attachment_name: str,\n",
    "        db_name: str,\n",
    "        url: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize CouchDBFile from attachment content.\n",
    "\n",
    "        Args:\n",
    "            content: Text content from CouchDB attachment\n",
    "            doc_id: CouchDB document ID\n",
    "            attachment_name: Name of the attachment (e.g., \"article.txt.ann\")\n",
    "            db_name: Database name where document is stored (ingest_db_name)\n",
    "            url: Optional URL from the CouchDB row\n",
    "        \"\"\"\n",
    "        self._doc_id = doc_id\n",
    "        self._attachment_name = attachment_name\n",
    "        self._db_name = db_name\n",
    "        self._url = url\n",
    "        self._line_number = 0\n",
    "        self._page_number = 1\n",
    "        self._empirical_page_number = None\n",
    "\n",
    "        # Split content into lines\n",
    "        self._content_lines = content.split('\\n')\n",
    "\n",
    "    def _get_content_iterator(self) -> Iterator[str]:\n",
    "        \"\"\"Get iterator over content lines.\"\"\"\n",
    "        return iter(self._content_lines)\n",
    "\n",
    "    @property\n",
    "    def filename(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a composite identifier for CouchDB documents.\n",
    "\n",
    "        Format: db_name/doc_id/attachment_name\n",
    "        This allows tracking the source of each line.\n",
    "        \"\"\"\n",
    "        return f\"{self._db_name}/{self._doc_id}/{self._attachment_name}\"\n",
    "\n",
    "    @property\n",
    "    def doc_id(self) -> str:\n",
    "        \"\"\"CouchDB document ID.\"\"\"\n",
    "        return self._doc_id\n",
    "\n",
    "    @property\n",
    "    def attachment_name(self) -> str:\n",
    "        \"\"\"Attachment filename.\"\"\"\n",
    "        return self._attachment_name\n",
    "\n",
    "    @property\n",
    "    def db_name(self) -> str:\n",
    "        \"\"\"Database name (ingest_db_name).\"\"\"\n",
    "        return self._db_name\n",
    "\n",
    "    @property\n",
    "    def url(self) -> Optional[str]:\n",
    "        \"\"\"URL from the CouchDB row.\"\"\"\n",
    "        return self._url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3de964f-332f-470e-a32e-ffef1a338047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_couchdb_partition(\n",
    "    partition: Iterator[Row],\n",
    "    db_name: str\n",
    ") -> Iterator[Line]:\n",
    "    \"\"\"\n",
    "    Read annotated files from CouchDB rows in a PySpark partition.\n",
    "\n",
    "    This is the UDF alternative to read_files() for CouchDB-backed data.\n",
    "    It processes rows containing CouchDB attachment content and yields\n",
    "    Line objects that preserve database metadata.\n",
    "\n",
    "    Args:\n",
    "        partition: Iterator of PySpark Rows with columns:\n",
    "            - doc_id: CouchDB document ID\n",
    "            - attachment_name: Attachment filename\n",
    "            - value: Text content from attachment\n",
    "        db_name: Database name to store in metadata (ingest_db_name)\n",
    "\n",
    "    Yields:\n",
    "        Line objects with content and CouchDB metadata (doc_id, attachment_name, db_name)\n",
    "\n",
    "    Example:\n",
    "        >>> # In a PySpark context\n",
    "        >>> from pyspark.sql.functions import col\n",
    "        >>> from couchdb_file import read_couchdb_partition\n",
    "        >>>\n",
    "        >>> # Assume df has columns: doc_id, attachment_name, value\n",
    "        >>> def process_partition(partition):\n",
    "        ...     lines = read_couchdb_partition(partition, \"mycobank\")\n",
    "        ...     # Process lines with finder.parse_annotated()\n",
    "        ...     return lines\n",
    "        >>>\n",
    "        >>> result = df.rdd.mapPartitions(process_partition)\n",
    "    \"\"\"\n",
    "    for row in partition:\n",
    "        # Extract url from row if available\n",
    "        url = row.doc.get('url', None)\n",
    "\n",
    "        # Create CouchDBFile object from row data\n",
    "        file_obj = CouchDBFile(\n",
    "            content=row.value,\n",
    "            doc_id=row.doc_id,\n",
    "            attachment_name=row.attachment_name,\n",
    "            db_name=db_name,\n",
    "            url=url\n",
    "        )\n",
    "\n",
    "        # Yield all lines from this file\n",
    "        for line in file_obj.read_line():\n",
    "            yield line\n",
    "\n",
    "\n",
    "def read_couchdb_rows(\n",
    "    rows: List[Row],\n",
    "    db_name: str\n",
    ") -> Iterator[Line]:\n",
    "    \"\"\"\n",
    "    Read annotated files from a list of CouchDB rows.\n",
    "\n",
    "    This is a convenience function for non-distributed processing or testing.\n",
    "    For production use with PySpark, use read_couchdb_partition().\n",
    "\n",
    "        rows: List of Rows with columns:\n",
    "            - doc_id: CouchDB document ID\n",
    "            - attachment_name: Attachment filename\n",
    "            - value: Text content from attachment\n",
    "        db_name: Database name to store in metadata\n",
    "\n",
    "    Yields:\n",
    "        Line objects with content and CouchDB metadata\n",
    "\n",
    "    Example:\n",
    "        >>> from couchdb_file import read_couchdb_rows\n",
    "        >>>\n",
    "        >>> # Collect rows from DataFrame\n",
    "        >>> rows = df.collect()\n",
    "        >>>\n",
    "        >>> # Process all lines\n",
    "        >>> lines = read_couchdb_rows(rows, \"mycobank\")\n",
    "        >>> paragraphs = parse_annotated(lines)\n",
    "        >>> taxa = group_paragraphs(paragraphs)\n",
    "    \"\"\"\n",
    "    return read_couchdb_partition(iter(rows), db_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7201b4fb-3b89-414e-b0d7-d62367bd05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_taxon_doc_id(doc_id: str, url: Optional[str], line_number: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique, deterministic document ID for a taxon.\n",
    "\n",
    "    This ensures idempotent writes - the same taxon from the same source\n",
    "    will always have the same document ID.\n",
    "\n",
    "    Args:\n",
    "        doc_id: Source document ID from ingest database\n",
    "        url: URL from the source line\n",
    "        line_number: Line number from the source\n",
    "\n",
    "    Returns:\n",
    "        Unique document ID as a hash string\n",
    "    \"\"\"\n",
    "    # Create composite key\n",
    "    key_parts = [\n",
    "        doc_id,\n",
    "        url if url else \"no_url\",\n",
    "        str(line_number)\n",
    "    ]\n",
    "    composite_key = \":\".join(key_parts)\n",
    "\n",
    "    # Generate deterministic hash\n",
    "    hash_obj = hashlib.sha256(composite_key.encode('utf-8'))\n",
    "    doc_hash = hash_obj.hexdigest()\n",
    "\n",
    "    return f\"taxon_{doc_hash}\"\n",
    "\n",
    "\n",
    "def extract_taxa_from_partition(\n",
    "    partition: Iterator[Row],\n",
    "    ingest_db_name: str\n",
    ") -> Iterator[Taxon]:\n",
    "    \"\"\"\n",
    "    Extract Taxa from a partition of CouchDB rows.\n",
    "\n",
    "    This function processes annotated files from CouchDB and yields\n",
    "    Taxon objects for further processing.\n",
    "\n",
    "    Args:\n",
    "        partition: Iterator of Rows with columns:\n",
    "            - doc_id: CouchDB document ID\n",
    "            - attachment_name: Attachment filename\n",
    "            - value: Text content\n",
    "            - url: Optional URL\n",
    "        ingest_db_name: Database name for metadata tracking\n",
    "\n",
    "    Yields:\n",
    "        Taxon objects with nomenclature and description paragraphs\n",
    "    \"\"\"\n",
    "    # Read lines from partition\n",
    "    lines = read_couchdb_partition(partition, ingest_db_name)\n",
    "\n",
    "    # Parse annotated content\n",
    "    paragraphs = parse_annotated(lines)\n",
    "\n",
    "    # Remove interstitial paragraphs\n",
    "    filtered = remove_interstitials(paragraphs)\n",
    "\n",
    "    # Convert to list to preserve paragraph objects for metadata extraction\n",
    "    filtered_list = list(filtered)\n",
    "\n",
    "    # Group into taxa (returns Taxon objects with references to paragraphs)\n",
    "    taxa = group_paragraphs(iter(filtered_list))\n",
    "\n",
    "    # Yield Taxon objects directly\n",
    "    for taxon in taxa:\n",
    "        # Only yield taxa that have nomenclature\n",
    "        if taxon.has_nomenclature():\n",
    "            yield taxon\n",
    "\n",
    "\n",
    "def convert_taxa_to_rows(partition: Iterator[Taxon]) -> Iterator[Row]:\n",
    "    \"\"\"\n",
    "    Convert Taxon objects to PySpark Rows suitable for DataFrame creation.\n",
    "\n",
    "    Args:\n",
    "        partition: Iterator of Taxon objects\n",
    "\n",
    "    Yields:\n",
    "        PySpark Row objects with fields:\n",
    "            - taxon: String of concatenated nomenclature paragraphs\n",
    "            - description: String of concatenated description paragraphs\n",
    "            - source: Dict with keys doc_id, url, db_name\n",
    "            - line_number: Line number of first nomenclature paragraph\n",
    "            - paragraph_number: Paragraph number of first nomenclature paragraph\n",
    "            - page_number: Page number of first nomenclature paragraph\n",
    "            - empirical_page_number: Empirical page number of first nomenclature paragraph\n",
    "    \"\"\"\n",
    "    for taxon in partition:\n",
    "        taxon_dict = taxon.as_row()\n",
    "        # Convert dict to Row\n",
    "        yield Row(**taxon_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22648996-4d9f-47f1-ab4f-6b5fd54f97e9",
   "metadata": {},
   "source": [
    "## Build Taxon objects\n",
    "\n",
    "Here we extract the Taxon objects from the annotated attachments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08069896-722b-45c7-8b95-78cbb89e8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_couchdb_url = couchdb_url\n",
    "ingest_username = couchdb_username\n",
    "ingest_password = couchdb_password\n",
    "pattern = '*.txt.ann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce8735e5-c6ca-4ca5-8265-7e3e1de1d8dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: string (nullable = true)\n",
      " |-- doc: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- attachment_name: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ingest_conn = CouchDBConnection(\n",
    "    ingest_couchdb_url,\n",
    "    ingest_db_name,\n",
    "    ingest_username,\n",
    "    ingest_password\n",
    ")\n",
    "\n",
    "df = ingest_conn.load_distributed(spark, pattern)\n",
    "\n",
    "# Extract taxa from each partition as Taxon objects, then convert to dicts\n",
    "# Schema matches Taxon.as_row() output format\n",
    "from pyspark.sql.types import MapType\n",
    "\n",
    "extract_schema = StructType([\n",
    "    StructField(\"taxon\", StringType(), False),\n",
    "    StructField(\"description\", StringType(), False),\n",
    "    StructField(\"source\", MapType(StringType(), StringType()), False),\n",
    "    StructField(\"line_number\", StringType(), False),\n",
    "    StructField(\"paragraph_number\", StringType(), False),\n",
    "    StructField(\"page_number\", StringType(), False),\n",
    "    StructField(\"empirical_page_number\", StringType(), True),\n",
    "])\n",
    "\n",
    "def extract_partition(partition):\n",
    "    # Extract Taxon objects\n",
    "    taxa = extract_taxa_from_partition(partition, ingest_db_name)\n",
    "    # Convert to Rows for DataFrame\n",
    "    return convert_taxa_to_rows(taxa)\n",
    "\n",
    "taxa_rdd = df.rdd.mapPartitions(extract_partition)\n",
    "taxa_df = spark.createDataFrame(taxa_rdd, extract_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317facc-23bb-4adb-9797-218fa148f79b",
   "metadata": {},
   "source": [
    "There is a problem with the model. Out of more than 2000 articles we only found 54 Nomenclature labels. There should be almost an order of magnitude more. This is not directly relevant to the current project, so this is TODO for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd35b563-4ba9-4b64-8d5b-950a5200e26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "[Stage 54:>                                                         (0 + 1) / 1]\n",
      "=== parse_annotated Label Summary ===\n",
      "Total labels counted: 1002\n",
      "\n",
      "Label distribution:\n",
      "  Misc-exposition              783 ( 78.1%)\n",
      "  Description                  188 ( 18.8%)\n",
      "  Nomenclature                  30 (  3.0%)\n",
      "  None                           1 (  0.1%)\n",
      "========================================\n",
      "[Stage 55:>                                                         (0 + 1) / 1]\n",
      "=== parse_annotated Label Summary ===\n",
      "Total labels counted: 1003\n",
      "\n",
      "Label distribution:\n",
      "  Misc-exposition              800 ( 79.8%)\n",
      "  Description                  178 ( 17.7%)\n",
      "  Nomenclature                  24 (  2.4%)\n",
      "  None                           1 (  0.1%)\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for t in taxa_rdd.toLocalIterator():\n",
    "    t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db587ef2-1bf8-4e6c-b13f-780465ac4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save taxa to taxon database\n",
    "save_schema = StructType([\n",
    "    StructField(\"doc_id\", StringType(), False),\n",
    "    StructField(\"success\", BooleanType(), False),\n",
    "    StructField(\"error_message\", StringType(), False),\n",
    "])\n",
    "\n",
    "def save_partition(partition):\n",
    "    return save_taxa_to_couchdb_partition(\n",
    "        partition,\n",
    "        taxon_couchdb_url,\n",
    "        taxon_db_name,\n",
    "        taxon_username,\n",
    "        taxon_password\n",
    "    )\n",
    "\n",
    "results_df = taxa_df.rdd.mapPartitions(save_partition).toDF(save_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ee719-eb73-425b-ad44-6f30158c8d5e",
   "metadata": {},
   "source": [
    "## Dr. Drafts document embedding\n",
    "\n",
    "Dr. Drafts loads documents from CouchDB. Save the model to redis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f1457-b1bf-4e02-9b0c-ce194f33969d",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* doi Foundation, \"DOI Citation Formatter HTTP API\", https://citation.doi.org/api-docs.html, accessed 2025-11-12.\n",
    "* Yang, Jie and Zhang, Yue and Li, Linwei and Li, Xingxuan, 2018, \"YEDDA: A Lightweight Collaborative Text Span Annotation Tool\", Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, http://aclweb.org/anthology/P18-4006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82412f-0fd7-4526-a949-3732c2b26624",
   "metadata": {},
   "source": [
    "## Appendix: On the use of an AI Coder\n",
    "\n",
    "Portions of this work were completed with the aid of Claude Code Pro. I wish to give a clarifying example of how I've used this very powerful tool, and reveal why I am comfortable with claiming authorship of the resulting code.\n",
    "\n",
    "For this project I needed results from an earlier class project in which a trio of students built and evaluated models for classifying paragraphs. The earlier work was built as a iPython Notebook, with many examples and inline code. Just copying the earlier notebook would have introduced many irrelevant details and would not further the overall project.\n",
    "\n",
    "I asked Claude Code to translate the notebook into a module that I could import. It did a pretty good job. Without being told, it made a submodule, extract the illustrative code as examples, wrote reasonable documentation and created packaging for the module.\n",
    "\n",
    "The skill level of the coding was roughly that of a highly disciplined average junior programmer. The architecture was simplistic and violated several design constraints such as DRY. I requested specific refactorings, such as asking for a group of functions to be converted into an object that shared duplicated parameters.\n",
    "\n",
    "The initial code used REST interfaces directly, and read all the data into a single machine, not using pyspark correctly. Through a series of refactorings, I asked that the code use appropriate libraries I named, and create correct udf functions to execute transformations in parallel.\n",
    "\n",
    "I walked the AI through creating an object that I could use to illustrate my use of redis and couchdb interfaces, while leaving the irrelevant details in a separate library.\n",
    "\n",
    "In short, I still have to understand good design principles. I have to be able to recognize where appropriate libraries were applicable. I still have to understand the frameworks I am working with.\n",
    "\n",
    "I now have a strong understanding of the difference between \"vibe coding\" and AI-assisted software engineering. In my first 4 hours with Claude Code, I was able to produce roughly 4 days' worth of professional-grade working code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
