{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a626f7e-4127-4227-9906-dc08dd9135ce",
   "metadata": {},
   "source": [
    "# SKOL IV: All the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0692319-632c-402b-8db5-c7f63fe7d3e0",
   "metadata": {},
   "source": [
    "Synoptic Key of Life (SKOL) is a web site and application that aims to provide easy access to all of the open taxonomic literature in Mycology. A synoptic key is a tool that helps you identify an organism making successive observations, building up a detailed description of the organism in front of you. There are many fine synoptic keys available for particular taxa, but they are all hand-built. SKOL uses AI to build the synoptic key automatically.\n",
    "\n",
    "The goal is to make it easier for advanced amateur mycologists to build technical descriptions of fungi.''\n",
    "\n",
    "## Storage needs\n",
    "\n",
    "SKOL uses a diverse set of databases to hold different artifacts.\n",
    "\n",
    "The original literature is ingested into the document database CouchDB (citation needed) along with available publication metadata. The originals are typically PDF files which are stored as attachments on the CouchDB ingestion records.\n",
    "\n",
    "Text is extracted from the ingested files, using OCR if necessary. This text is a second attachment on the ingestion record.\n",
    "\n",
    "A classifier is trained from hand-annotated articles and stored in Redis. The model has an expiration period of several weeks. The classifier then annotates each text document with labels for Nomenclature, Description, and Misc-exposition. It stores these annotated articles as attachments on the CouchDB ingestion records.\n",
    "\n",
    "Taxon names (typically species names with literature annotation) and combined with matching descriptions into Taxon records and stored in another CouchDB database. These records are the core data for SKOL.\n",
    "\n",
    "The Taxon records are processed a number of ways: sentence embedding, JSON encoding, and artificial cladograms.\n",
    "\n",
    "The sentence embedding is done with an SBERT model (citation needed) and stored as a blob in Redis. The embedding has an expiration period of 24 hours.\n",
    "\n",
    "A Mistral model (citation needed) converts each Taxon record description is converted into a hierarchy of features, subfeatures, and values. The epectation is that these data structures will eventually form the basis of pull-down menus in the SKOL user interface. These JSON structures are stored in another CouchDB database.\n",
    "\n",
    "The sentence embeddings are further processed into a single tree of Taxon reccords based on their distance from each other in the sentence embedding space. This tree is stored in a neo4j database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ae6b20-7abd-4ef6-b2cf-8d221a40d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/11 22:27:28 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 172.16.227.68 instead (on interface wlp130s0f0)\n",
      "25/12/11 22:27:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0ee4e82c-db0b-4e74-a211-a0f9c1a0d605;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 222ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0ee4e82c-db0b-4e74-a211-a0f9c1a0d605\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/6ms)\n",
      "25/12/11 22:27:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Spark context Web UI available at http://172.16.227.68:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1765492051742).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 11.0.1)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> :quit\n"
     ]
    }
   ],
   "source": [
    "bahir_package = 'org.apache.bahir:spark-sql-cloudant_2.12:2.4.0'\n",
    "!spark-shell --packages $bahir_package < /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41b1092-2658-441b-8c72-ee758d7759af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Forces synchronous execution, making it easier to track GPU operations.\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' \n",
    "\n",
    "# Enables basic CUDA debug logging.\n",
    "os.environ['CUDA_DEBUG'] = '1' \n",
    "\n",
    "# Other potentially useful variables for more detailed logging:\n",
    "# os.environ['CUDA_API_CALLS'] = '1' # Logs CUDA API calls\n",
    "os.environ['CUDA_LOG_LEVEL'] = 'DEBUG' # Or 'DEBUG', 'WARNING', etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9615b87-4962-47ca-b10f-98558713196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765492055.529115  419100 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "I0000 00:00:1765492055.558996  419100 cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765492056.186547  419100 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import pickle\n",
    "import requests\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('error', category=UserWarning)\n",
    "\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/nvidia/cusparselt/lib'\n",
    "\n",
    "# Be sure to get version 2: https://simple-repository.app.cern.ch/project/bibtexparser/2.0.0b8/description\n",
    "import bibtexparser\n",
    "import couchdb\n",
    "import feedparser\n",
    "import fitz # PyMuPDF\n",
    "\n",
    "import pandas as pd  # TODO(piggy): Remove this dependency in favor of pure pyspark DataFrames.\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, CountVectorizer, IDF, StringIndexer, VectorAssembler, IndexToString\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name, collect_list, regexp_extract, col, udf,\n",
    "    explode, trim, row_number, min, expr, concat, lit\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, BooleanType, IntegerType, MapType, NullType,\n",
    "    StringType, StructType, StructField\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import redis\n",
    "from uuid import uuid4\n",
    "\n",
    "# Local modules\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "parent_path = Path(parent_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from couchdb_file import CouchDBFile as CDBF\n",
    "from extract_taxa_to_couchdb import (\n",
    "    TaxonExtractor,\n",
    "    generate_taxon_doc_id,\n",
    "    extract_taxa_from_partition,\n",
    "    convert_taxa_to_rows\n",
    ")\n",
    "from fileobj import FileObject\n",
    "from finder import parse_annotated, remove_interstitials\n",
    "import line\n",
    "from line import Line\n",
    "\n",
    "# Import SKOL classifiers\n",
    "from skol_classifier.classifier_v2 import SkolClassifierV2 as SC\n",
    "from skol_classifier.couchdb_io import CouchDBConnection as CDBC\n",
    "from skol_classifier.model import SkolModel\n",
    "from skol_classifier.output_formatters import CouchDBOutputWriter as CDBOW, YeddaFormatter\n",
    "from skol_classifier.preprocessing import SuffixTransformer, ParagraphExtractor\n",
    "from skol_classifier.utils import get_file_list\n",
    "\n",
    "from taxon import group_paragraphs, Taxon\n",
    "\n",
    "from taxa_json_translator import TaxaJSONTranslator as TJT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd14626-a4bd-4684-81ef-ec5682a1d9aa",
   "metadata": {},
   "source": [
    "## Important constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc77721a-05a7-48f0-8780-fa71734ff01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "couchdb_host = \"127.0.0.1:5984\" # e.g., \"ACCOUNT.cloudant.com\" or \"localhost\"\n",
    "couchdb_username = \"admin\"\n",
    "couchdb_password = \"SU2orange!\"\n",
    "ingest_db_name = \"skol_dev\"  # Development ingestion database\n",
    "taxon_db_name = \"skol_taxa_dev\"  # Development Taxa database\n",
    "json_taxon_db_name = \"skol_taxa_full_dev\"  # Development Taxa database with JSON translations\n",
    "\n",
    "redis_host = 'localhost'\n",
    "redis_port = 6379\n",
    "\n",
    "embedding_name = 'skol:embedding:v1.3'\n",
    "embedding_expire = 60 * 60 * 24  # Expire after 24 hours\n",
    "classifier_model_name = \"skol:classifier:model:rnn-v1.6\"\n",
    "classifier_model_expire = 60 * 60 * 24  # Expire after 1 day.\n",
    "\n",
    "neo4j_uri = \"bolt://localhost:7687\"\n",
    "\n",
    "couchdb_url = f'http://{couchdb_host}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe0cba-ccae-4b9f-aff7-adccf30df427",
   "metadata": {},
   "source": [
    "## robots.txt\n",
    "\n",
    "We want to be a well-behaved web scraper. Respect `robots.txt`, a standardized file that tells us what parts of a web site a scraper is allowed to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca180b6-224c-497b-b5c2-7ca3673d46e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: cloudant.protocol\n",
      "Warning: Ignoring non-Spark config property: cloudant.password\n",
      "Warning: Ignoring non-Spark config property: cloudant.host\n",
      "Warning: Ignoring non-Spark config property: cloudant.username\n",
      "25/12/11 22:27:37 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 172.16.227.68 instead (on interface wlp130s0f0)\n",
      "25/12/11 22:27:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1441bea5-9139-44fc-9a80-4ed36a033574;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 216ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1441bea5-9139-44fc-9a80-4ed36a033574\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/6ms)\n",
      "25/12/11 22:27:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "user_agent = \"synoptickeyof.life\"\n",
    "\n",
    "ingenta_rp = RobotFileParser()\n",
    "ingenta_rp.set_url(\"https://www.ingentaconnect.com/robots.txt\")\n",
    "ingenta_rp.read() # Reads and parses the robots.txt file from the URL\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CouchDB Spark SQL Example in Python using dataframes\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"cloudant.protocol\", \"http\") \\\n",
    "    .config(\"cloudant.host\", couchdb_host) \\\n",
    "    .config(\"cloudant.username\", couchdb_username) \\\n",
    "    .config(\"cloudant.password\", couchdb_password) \\\n",
    "    .config(\"spark.jars.packages\", bahir_package) \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\",\n",
    "            \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.security.action=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.util.calendar=ALL-UNNAMED\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\",\n",
    "            \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.security.action=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.util.calendar=ALL-UNNAMED\") \\\n",
    "    .config(\"spark.submit.pyFiles\",\n",
    "            f'{parent_path / \"line.py\"},{parent_path / \"fileobj.py\"},'\n",
    "            f'{parent_path / \"couchdb_file.py\"},{parent_path / \"finder.py\"},'\n",
    "            f'{parent_path / \"taxon.py\"},{parent_path / \"paragraph.py\"},'\n",
    "            f'{parent_path / \"label.py\"},{parent_path / \"file.py\"},'\n",
    "            f'{parent_path / \"extract_taxa_to_couchdb.py\"}'\n",
    "           ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!\n",
    "\n",
    "couch = couchdb.Server(couchdb_url)\n",
    "couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "\n",
    "if ingest_db_name not in couch:\n",
    "    db = couch.create(ingest_db_name)\n",
    "else:\n",
    "    db = couch[ingest_db_name]\n",
    "\n",
    "# Connect to Redis\n",
    "redis_client = redis.Redis(\n",
    "    host=redis_host,\n",
    "    port=redis_port,\n",
    "    db=0,\n",
    "    decode_responses=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e7f93-bb66-4922-b3f9-c9429bc304ac",
   "metadata": {},
   "source": [
    "## The Data Sources\n",
    "\n",
    "The goal is to collect all the open access taxonomic literature in Mycology. Most of the sources below mainly cover macro-fungi and slime molds.\n",
    "\n",
    "### Ingested Data Sources\n",
    "\n",
    "* [Mycotaxon at Ingenta Connect](https://www.ingentaconnect.com/content/mtax/mt)\n",
    "* [Studies in Mycology at Ingenta Connect](https://www.studiesinmycology.org/)\n",
    "\n",
    "### Source of many older public domain and open access works\n",
    "\n",
    "Mycoweb includes scans of many older works in mycology. I have local copies but need to write ingesters for them.\n",
    "\n",
    "* [Mycoweb](https://mykoweb.com/)\n",
    "\n",
    "### Journals in hand\n",
    "\n",
    "These are journals we've collected over the years. The initial annotated issues are from early years of Mycotaxon. We still need to write ingesters for all of these.\n",
    "\n",
    "* Mycologia (back issues)\n",
    "* [Mycologia at Taylor and Francis](https://www.tandfonline.com/journals/umyc20)\n",
    "  Mycologia is the main journal of the Mycological Society of America. It is a mix of open access and traditional access articles. The connector for this journal will need to identify the open access articles.\n",
    "* Persoonia (all issues)\n",
    "  Persoonia is no longer published.\n",
    "* Mycotaxon (back issues)\n",
    "  Mycotaxon is no longer published.\n",
    "\n",
    "### Journals that need connectors\n",
    "\n",
    "These are journals we're aware that include open access articles.\n",
    "\n",
    "* [Amanitaceae.org](http://www.tullabs.com/amanita/?home)\n",
    "* [Mycosphere](https://mycosphere.org/)\n",
    "* [Mycoscience](https://mycoscience.org/)\n",
    "* [Journal of Fungi](https://www.mdpi.com/journal/jof)\n",
    "* [Mycology](https://www.tandfonline.com/journals/tmyc20)\n",
    "* [Open Access Journal of Mycology & Mycological Sciences](https://www.medwinpublishers.com/OAJMMS/)\n",
    "* [Mycokeys](https://mycokeys.pensoft.net/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a4213-adb5-49e5-b973-570a75cc2cce",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Each journal or other data source gets an ingester that puts PDFs into our document store along with any metadata we can collect. The metadata is sufficient to create citations for each issue, book, or article. If bibtex citations are available we prefer to store these verbatim.\n",
    "\n",
    "### Ingenta RSS ingestion\n",
    "\n",
    "Ingenta Connect is an electronic publisher that holds two Mycology journals. New articles are available via RSS (Really Simple Syndication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d303cc7-f01c-4e15-87ff-6c259d010591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_bibtex(\n",
    "        db: couchdb.Database,\n",
    "        content: bytes,\n",
    "        bibtex_link: str,\n",
    "        meta: Dict[str, Any],\n",
    "        rp\n",
    "        ) -> None:\n",
    "    \"\"\"Load documents referenced in an Ingenta BibTeX database.\"\"\"\n",
    "    bib_database = bibtexparser.parse_string(content)\n",
    "\n",
    "    bibtex_data = {\n",
    "        'link': bibtex_link,\n",
    "        'bibtex': bibtexparser.write_string(bib_database),\n",
    "    }\n",
    "\n",
    "    for bib_entry in bib_database.entries:\n",
    "        doc = {\n",
    "            '_id': uuid4().hex,\n",
    "            'meta': meta,\n",
    "            'pdf_url': f\"{bib_entry['url']}?crawler=true\",\n",
    "        }\n",
    "\n",
    "        # Do not fetch if we already have an entry.\n",
    "        selector = {'selector': {'pdf_url': doc['pdf_url']}}\n",
    "        found = False\n",
    "        for e in db.find(selector):\n",
    "            found = True\n",
    "        if found:\n",
    "            print(f\"Skipping {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        if not rp.can_fetch(user_agent, doc['pdf_url']):\n",
    "            # TODO(piggy): We should probably log blocked URLs.\n",
    "            print(f\"Robot permission denied {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Adding {doc['pdf_url']}\")\n",
    "        for k in bib_entry.fields_dict.keys():\n",
    "            doc[k] = bib_entry[k]\n",
    "\n",
    "        doc_id, doc_rev = db.save(doc)\n",
    "        with requests.get(doc['pdf_url'], stream=False) as pdf_f:\n",
    "            pdf_f.raise_for_status()\n",
    "            pdf_doc = pdf_f.content\n",
    "\n",
    "        attachment_filename = 'article.pdf'\n",
    "        attachment_content_type = 'application/pdf'\n",
    "        attachment_file = BytesIO(pdf_doc)\n",
    "\n",
    "        db.put_attachment(doc, attachment_file, attachment_filename, attachment_content_type)\n",
    "\n",
    "        print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b323f98-5bd8-4106-b79c-2ff9ac1ac74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_ingenta(\n",
    "        db: couchdb.Database,\n",
    "        rss_url: str,\n",
    "        rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest documents from an Ingenta RSS feed.\"\"\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    feed_meta = {\n",
    "        'url': rss_url,_utils/#/_al\n",
    "        'title': feed.feed.title,\n",
    "        'link': feed.feed.link,\n",
    "        'description': feed.feed.description,\n",
    "    }\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        entry_meta = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "        }\n",
    "        if hasattr(entry, 'summary'):\n",
    "            entry_meta['summary'] = entry.summary\n",
    "        if hasattr(entry, 'description'):\n",
    "            entry_meta['description'] = entry.description\n",
    "\n",
    "        bibtex_link = f'{entry.link}?format=bib'\n",
    "        print(f\"bibtex_link: {bibtex_link}\")\n",
    "\n",
    "        if not rp.can_fetch(user_agent, bibtex_link):\n",
    "            print(f\"Robot permission denied {bibtex_link}\")\n",
    "            continue\n",
    "\n",
    "        with requests.get(bibtex_link, stream=False) as bibtex_f:\n",
    "            bibtex_f.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            ingest_from_bibtex(\n",
    "                db=db,\n",
    "                content=bibtex_f.content\\\n",
    "                    .replace(b\"\\\"\\nparent\", b\"\\\",\\nparent\")\\\n",
    "                    .replace(b\"\\n\", b\"\"),\n",
    "                bibtex_link=bibtex_link,\n",
    "                meta={\n",
    "                    'feed': feed_meta,\n",
    "                    'entry': entry_meta,\n",
    "                },\n",
    "                rp=rp\n",
    "            )\n",
    "        print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234f9cc5-9b7e-4559-ad10-f24ad269e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_local_bibtex(\n",
    "    db: couchdb.Database,\n",
    "    root: Path,\n",
    "    rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest from a local directory with Ingenta bibtext files in it.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if not filename.endswith('format=bib'):\n",
    "                continue\n",
    "            full_filepath = os.path.join(dirpath, filename)\n",
    "            bibtex_link = f\"https://www.ingentaconnect.com/{full_filepath[len(str(root)):]}\"\n",
    "            with open(full_filepath) as f:\n",
    "                # Paper over a syntax problem in Ingenta Connect Bibtex files.\n",
    "                content = f.read()\\\n",
    "                    .replace(\"\\\"\\nparent\", \"\\\",\\nparent\")\\\n",
    "                    .replace(\"\\n\", \"\")\n",
    "                ingest_from_bibtex(db, content, bibtex_link, meta={}, rp=rp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58829ec0-93f7-44dd-87bc-511ab2d238d0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mycotaxon\n",
    "\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/mtax/mt?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40602cc3-e231-497d-adc7-d6d4471514cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Studies in Mycology\n",
    "\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/wfbi/sim?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "651f82ea-b3b8-4f0d-adf0-f5550d7ed6d5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ingest_from_local_bibtex(\n",
    "    db=db,\n",
    "    root=Path(\"/data/skol/www/www.ingentaconnect.com\"),\n",
    "    rp=ingenta_rp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf05ad-d2bf-4716-b253-07455cfefcd0",
   "metadata": {},
   "source": [
    "### Text extraction\n",
    "\n",
    "We extract the text, optionally with OCR. Add as an additional attachment on the source record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5e9725-0c3a-4c41-8ebf-2929c4dc4334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\n",
    "    format=\"org.apache.bahir.cloudant\",\n",
    "    database=ingest_db_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e75c64e-95cb-478a-9a85-46d1297e4225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _id: string, _rev: string, abstract: string, author: string, doi: string, eissn: string, issn: string, itemtype: string, journal: string, number: string, pages: string, parent_itemid: string, pdf_url: string, publication date: string, publishercode: string, title: string, url: string, volume: string, year: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9befa546-a895-4a13-8744-d512587d9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Type: text/html; charset=UTF-8\n",
    "\n",
    "def pdf_to_text(pdf_contents: bytes) -> bytes:\n",
    "    doc = fitz.open(stream=BytesIO(pdf_contents), filetype=\"pdf\")\n",
    "\n",
    "    full_text = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        # Possibly perform OCR on the page\n",
    "        text = page.get_text(\"text\", flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_DEHYPHENATE)\n",
    "        # full_text += f\"\\n--- PDF Page {page_num+1} ---\\n\"  # TODO(piggy): Introduce PDF page tracking in line-by-line and paragraph parsers.\n",
    "        full_text += text\n",
    "\n",
    "    return full_text.encode(\"utf-8\")\n",
    "\n",
    "def add_text_to_partition(iterator) -> None:\n",
    "    couch = couchdb.Server(couchdb_url)\n",
    "    couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "    local_db = couch[ingest_db_name]\n",
    "    for row in iterator:\n",
    "        if not row:\n",
    "            continue\n",
    "        if not row._attachments:\n",
    "            continue\n",
    "        row_dict = row.asDict()\n",
    "        attachment_dict = row._attachments.asDict()\n",
    "        for pdf_filename in attachment_dict:\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            if pdf_path.suffix != '.pdf':\n",
    "                continue\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            txt_path_str = pdf_path.stem + '.txt'\n",
    "            # if txt_path_str in attachment_dict:\n",
    "            #     # TODO(piggy): Recalculate text if text is terrible. Too much noise vocabulary?\n",
    "            #     print(f\"Already have text for {row.pdf_url}\")\n",
    "            #     continue\n",
    "            print(f\"{row._id}, {row.pdf_url}\")\n",
    "            pdf_file = local_db.get_attachment(row._id, str(pdf_path)).read()\n",
    "            txt_file = pdf_to_text(pdf_file)\n",
    "            attachment_content_type = 'text/simple; charset=UTF-8'\n",
    "            attachment_file = BytesIO(txt_file)\n",
    "            local_db.put_attachment(row_dict, attachment_file, txt_path_str, attachment_content_type)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74366376-ec05-474f-b172-216093c62865",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df.select(\"*\").foreachPartition(add_text_to_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11805c62-ae42-4959-9673-1e09261a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to skol_classifier.CouchDBConnection.\n",
    "\n",
    "class CouchDBConnection(CDBC):\n",
    "    \"\"\"\n",
    "    Manages CouchDB connection and provides I/O operations.\n",
    "\n",
    "    This class encapsulates connection parameters and provides an idempotent\n",
    "    connection method that can be safely called multiple times.\n",
    "    \"\"\"\n",
    "    # Shared schema definitions (DRY principle)\n",
    "    LOAD_SCHEMA = StructType([\n",
    "        StructField(\"doc_id\", StringType(), False),\n",
    "        StructField(\"human_url\", StringType(), False),\n",
    "        StructField(\"attachment_name\", StringType(), False),\n",
    "        StructField(\"value\", StringType(), False),\n",
    "    ])\n",
    "\n",
    "    SAVE_SCHEMA = StructType([\n",
    "        StructField(\"doc_id\", StringType(), False),\n",
    "        StructField(\"attachment_name\", StringType(), False),\n",
    "        StructField(\"success\", BooleanType(), False),\n",
    "    ])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize CouchDB connection parameters.\n",
    "\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL (e.g., \"http://localhost:5984\")\n",
    "            database: Database name\n",
    "            username: Optional username for authentication\n",
    "            password: Optional password for authentication\n",
    "        \"\"\"\n",
    "        self.couchdb_url = couchdb_url\n",
    "        self.database = database\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self._server = None\n",
    "        self._db = None\n",
    "\n",
    "    def _connect(self):\n",
    "        \"\"\"\n",
    "        Idempotent connection method that returns a CouchDB server object.\n",
    "\n",
    "        This method can be called multiple times safely - it will only create\n",
    "        a connection if one doesn't already exist.\n",
    "\n",
    "        Returns:\n",
    "            couchdb.Server: Connected CouchDB server object\n",
    "        \"\"\"\n",
    "        if self._server is None:\n",
    "            self._server = couchdb.Server(self.couchdb_url)\n",
    "            if self.username and self.password:\n",
    "                self._server.resource.credentials = (self.username, self.password)\n",
    "\n",
    "        if self._db is None:\n",
    "            self._db = self._server[self.database]\n",
    "\n",
    "        return self._server\n",
    "\n",
    "    @property\n",
    "    def db(self):\n",
    "        \"\"\"Get the database object, connecting if necessary.\"\"\"\n",
    "        if self._db is None:\n",
    "            self._connect()\n",
    "        return self._db\n",
    "\n",
    "    def get_all_doc_ids(self, pattern: str = \"*\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of document IDs matching the pattern from CouchDB.\n",
    "\n",
    "        Args:\n",
    "            pattern: Pattern for document IDs (e.g., \"taxon_*\", \"*\")\n",
    "                    - \"*\" matches all non-design documents\n",
    "                    - \"prefix*\" matches documents starting with prefix\n",
    "                    - \"exact\" matches exactly\n",
    "\n",
    "        Returns:\n",
    "            List of matching document IDs\n",
    "        \"\"\"\n",
    "        db = self.db\n",
    "\n",
    "        # Get all document IDs (excluding design documents)\n",
    "        all_doc_ids = [doc_id for doc_id in list(db) if not doc_id.startswith('_design/')]\n",
    "\n",
    "        # Filter by pattern\n",
    "        if pattern == \"*\":\n",
    "            # Return all non-design documents\n",
    "            return all_doc_ids\n",
    "        elif pattern.endswith('*'):\n",
    "            # Prefix matching\n",
    "            prefix = pattern[:-1]\n",
    "            return [doc_id for doc_id in all_doc_ids if doc_id.startswith(prefix)]\n",
    "        else:\n",
    "            # Exact match\n",
    "            return [doc_id for doc_id in all_doc_ids if doc_id == pattern]\n",
    "\n",
    "    def get_document_list(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Get a list of documents with text attachments from CouchDB.\n",
    "\n",
    "        This only fetches document metadata (not content) to create a DataFrame\n",
    "        that can be processed in parallel. Creates ONE ROW per attachment, so if\n",
    "        a document has multiple attachments matching the pattern, it will have\n",
    "        multiple rows in the resulting DataFrame.\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names (e.g., \"*.txt\")\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, attachment_name\n",
    "            One row per (doc_id, attachment_name) pair\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB (driver only)\n",
    "        db = self.db\n",
    "\n",
    "        # Get all documents with attachments matching pattern\n",
    "        doc_list = []\n",
    "        for doc_id in db:\n",
    "            try:\n",
    "                doc = db[doc_id]\n",
    "                attachments = doc.get('_attachments', {})\n",
    "\n",
    "                # Loop through ALL attachments in the document\n",
    "                for att_name in attachments.keys():\n",
    "                    # Check if attachment matches pattern\n",
    "                    # Pattern matching: \"*.txt\" matches files ending with .txt\n",
    "                    if pattern == \"*.txt\" and att_name.endswith('.txt'):\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern == \"*.*\" or pattern == \"*\":\n",
    "                        # Match all attachments\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern.startswith(\"*.\") and att_name.endswith(pattern[1:]):\n",
    "                        # Generic pattern matching for *.ext\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "            except Exception:\n",
    "                # Skip documents we can't read\n",
    "                continue\n",
    "\n",
    "        # Create DataFrame with document IDs and attachment names\n",
    "        schema = StructType([\n",
    "            StructField(\"doc_id\", StringType(), False),\n",
    "            StructField(\"attachment_name\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        return spark.createDataFrame(doc_list, schema)\n",
    "\n",
    "    def fetch_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row]\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Fetch CouchDB attachments for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id and attachment_name\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, human_url, attachment_name, and value (content).\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document has multiple .txt attachments, there will be multiple rows\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Get the specific attachment for this row\n",
    "                    if row.attachment_name in doc.get('_attachments', {}):\n",
    "                        attachment = db.get_attachment(doc, row.attachment_name)\n",
    "                        if attachment:\n",
    "                            content = attachment.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "                            yield Row(\n",
    "                                doc_id=row.doc_id,\n",
    "                                human_url=doc.get('url', 'missing_human_url'),\n",
    "                                attachment_name=row.attachment_name,\n",
    "                                value=content\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    # Log error but continue processing\n",
    "                    print(f\"Error fetching {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            return\n",
    "\n",
    "    def save_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row],\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Save annotated content to CouchDB for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id, attachment_name, final_aggregated_pg\n",
    "                       and optionally human_url\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, attachment_name, and success status.\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document had multiple .txt files, we save multiple .ann files\n",
    "            for row in partition:\n",
    "                success = False\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Update human_url field if provided\n",
    "                    if hasattr(row, 'human_url') and row.human_url:\n",
    "                        doc['url'] = row.human_url\n",
    "                        db.save(doc)\n",
    "                        # Reload doc to get updated _rev\n",
    "                        doc = db[row.doc_id]\n",
    "\n",
    "                    # Create new attachment name by appending suffix\n",
    "                    # e.g., \"article.txt\" becomes \"article.txt.ann\"\n",
    "                    new_attachment_name = f\"{row.attachment_name}{suffix}\"\n",
    "\n",
    "                    # Save the annotated content as a new attachment\n",
    "                    db.put_attachment(\n",
    "                        doc,\n",
    "                        row.final_aggregated_pg.encode('utf-8'),\n",
    "                        filename=new_attachment_name,\n",
    "                        content_type='text/plain'\n",
    "                    )\n",
    "\n",
    "                    success = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=success\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            # Yield failures for all rows\n",
    "            for row in partition:\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n",
    "    def load_distributed(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Load text attachments from CouchDB using foreachPartition.\n",
    "\n",
    "        This function:\n",
    "        1. Gets list of documents (on driver)\n",
    "        2. Creates a DataFrame with doc IDs\n",
    "        3. Uses mapPartitions to fetch content efficiently (one connection per partition)\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, attachment_name, and value.\n",
    "        \"\"\"\n",
    "        # Get document list\n",
    "        doc_df = self.get_document_list(spark, pattern)\n",
    "\n",
    "        # Use mapPartitions for efficient batch fetching\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def fetch_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.fetch_partition(partition)\n",
    "\n",
    "        # Apply mapPartitions using shared schema\n",
    "        result_df = doc_df.rdd.mapPartitions(fetch_partition).toDF(self.LOAD_SCHEMA)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def save_distributed(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Save annotated predictions to CouchDB using foreachPartition.\n",
    "\n",
    "        This function uses mapPartitions where each partition creates a single\n",
    "        CouchDB connection and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with columns: doc_id, attachment_name, final_aggregated_pg\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with doc_id, attachment_name, and success columns\n",
    "        \"\"\"\n",
    "        # Use mapPartitions for efficient batch saving\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def save_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.save_partition(partition, suffix)\n",
    "\n",
    "        # Apply mapPartitions using shared schema\n",
    "        result_df = df.rdd.mapPartitions(save_partition).toDF(self.SAVE_SCHEMA)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def process_partition_with_func(\n",
    "        self,\n",
    "        partition: Iterator[Row],\n",
    "        processor_func,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Generic function to read, process, and save in one partition operation.\n",
    "\n",
    "        This allows custom processing logic while maintaining single connection per partition.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows\n",
    "            processor_func: Function to process content (takes content string, returns processed string)\n",
    "            suffix: Suffix for output attachment\n",
    "\n",
    "        Yields:\n",
    "            Rows with processing results, including success status for logging.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Fetch\n",
    "                    if row.attachment_name in doc.get('_attachments', {}):\n",
    "                        attachment = db.get_attachment(doc, row.attachment_name)\n",
    "                        if attachment:\n",
    "                            content = attachment.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "                            # Process\n",
    "                            processed = processor_func(content)\n",
    "\n",
    "                            # Save\n",
    "                            new_attachment_name = f\"{row.attachment_name}{suffix}\"\n",
    "                            db.put_attachment(\n",
    "                                doc,\n",
    "                                processed.encode('utf-8'),\n",
    "                                filename=new_attachment_name,\n",
    "                                content_type='text/plain'\n",
    "                            )\n",
    "\n",
    "                            yield Row(\n",
    "                                doc_id=row.doc_id,\n",
    "                                attachment_name=row.attachment_name,\n",
    "                                success=True\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            for row in partition:\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "689d4df6-59c4-4ac4-9f9a-e849bd3b21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDBOutputWriter(CDBOW):\n",
    "    \"\"\"\n",
    "    Writes predictions back to CouchDB as attachments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: str,\n",
    "        password: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the writer.\n",
    "\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL\n",
    "            database: Database name\n",
    "            username: CouchDB username\n",
    "            password: CouchDB password\n",
    "        \"\"\"\n",
    "        self.conn = CouchDBConnection(\n",
    "            couchdb_url=couchdb_url,\n",
    "            database=database,\n",
    "            username=username,\n",
    "            password=password\n",
    "        )\n",
    "\n",
    "    def save_annotated(\n",
    "        self,\n",
    "        predictions: DataFrame,\n",
    "        suffix: str = \".ann\",\n",
    "        coalesce_labels: bool = False,\n",
    "        line_level: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save predictions to CouchDB as attachments.\n",
    "\n",
    "        Args:\n",
    "            predictions: DataFrame with predictions\n",
    "            suffix: Suffix for attachment names\n",
    "            coalesce_labels: Whether to coalesce consecutive labels\n",
    "            line_level: Whether data is line-level\n",
    "        \"\"\"\n",
    "        # Format predictions\n",
    "        if \"annotated_value\" not in predictions.columns:\n",
    "            predictions = YeddaFormatter.format_predictions(predictions)\n",
    "\n",
    "        # Coalesce if requested\n",
    "        if coalesce_labels and line_level:\n",
    "            predictions = YeddaFormatter.coalesce_consecutive_labels(\n",
    "                predictions, line_level=True\n",
    "            )\n",
    "            # For coalesced output, we have coalesced_annotations column\n",
    "            # We need to join them into final_aggregated_pg\n",
    "            from pyspark.sql.functions import expr\n",
    "            predictions = predictions.withColumn(\n",
    "                \"final_aggregated_pg\",\n",
    "                expr(\"array_join(coalesced_annotations, '\\n')\")\n",
    "            )\n",
    "        else:\n",
    "            # Aggregate annotated values by document\n",
    "            groupby_col = \"doc_id\" if \"doc_id\" in predictions.columns else \"filename\"\n",
    "            attachment_col = \"attachment_name\" if \"attachment_name\" in predictions.columns else \"filename\"\n",
    "\n",
    "            # Check if we have line_number for ordering\n",
    "            if \"line_number\" in predictions.columns:\n",
    "                from pyspark.sql.functions import expr, first\n",
    "                # Preserve human_url if it exists\n",
    "                if \"human_url\" in predictions.columns:\n",
    "                    predictions = (\n",
    "                        predictions.groupBy(groupby_col, attachment_col)\n",
    "                        .agg(\n",
    "                            expr(\"sort_array(collect_list(struct(line_number, annotated_value)))\").alias(\"sorted_list\"),\n",
    "                            first(\"human_url\").alias(\"human_url\")\n",
    "                        )\n",
    "                        .withColumn(\"annotated_value_ordered\", expr(\"transform(sorted_list, x -> x.annotated_value)\"))\n",
    "                        .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotated_value_ordered, '\\n')\"))\n",
    "                        .select(groupby_col, \"human_url\", attachment_col, \"final_aggregated_pg\")\n",
    "                    )\n",
    "                else:\n",
    "                    predictions = (\n",
    "                        predictions.groupBy(groupby_col, attachment_col)\n",
    "                        .agg(\n",
    "                            expr(\"sort_array(collect_list(struct(line_number, annotated_value)))\").alias(\"sorted_list\")\n",
    "                        )\n",
    "                        .withColumn(\"annotated_value_ordered\", expr(\"transform(sorted_list, x -> x.annotated_value)\"))\n",
    "                        .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotated_value_ordered, '\\n')\"))\n",
    "                        .select(groupby_col, attachment_col, \"final_aggregated_pg\")\n",
    "                    )\n",
    "            else:\n",
    "                from pyspark.sql.functions import collect_list, expr, first\n",
    "                # Preserve human_url if it exists\n",
    "                if \"human_url\" in predictions.columns:\n",
    "                    predictions = (\n",
    "                        predictions.groupBy(groupby_col, attachment_col)\n",
    "                        .agg(\n",
    "                            collect_list(\"annotated_value\").alias(\"annotations\"),\n",
    "                            first(\"human_url\").alias(\"human_url\")\n",
    "                        )\n",
    "                        .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotations, '\\n')\"))\n",
    "                        .select(groupby_col, \"human_url\", attachment_col, \"final_aggregated_pg\")\n",
    "                    )\n",
    "                else:\n",
    "                    predictions = (\n",
    "                        predictions.groupBy(groupby_col, attachment_col)\n",
    "                        .agg(\n",
    "                            collect_list(\"annotated_value\").alias(\"annotations\")\n",
    "                        )\n",
    "                        .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotations, '\\n')\"))\n",
    "                        .select(groupby_col, attachment_col, \"final_aggregated_pg\")\n",
    "                    )\n",
    "\n",
    "            # Rename columns for CouchDB save\n",
    "            if groupby_col != \"doc_id\":\n",
    "                predictions = predictions.withColumnRenamed(groupby_col, \"doc_id\")\n",
    "            if attachment_col != \"attachment_name\":\n",
    "                predictions = predictions.withColumnRenamed(attachment_col, \"attachment_name\")\n",
    "\n",
    "        # Use CouchDB connection to save\n",
    "        self.conn.save_distributed(predictions, suffix=suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a0fa58-e693-4978-b33e-67010e066948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main classifier module for SKOL text classification\n",
    "\"\"\"\n",
    "class SkolClassifierV2(SC):\n",
    "    \"\"\"\n",
    "    Text classifier for taxonomic literature.\n",
    "\n",
    "    This version only includes the redis and couchdb I/O methods.\n",
    "    All other methods are in SC.\n",
    "\n",
    "    Supports multiple classification models (Logistic Regression, Random Forest, RNN)\n",
    "    and feature types (word TF-IDF, suffix TF-IDF, combined).\n",
    "    \"\"\"\n",
    "\n",
    "    def _load_raw_from_couchdb(self) -> DataFrame:\n",
    "        \"\"\"Load raw text from CouchDB.\"\"\"\n",
    "        conn = CouchDBConnection(\n",
    "            self.couchdb_url,\n",
    "            self.couchdb_database,\n",
    "            self.couchdb_username,\n",
    "            self.couchdb_password\n",
    "        )\n",
    "\n",
    "        df = conn.load_distributed(self.spark, self.couchdb_pattern)\n",
    "\n",
    "        # Split into lines if line_level mode\n",
    "        if self.line_level:\n",
    "            from pyspark.sql.functions import explode, split, col, trim, row_number, lit\n",
    "            from pyspark.sql.window import Window\n",
    "\n",
    "            df = df.withColumn(\"value\", explode(split(col(\"value\"), \"\\\\n\")))\n",
    "            df = df.filter(trim(col(\"value\")) != \"\")\n",
    "\n",
    "            # Add line numbers\n",
    "            window_spec = Window.partitionBy(\"doc_id\", \"attachment_name\").orderBy(lit(1))\n",
    "            df = df.withColumn(\"line_number\", row_number().over(window_spec) - 1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _load_annotated_from_couchdb(self) -> DataFrame:\n",
    "        \"\"\"Load annotated data from CouchDB.\"\"\"\n",
    "        # Load raw annotations from CouchDB\n",
    "        conn = CouchDBConnection(\n",
    "            self.couchdb_url,\n",
    "            self.couchdb_database,\n",
    "            self.couchdb_username,\n",
    "            self.couchdb_password\n",
    "        )\n",
    "\n",
    "        # Look for .ann files for training\n",
    "        pattern = self.couchdb_pattern\n",
    "        if not pattern.endswith('.ann'):\n",
    "            pattern = pattern.replace('.txt', '.txt.ann')\n",
    "\n",
    "        df = conn.load_distributed(self.spark, pattern)\n",
    "\n",
    "        # Parse annotations\n",
    "        from .preprocessing import AnnotatedTextParser\n",
    "\n",
    "        parser = AnnotatedTextParser(line_level=self.line_level)\n",
    "        return parser.parse(df)\n",
    "\n",
    "    def _save_to_couchdb(self, predictions: DataFrame) -> None:\n",
    "        \"\"\"Save predictions to CouchDB.\"\"\"\n",
    "        from skol_classifier.output_formatters import CouchDBOutputWriter\n",
    "\n",
    "        writer = CouchDBOutputWriter(\n",
    "            couchdb_url=self.couchdb_url,\n",
    "            database=self.couchdb_database,\n",
    "            username=self.couchdb_username,\n",
    "            password=self.couchdb_password\n",
    "        )\n",
    "\n",
    "        writer.save_annotated(\n",
    "            predictions,\n",
    "            suffix=self.output_couchdb_suffix,\n",
    "            coalesce_labels=self.coalesce_labels,\n",
    "            line_level=self.line_level\n",
    "        )\n",
    "\n",
    "    def _save_to_couchdb(self, predictions: DataFrame) -> None:\n",
    "        \"\"\"Save predictions to CouchDB.\"\"\"\n",
    "        from skol_classifier.output_formatters import CouchDBOutputWriter\n",
    "\n",
    "        writer = CouchDBOutputWriter(\n",
    "            couchdb_url=self.couchdb_url,\n",
    "            database=self.couchdb_database,\n",
    "            username=self.couchdb_username,\n",
    "            password=self.couchdb_password\n",
    "        )\n",
    "\n",
    "        writer.save_annotated(\n",
    "            predictions,\n",
    "            suffix=self.output_couchdb_suffix,\n",
    "            coalesce_labels=self.coalesce_labels,\n",
    "            line_level=self.line_level\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab2d58-0ea7-4288-889c-b1bf9c360743",
   "metadata": {},
   "source": [
    "## Build a classifier to identify paragraph types.\n",
    "\n",
    "We save this to redis so that we don't need to train the model every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m4m45o9n97i",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotated files from: /data/piggy/src/github.com/piggyatbaqaqi/skol/data/annotated\n",
      "Found 190 annotated files\n",
      "Training classifier with SkolClassifierV2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "W0000 00:00:1765492076.627406  419100 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765492076.630524  419100 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765492076.632476  419100 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765492076.632479  419100 gpu_device.cc:2456] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0a. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1765492076.640807  419100 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765492076.642765  419100 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765492076.644687  419100 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765492076.644690  419100 gpu_device.cc:2456] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0a. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1765492076.764403  419100 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765492076.765630  419100 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0000 00:00:1765492076.767146  419100 cuda_executor.cc:1801] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "I0000 00:00:1765492076.767161  419100 gpu_device.cc:2040] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21907 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5090 Laptop GPU, pci bus id: 0000:02:00.0, compute capability: 12.0a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPUs: ['/physical_device:GPU:0']\n",
      "Configured 1 GPU(s) with memory growth enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RNN Fit] Training RNN model with generator-based approach...\n",
      "[RNN Fit]   Estimated sequences: ~21850\n",
      "[RNN Fit]   Batch size: 512\n",
      "[RNN Fit]   Steps per epoch: 42\n",
      "[RNN Fit]   Epochs: 6\n",
      "[RNN Fit]   Window size: 20\n",
      "[RNN Fit]   Input size: 1000\n",
      "[RNN Fit] Creating data generator...\n",
      "[RNN Fit] Starting Keras model.fit()...\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765492094.609334  420520 cuda_dnn.cc:462] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.8684 - loss: 0.3826\n",
      "Epoch 2/6\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - accuracy: 0.9304 - loss: 0.1945\n",
      "Epoch 3/6\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9448 - loss: 0.1519\n",
      "Epoch 4/6\n",
      "\u001b[1m17/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9516 - loss: 0.1329"
     ]
    }
   ],
   "source": [
    "# Train classifier on annotated data and save to Redis using SkolClassifierV2\n",
    "model_config = {\n",
    "    \"name\": \"RNN BiLSTM (line-level, advanced config)\",\n",
    "    \"model_type\": \"rnn\",\n",
    "    \"use_suffixes\": True,\n",
    "    \"line_level\": True,\n",
    "    \"input_size\": 1000,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": 3,\n",
    "    \"dropout\": 0.3,\n",
    "    \"window_size\": 20,\n",
    "    # \"batch_size\": 16,  # 442MiB footprint\n",
    "    # \"batch_size\": 128,  # 570MiB footprint\n",
    "    # \"batch_size\": 512,  # 1370MiB footprint\n",
    "    \"batch_size\": 1024,  # footprint\n",
    "    #\"epochs\": 4,\n",
    "    \"epochs\": 6,\n",
    "    \"num_workers\": 2,\n",
    "    \"verbosity\": 2,\n",
    "}\n",
    "# model_config =  {\n",
    "#     \"name\": \"Logistic Regression (line-level, words + suffixes)\",\n",
    "#     \"model_type\": \"logistic\",\n",
    "#     \"use_suffixes\": True,\n",
    "#     \"maxIter\": 10,\n",
    "#     \"regParam\": 0.01,\n",
    "#     \"line_level\": True\n",
    "# }\n",
    "\n",
    "if not redis_client.exists(classifier_model_name):\n",
    "    # Get annotated training files\n",
    "    annotated_path = Path.cwd().parent / \"data\" / \"annotated\"\n",
    "    print(f\"Loading annotated files from: {annotated_path}\")\n",
    "\n",
    "    if annotated_path.exists():\n",
    "        annotated_files = get_file_list(str(annotated_path), pattern=\"**/*.ann\")\n",
    "\n",
    "        if len(annotated_files) > 0:\n",
    "            print(f\"Found {len(annotated_files)} annotated files\")\n",
    "\n",
    "            # Train using SkolClassifierV2 with unified API\n",
    "            print(\"Training classifier with SkolClassifierV2...\")\n",
    "            classifier = SkolClassifierV2(\n",
    "                spark=spark,\n",
    "\n",
    "                # Input\n",
    "                input_source='files',\n",
    "                file_paths=annotated_files,\n",
    "\n",
    "                # Model I/O\n",
    "                auto_load_model=False,  # Fit a new model.\n",
    "                model_storage='redis',\n",
    "                redis_client=redis_client,\n",
    "                redis_key=classifier_model_name,\n",
    "                redis_expire=classifier_model_expire,\n",
    "\n",
    "\n",
    "                # Output options\n",
    "                output_dest='couchdb',\n",
    "                couchdb_url=couchdb_url,\n",
    "                couchdb_database=ingest_db_name,\n",
    "                output_couchdb_suffix='.ann',\n",
    "                \n",
    "                # Model and preprocssing options\n",
    "                **model_config\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            results = classifier.fit()\n",
    "\n",
    "            print(f\"Training complete!\")\n",
    "            print(f\"  Accuracy: {results.get('accuracy', 0):.4f}\")\n",
    "            print(f\"  F1 Score: {results.get('f1_score', 0):.4f}\")\n",
    "\n",
    "            classifier.save_model()\n",
    "            print(f\" Model saved to Redis with key: {classifier_model_name}\")\n",
    "        else:\n",
    "            print(f\"No annotated files found in {annotated_path}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {annotated_path}\")\n",
    "        print(\"Please ensure annotated training data is available.\")\n",
    "else:\n",
    "    print(f\"Skipping generation032.1 of model {classifier_model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fe9eb-1f1b-4aa8-9796-169faeeaf432",
   "metadata": {},
   "source": [
    "## Extract the taxa names and descriptions\n",
    "\n",
    "We use a classifier to extract taxa names and descriptions from articles, issues, and books. The YEDDA annotated texts are written back to CouchDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d42f1-76de-4900-a2d8-7c3001e7ea52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict from CouchDB and save back to CouchDB using SkolClassifierV2\n",
    "print(\"Initializing classifier with unified V2 API...\")\n",
    "\n",
    "classifier = SkolClassifierV2(\n",
    "    spark=spark,\n",
    "    input_source='couchdb',\n",
    "    couchdb_url=couchdb_url,\n",
    "    couchdb_database=ingest_db_name,\n",
    "    couchdb_username=couchdb_username,\n",
    "    couchdb_password=couchdb_password,\n",
    "    couchdb_pattern='*.txt',\n",
    "    output_dest='couchdb',\n",
    "    output_couchdb_suffix='.ann',\n",
    "    model_storage='redis',\n",
    "    redis_client=redis_client,\n",
    "    redis_key=classifier_model_name,\n",
    "    auto_load_model=True,\n",
    "    coalesce_labels=True,\n",
    "    output_format='annotated',\n",
    "    **model_config\n",
    ")\n",
    "\n",
    "print(f\"Model loaded from Redis: {classifier_model_name}\")\n",
    "\n",
    "# Load, predict, and save in a streamlined workflow\n",
    "print(\"\\nLoading and classifying documents from CouchDB...\")\n",
    "raw_df = classifier.load_raw()\n",
    "print(f\"Loaded {raw_df.count()} text documents\")\n",
    "raw_df.show(10)\n",
    "\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = classifier.predict(raw_df)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "predictions.select(\n",
    "    \"doc_id\", \"attachment_name\", \"predicted_label\"\n",
    ").show(5, truncate=50)\n",
    "\n",
    "# Save results back to CouchDB\n",
    "print(\"\\nSaving predictions back to CouchDB...\")\n",
    "classifier.save_annotated(predictions)\n",
    "\n",
    "print(f\"\\n Predictions saved to CouchDB as .ann attachments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1701a1c-864f-494a-a393-7ec77ac14f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"predicted_label\", \"annotated_value\").where('predicted_label = \"Nomenclature\"').show()\n",
    "predictions.groupBy(\"predicted_label\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002ae67-9dc9-4335-adf4-d249905d2ad9",
   "metadata": {},
   "source": [
    "Here we estimate an approximation for the number of Taxon structures we'd like to find. The abbreviation \"nov.\" (\"novum\") indicates a new taxon in the current article. This should be a lower bound, as it is not unusual to redescribe a species, e.g. in a survey article or monograph on a genus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9de42c-8b11-4dfe-aa4b-faa838f3dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"*\").filter(col(\"annotated_value\").contains(\"nov.\")).where(\"predicted_label = 'Nomenclature'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca929b6a-75e5-4dcb-ace2-8d420f42ce41",
   "metadata": {},
   "source": [
    "## Build the Taxon objects and store them in CouchDB\n",
    "We use CouchDB to store a full record for each taxon. We copy all metadata to the taxon records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e285b7-0afa-486e-94f6-41bcb1d7524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDBFile(CDBF):\n",
    "    \"\"\"\n",
    "    File-like object that reads from CouchDB attachment content.\n",
    "\n",
    "    This class extends FileObject to support reading text from CouchDB\n",
    "    attachments while preserving database metadata (doc_id, attachment_name,\n",
    "    and database name).\n",
    "    \"\"\"\n",
    "\n",
    "    _doc_id: str\n",
    "    _attachment_name: str\n",
    "    _db_name: str\n",
    "    _human_url: Optional[str]\n",
    "    _content_lines: List[str]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        content: str,\n",
    "        doc_id: str,\n",
    "        attachment_name: str,\n",
    "        db_name: str,\n",
    "        human_url: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize CouchDBFile from attachment content.\n",
    "\n",
    "        Args:\n",
    "            content: Text content from CouchDB attachment\n",
    "            doc_id: CouchDB document ID\n",
    "            attachment_name: Name of the attachment (e.g., \"article.txt.ann\")\n",
    "            db_name: Database name where document is stored (ingest_db_name)\n",
    "            url: Optional URL from the CouchDB row\n",
    "        \"\"\"\n",
    "        self._doc_id = doc_id\n",
    "        self._attachment_name = attachment_name\n",
    "        self._db_name = db_name\n",
    "        self._human_url = human_url\n",
    "        self._line_number = 0\n",
    "        self._page_number = 1\n",
    "        self._empirical_page_number = None\n",
    "\n",
    "        # Split content into lines\n",
    "        self._content_lines = content.split('\\n')\n",
    "\n",
    "    def _get_content_iterator(self) -> Iterator[str]:\n",
    "        \"\"\"Get iterator over content lines.\"\"\"\n",
    "        return iter(self._content_lines)\n",
    "\n",
    "    @property\n",
    "    def filename(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a composite identifier for CouchDB documents.\n",
    "\n",
    "        Format: db_name/doc_id/attachment_name\n",
    "        This allows tracking the source of each line.\n",
    "        \"\"\"\n",
    "        return f\"{self._db_name}/{self._doc_id}/{self._attachment_name}\"\n",
    "\n",
    "    @property\n",
    "    def doc_id(self) -> str:\n",
    "        \"\"\"CouchDB document ID.\"\"\"\n",
    "        return self._doc_id\n",
    "\n",
    "    @property\n",
    "    def attachment_name(self) -> str:\n",
    "        \"\"\"Attachment filename.\"\"\"\n",
    "        return self._attachment_name\n",
    "\n",
    "    @property\n",
    "    def db_name(self) -> str:\n",
    "        \"\"\"Database name (ingest_db_name).\"\"\"\n",
    "        return self._db_name\n",
    "\n",
    "    @property\n",
    "    def human_url(self) -> Optional[str]:\n",
    "        \"\"\"URL from the CouchDB row.\"\"\"\n",
    "        return self._human_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22648996-4d9f-47f1-ab4f-6b5fd54f97e9",
   "metadata": {},
   "source": [
    "## Build Taxon objects\n",
    "\n",
    "Here we extract the Taxon objects from the annotated attachments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08069896-722b-45c7-8b95-78cbb89e8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_couchdb_url = couchdb_url\n",
    "ingest_username = couchdb_username\n",
    "ingest_password = couchdb_password\n",
    "taxon_couchdb_url = couchdb_url\n",
    "taxon_username = couchdb_username\n",
    "taxon_password = couchdb_password\n",
    "pattern = '*.txt.ann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed264e3-3e3c-4c32-bb99-2205edaa4bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create TaxonExtractor instance with database configuration\n",
    "extractor = TaxonExtractor(\n",
    "    spark=spark,\n",
    "    ingest_couchdb_url=ingest_couchdb_url,\n",
    "    ingest_db_name=ingest_db_name,\n",
    "    taxon_db_name=taxon_db_name,\n",
    "    ingest_username=ingest_username,\n",
    "    ingest_password=ingest_password,\n",
    "    taxon_username=taxon_username,\n",
    "    taxon_password=taxon_password\n",
    ")\n",
    "\n",
    "print(\"TaxonExtractor initialized\")\n",
    "print(f\"  Ingest DB: {ingest_db_name}\")\n",
    "print(f\"  Taxon DB:  {taxon_db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5acdf9-f40f-4ef7-b18b-b5f8236b248b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Load annotated documents\n",
    "print(\"\\nStep 1: Loading annotated documents from CouchDB...\")\n",
    "annotated_df = extractor.load_annotated_documents(pattern='*.txt.ann')\n",
    "print(f\"Loaded {annotated_df.count()} annotated documents\")\n",
    "annotated_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952917da-82e8-4ed0-8876-e1705f4631ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Extract taxa to DataFrame\n",
    "print(\"\\nStep 2: Extracting taxa from annotated documents...\")\n",
    "taxa_df = extractor.extract_taxa(annotated_df)\n",
    "print(f\"Extracted {taxa_df.count()} taxa\")\n",
    "taxa_df.printSchema()\n",
    "taxa_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f01347-0543-4499-a2e7-065a54e61938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Inspect actual Taxon objects from the RDD (optional debugging)\n",
    "print(\"\\n=== Sample Taxon Objects ===\")\n",
    "taxa_rdd = annotated_df.rdd.mapPartitions(\n",
    "    lambda partition: extract_taxa_from_partition(iter(partition), ingest_db_name)  # type: ignore[reportUnknownArgumentType]\n",
    ")\n",
    "for i, taxon in enumerate(taxa_rdd.take(3)):\n",
    "    print(f\"\\nTaxon {i+1}:\")\n",
    "    print(f\"  Type: {type(taxon)}\")\n",
    "    print(f\"  Has nomenclature: {taxon.has_nomenclature()}\")\n",
    "    taxon_row = taxon.as_row()\n",
    "    print(f\"  Taxon name: {taxon_row['taxon'][:80]}...\")\n",
    "    print(f\"  Source: {taxon_row['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7af36b-f190-4dd7-8f29-a9ed11f64c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Save taxa to CouchDB\n",
    "print(\"\\nStep 4: Saving taxa to CouchDB...\")\n",
    "results_df = extractor.save_taxa(taxa_df)\n",
    "\n",
    "# Show detailed results\n",
    "results_df.groupBy(\"success\").count().show(truncate=False)\n",
    "\n",
    "# If there are failures, show error messages\n",
    "print(\"\\nError messages:\")\n",
    "results_df.filter(\"success = false\").select(\"error_message\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ewsk11g9lpl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run the complete pipeline in one step\n",
    "# Uncomment to use the simplified one-step approach:\n",
    "\n",
    "# print(\"\\nRunning complete pipeline...\")\n",
    "# results = extractor.run_pipeline(pattern='*.txt.ann')\n",
    "#\n",
    "# successful = results.filter(\"success = true\").count()\n",
    "# failed = results.filter(\"success = false\").count()\n",
    "#\n",
    "# print(f\"\\nPipeline Results:\")\n",
    "# print(f\"  Successful: {successful}\")\n",
    "# print(f\"  Failed:     {failed}\")\n",
    "#\n",
    "# results.groupBy(\"success\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769c280-8139-4228-b5a7-d21dab100910",
   "metadata": {},
   "source": [
    "### Observations on the classification models\n",
    "\n",
    "The line-by-line classification model is classifying many Description lines as Misc-exposition. It works reasonably well for Nomenclature.\n",
    "\n",
    "The problem with the paragraph classification model is that the heuristic paragrph parser does not generalize well to the more modern journals.\n",
    "\n",
    "One possible approach to investigate is adding heuristics to the label-merging code to convert some Misc-exposition lines to Description if they are surrounded by Description paragraphs.\n",
    "\n",
    "A more sophisticated approach is to use a completely new model that has some memory, such as an RNN, or a two pass model that adds the label of the previous line(s) as added features for each line.\n",
    "\n",
    "It may become necessary to hand annotate some of the more modern journals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ee719-eb73-425b-ad44-6f30158c8d5e",
   "metadata": {},
   "source": [
    "## Dr. Drafts document embedding\n",
    "\n",
    "Dr. Drafts is the framework we use to embed all the descriptions into a searchable space. SBERT is a model that can embed sentences into a semantic space such that sentences with similar meaning are near each other. The data structure that we build here is central to the eventual function of the SKOL web site.\n",
    "\n",
    "Dr. Drafts loads taxon documents from the CouchDB, and builds an embedding which it saves to redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c550d-15f4-426c-b11e-d7ac6691e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dr_drafts_mycosearch.data import SKOL_TAXA as STX\n",
    "from dr_drafts_mycosearch.compute_embeddings import EmbeddingsComputer as EC\n",
    "\n",
    "class SKOL_TAXA(STX):\n",
    "    \"\"\"Data interface for Synopotic Key of Life Taxa in CouchDB.\"\"\"\n",
    "        def load_data(self):\n",
    "        \"\"\"Load taxon data from CouchDB into a pandas DataFrame.\"\"\"\n",
    "        # Connect to CouchDB\n",
    "        server = couchdb.Server(self.couchdb_url)\n",
    "        if self.username and self.password:\n",
    "            server.resource.credentials = (self.username, self.password)\n",
    "\n",
    "        # Access the database\n",
    "        if self.db_name not in server:\n",
    "            raise ValueError(f\"Database '{self.db_name}' not found in CouchDB server\")\n",
    "\n",
    "        db = server[self.db_name]\n",
    "\n",
    "        # Fetch all documents from the database\n",
    "        records = []\n",
    "        for doc_id in db:\n",
    "            # Skip design documents\n",
    "            if doc_id.startswith('_design/'):\n",
    "                continue\n",
    "\n",
    "            doc = db[doc_id]\n",
    "            print(f\"DEBUG: doc: {doc}\")  # Debugging line to inspect document structure\n",
    "            records.append(doc)\n",
    "\n",
    "        if not records:\n",
    "            # Create empty DataFrame if no records found\n",
    "            self.df = pd.DataFrame()\n",
    "            print(f\"Warning: No taxon records found in database '{self.db_name}'\")\n",
    "            return\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        self.df = pd.DataFrame(records)\n",
    "        assert self.df.iloc[0]['source']['human_url'].startswith('http'), \"Expected 'source.url' to start with 'http'\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a79b3c-4ac4-4208-9346-0a5bc90e0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsComputer(EC):\n",
    "    \"\"\"Class for computing and storing embeddings from narrative data.\"\"\"\n",
    "    \n",
    "    def write_embeddings_to_redis(self):\n",
    "        \"\"\"Write embeddings to Redis using instance configuration.\"\"\"\n",
    "        if self.redis_username and self.redis_password:\n",
    "            r = redis.from_url(self.redis_url, username=self.redis_username, password=self.redis_password, db=self.redis_db)\n",
    "        else:\n",
    "            r = redis.from_url(self.redis_url, db=self.redis_db)\n",
    "\n",
    "        pickled_data = pickle.dumps(self.result)\n",
    "        r.set(self.embedding_name, pickled_data)\n",
    "        if self.redist_expire is not None:\n",
    "            r.expire(self.embedding_name, self.redist_expire)\n",
    "        print(f'Embeddings written to Redis (db={self.redis_db}) with key: {self.embedding_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345caed-a301-49ca-a182-ce0ace7e839b",
   "metadata": {},
   "source": [
    "## Compute Embeddings\n",
    "\n",
    "We use SBERT to embed the taxa into a search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c806139-15d0-4904-9ec9-8f7e6ada5b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skol_taxa = SKOL_TAXA(\n",
    "    couchdb_url=\"http://localhost:5984\",\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password,\n",
    "    db_name=taxon_db_name\n",
    ")\n",
    "descriptions = skol_taxa.get_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e661cb-fc85-4c1a-9617-b61ca7d1199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not redis_client.exists(embedding_name):\n",
    "\n",
    "    embedder = EmbeddingsComputer(\n",
    "        idir='/dev/null',\n",
    "        redis_url='redis://localhost:6379',\n",
    "        redis_expire=embedding_expire,\n",
    "        embedding_name=embedding_name,\n",
    "    )\n",
    "\n",
    "    embedding_result = embedder.run(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1498755-a114-4c46-985d-47f47e7abee8",
   "metadata": {},
   "source": [
    "## Compute JSON versions of all descriptions\n",
    "\n",
    "There is an anticipated need for the details of each description to be available as a nested JSON structure, which can be used to build menus with features, subfeatures, and values.\n",
    "\n",
    "The TaxaJSONTranslator reads taxa from the CouchDB and writes annotated taxa back out to CouchDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83954366-44a3-430f-92b9-526ac08637d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxaJSONTranslator(TJT):\n",
    "    \"\"\"\n",
    "    Translates taxa descriptions to structured JSON using a fine-tuned Mistral model.\n",
    "\n",
    "    This class is optimized for processing PySpark DataFrames created by\n",
    "    TaxonExtractor.load_taxa(), adding a new column with JSON-formatted features.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ef2d8-ce08-400a-91d6-f089d0f2e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TaxaJSONTranslator(\n",
    "    spark=spark,\n",
    "    base_model_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    max_length=2048,\n",
    "    max_new_tokens=1024,\n",
    "    device=\"cuda\",\n",
    "    load_in_4bit=True,\n",
    "    use_auth_token=True,\n",
    "    couchdb_url=couchdb_url,\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b0a40-6839-47c9-ac9e-409b2803f93e",
   "metadata": {},
   "source": [
    "### Run the mistral model to generate JSON from each Taxon description."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d39221ce-9e84-41fc-b741-0917afd8ecf6",
   "metadata": {},
   "source": [
    "descriptions_df = translator.load_taxa(db_name=taxon_db_name).limit(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "334f6d67-fef5-4d4c-b98c-71014a74e4ea",
   "metadata": {},
   "source": [
    "descriptions_df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14202225-e1be-4e68-8b7a-46f3002695b0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "json_annotated_df = translator.translate_descriptions_batch(\n",
    "    taxa_df=descriptions_df,\n",
    "    batch_size=10,\n",
    "    description_col=\"description\",\n",
    "    output_col=\"json_annotated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e317f-b02a-417e-a03a-cfcb27ec2e87",
   "metadata": {},
   "source": [
    "### Add the generated fields as a field on the objects generated by save_taxa."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c528c46-3018-408e-b2d2-6407456b3862",
   "metadata": {},
   "source": [
    "results_df = translator.save_taxa(json_annotated_df, db_name=json_taxon_db_name)\n",
    "\n",
    "results_df.groupBy(\"success\").count().show(truncate=False)\n",
    "\n",
    "print(\"\\nError messages:\")\n",
    "results_df.filter(\"success = false\").select(\"error_message\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526beef-5503-4fac-a9a5-427367d625a5",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "We use Agglomerative Clustering to group the taxa into \"clades\" based in cosine similarity of their SBERT embeddings. We then load them into neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e0d92-b22b-40ec-ad75-1aab43355c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxon_clusterer import TaxonClusterer as TC\n",
    "\n",
    "class TaxonClusterer(TC):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f97734-e9d9-4f2e-96f0-3f298d4a93a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterer = TaxonClusterer(\n",
    "    redis_host=\"localhost\",\n",
    "    redis_port=6379,\n",
    "    redis_db=0,\n",
    "    neo4j_uri=neo4j_uri,\n",
    ")\n",
    "\n",
    "# Load embeddings from Redis\n",
    "(embeddings, taxon_names, metadata) = clusterer.load_embeddings(embedding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9004117-e280-448e-ab39-b7f4aa449c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26683a0-6879-44be-869a-a87748aac888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "linkage_matrix = clusterer.cluster(method=\"average\", metric=\"cosine\")\n",
    "\n",
    "# Store in Neo4j with root named \"Fungi\"\n",
    "clusterer.store_in_neo4j(root_name=\"Fungi\", clear_existing=True)\n",
    "\n",
    "print(\" Clustering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f1457-b1bf-4e02-9b0c-ce194f33969d",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* doi Foundation, \"DOI Citation Formatter HTTP API\", https://citation.doi.org/api-docs.html, accessed 2025-11-12.\n",
    "* Yang, Jie and Zhang, Yue and Li, Linwei and Li, Xingxuan, 2018, \"YEDDA: A Lightweight Collaborative Text Span Annotation Tool\", Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, http://aclweb.org/anthology/P18-4006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82412f-0fd7-4526-a949-3732c2b26624",
   "metadata": {},
   "source": [
    "## Appendix: On the use of an AI Coder\n",
    "\n",
    "Portions of this work were completed with the aid of Claude Code Pro. I wish to give a clarifying example of how I've used this very powerful tool, and reveal why I am comfortable with claiming authorship of the resulting code.\n",
    "\n",
    "For this project I needed results from an earlier class project in which a trio of students built and evaluated models for classifying paragraphs. The earlier work was built as a iPython Notebook, with many examples and inline code. Just copying the earlier notebook would have introduced many irrelevant details and would not further the overall project.\n",
    "\n",
    "I asked Claude Code to translate the notebook into a module that I could import. It did a pretty good job. Without being told, it made a submodule, extract the illustrative code as examples, wrote reasonable documentation and created packaging for the module.\n",
    "\n",
    "The skill level of the coding was roughly that of a highly disciplined average junior programmer. The architecture was simplistic and violated several design constraints such as DRY. I requested specific refactorings, such as asking for a group of functions to be converted into an object that shared duplicated parameters.\n",
    "\n",
    "The initial code used REST interfaces directly, and read all the data into a single machine, not using pyspark correctly. Through a series of refactorings, I asked that the code use appropriate libraries I named, and create correct udf functions to execute transformations in parallel.\n",
    "\n",
    "I walked the AI through creating an object that I could use to illustrate my use of redis and couchdb interfaces, while leaving the irrelevant details in a separate library.\n",
    "\n",
    "In short, I still have to understand good design principles. I have to be able to recognize where appropriate libraries were applicable. I still have to understand the frameworks I am working with.\n",
    "\n",
    "I now have a strong understanding of the difference between \"vibe coding\" and AI-assisted software engineering. In my first 4 hours with Claude Code, I was able to produce roughly 4 days' worth of professional-grade working code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
