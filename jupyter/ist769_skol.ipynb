{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a626f7e-4127-4227-9906-dc08dd9135ce",
   "metadata": {},
   "source": [
    "# SKOL IV: All the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ae6b20-7abd-4ef6-b2cf-8d221a40d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/29 00:26:52 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 172.16.227.68 instead (on interface wlp130s0f0)\n",
      "25/11/29 00:26:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-99fc310f-67dd-4f74-bbea-6ac79fe66976;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 235ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-99fc310f-67dd-4f74-bbea-6ac79fe66976\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/9ms)\n",
      "25/11/29 00:26:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Spark context Web UI available at http://172.16.227.68:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1764376015542).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 21.0.9)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> :quit\n"
     ]
    }
   ],
   "source": [
    "bahir_package = 'org.apache.bahir:spark-sql-cloudant_2.12:2.4.0'\n",
    "!spark-shell --packages $bahir_package < /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9615b87-4962-47ca-b10f-98558713196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import pickle\n",
    "import requests\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# Be sure to get version 2: https://simple-repository.app.cern.ch/project/bibtexparser/2.0.0b8/description\n",
    "import bibtexparser\n",
    "import couchdb\n",
    "import feedparser\n",
    "import fitz # PyMuPDF\n",
    "\n",
    "import pandas as pd  # TODO(piggy): Remove this dependency in favor of pure pyspark DataFrames.\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, CountVectorizer, IDF, StringIndexer, VectorAssembler, IndexToString\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name, collect_list, regexp_extract, col, udf,\n",
    "    explode, trim, row_number, min, expr, concat, lit\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, BooleanType, IntegerType, MapType, NullType,\n",
    "    StringType, StructType, StructField\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import redis\n",
    "import torch\n",
    "from uuid import uuid4\n",
    "\n",
    "# Local modules\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "parent_path = Path(parent_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from couchdb_file import CouchDBFile as CDBF\n",
    "from fileobj import FileObject\n",
    "from finder import parse_annotated, remove_interstitials\n",
    "import line\n",
    "from line import Line\n",
    "\n",
    "# Import SKOL classifiers\n",
    "from skol_classifier.classifier_v2 import SkolClassifierV2 as SC\n",
    "from skol_classifier.preprocessing import SuffixTransformer, ParagraphExtractor\n",
    "from skol_classifier.utils import calculate_stats, get_file_list\n",
    "\n",
    "from taxon import group_paragraphs, Taxon\n",
    "\n",
    "from taxa_json_translator import TaxaJSONTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9570ec3-8bb3-41af-99df-d96907293a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: cloudant.protocol\n",
      "Warning: Ignoring non-Spark config property: cloudant.password\n",
      "Warning: Ignoring non-Spark config property: cloudant.host\n",
      "Warning: Ignoring non-Spark config property: cloudant.username\n",
      "25/11/29 00:27:00 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 172.16.227.68 instead (on interface wlp130s0f0)\n",
      "25/11/29 00:27:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b1c8cff0-ea2a-4e86-a043-ee28f0732728;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 209ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b1c8cff0-ea2a-4e86-a043-ee28f0732728\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/10ms)\n",
      "25/11/29 00:27:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "couchdb_host = \"127.0.0.1:5984\" # e.g., \"ACCOUNT.cloudant.com\" or \"localhost\"\n",
    "couchdb_username = \"admin\"\n",
    "couchdb_password = \"SU2orange!\"\n",
    "ingest_db_name = \"skol_dev\"\n",
    "taxon_db_name = \"skol_taxa_dev\"\n",
    "embedding_name = 'skol:embedding:v1.1'\n",
    "embedding_expire = 60 * 60 * 24  # Expire after 24 hours\n",
    "\n",
    "couchdb_url = f'http://{couchdb_host}'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CouchDB Spark SQL Example in Python using dataframes\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"cloudant.protocol\", \"http\") \\\n",
    "    .config(\"cloudant.host\", couchdb_host) \\\n",
    "    .config(\"cloudant.username\", couchdb_username) \\\n",
    "    .config(\"cloudant.password\", couchdb_password) \\\n",
    "    .config(\"spark.jars.packages\", bahir_package) \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .config(\"spark.submit.pyFiles\",\n",
    "            f'{parent_path / \"line.py\"},{parent_path / \"fileobj.py\"},'\n",
    "            f'{parent_path / \"couchdb_file.py\"},{parent_path / \"finder.py\"},'\n",
    "            f'{parent_path / \"taxon.py\"},{parent_path / \"paragraph.py\"},'\n",
    "            f'{parent_path / \"label.py\"},{parent_path / \"file.py\"},'\n",
    "            f'{parent_path / \"extract_taxa_to_couchdb.py\"}'\n",
    "           ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!\n",
    "\n",
    "couch = couchdb.Server(couchdb_url)\n",
    "couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "\n",
    "if ingest_db_name not in couch:\n",
    "    db = couch.create(ingest_db_name)\n",
    "else:\n",
    "    db = couch[ingest_db_name]\n",
    "\n",
    "user_agent = \"synoptickeyof.life\"\n",
    "\n",
    "ingenta_rp = RobotFileParser()\n",
    "ingenta_rp.set_url(\"https://www.ingentaconnect.com/robots.txt\")\n",
    "ingenta_rp.read() # Reads and parses the robots.txt file from the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e7f93-bb66-4922-b3f9-c9429bc304ac",
   "metadata": {},
   "source": [
    "## The Data Sources\n",
    "\n",
    "The goal is to collect all the open access taxonomic literature in Mycology. Most of the sources below mainly cover macro-fungi and slime molds.\n",
    "\n",
    "### Ingested Data Sources\n",
    "\n",
    "* [Mycotaxon at Ingenta Connect](https://www.ingentaconnect.com/content/mtax/mt)\n",
    "* [Studies in Mycology at Ingenta Connect](https://www.studiesinmycology.org/)\n",
    "\n",
    "### Source of many older public domain and open access works\n",
    "\n",
    "Mycoweb includes scans of many older works in mycology. I have local copies but need to ingest them.\n",
    "\n",
    "* [Mycoweb](https://mykoweb.com/)\n",
    "\n",
    "### Journals in hand\n",
    "\n",
    "These are journals we've collected over the years. The initial annotated issues are from early years of Mycotaxon. We still need to ingest all of these.\n",
    "\n",
    "* Mycologia (back issues)\n",
    "* [Mycologia at Taylor and Francis](https://www.tandfonline.com/journals/umyc20)\n",
    "  Mycologia is the main journal of the Mycological Society of America. It is a mix of open access and traditional access articles. The connector for this journal will need to identify the open access articles.\n",
    "* Persoonia (all issues)\n",
    "  Persoonia is no longer published.\n",
    "* Mycotaxon (back issues)\n",
    "  Mycotaxon is no longer published.\n",
    "\n",
    "### Journals that need connectors\n",
    "\n",
    "These are journals we're aware that include open access articles.\n",
    "\n",
    "* [Amanitaceae.org](http://www.tullabs.com/amanita/?home)\n",
    "* [Mycosphere](https://mycosphere.org/)\n",
    "* [Mycoscience](https://mycoscience.org/)\n",
    "* [Journal of Fungi](https://www.mdpi.com/journal/jof)\n",
    "* [Mycology](https://www.tandfonline.com/journals/tmyc20)\n",
    "* [Open Access Journal of Mycology & Mycological Sciences](https://www.medwinpublishers.com/OAJMMS/)\n",
    "* [Mycokeys](https://mycokeys.pensoft.net/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a4213-adb5-49e5-b973-570a75cc2cce",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Each journal or other data source gets an ingester that puts PDFs into our document store along with any metadata we can collect. The metadata is sufficient to create citations for each issue, book, or article. If bibtex citations are available we prefer to store these verbatim.\n",
    "\n",
    "### Ingenta RSS ingestion\n",
    "\n",
    "Ingenta Connect is an electronic publisher that holds two Mycology journals. New articles are available via RSS (Really Simple Syndication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d303cc7-f01c-4e15-87ff-6c259d010591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_bibtex(\n",
    "        db: couchdb.Database,\n",
    "        content: bytes,\n",
    "        bibtex_link: str,\n",
    "        meta: Dict[str, Any],\n",
    "        rp\n",
    "        ) -> None:\n",
    "    \"\"\"Load documents referenced in an Ingenta BibTeX database.\"\"\"\n",
    "    bib_database = bibtexparser.parse_string(content)\n",
    "\n",
    "    bibtex_data = {\n",
    "        'link': bibtex_link,\n",
    "        'bibtex': bibtexparser.write_string(bib_database),\n",
    "    }\n",
    "\n",
    "    for bib_entry in bib_database.entries:\n",
    "        doc = {\n",
    "            '_id': uuid4().hex,\n",
    "            'meta': meta,\n",
    "            'pdf_url': f\"{bib_entry['url']}?crawler=true\",\n",
    "        }\n",
    "\n",
    "        # Do not fetch if we already have an entry.\n",
    "        selector = {'selector': {'pdf_url': doc['pdf_url']}}\n",
    "        found = False\n",
    "        for e in db.find(selector):\n",
    "            found = True\n",
    "        if found:\n",
    "            print(f\"Skipping {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        if not rp.can_fetch(user_agent, doc['pdf_url']):\n",
    "            # TODO(piggy): We should probably record blocked URLs.\n",
    "            print(f\"Robot permission denied {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Adding {doc['pdf_url']}\")\n",
    "        for k in bib_entry.fields_dict.keys():\n",
    "            doc[k] = bib_entry[k]\n",
    "\n",
    "        doc_id, doc_rev = db.save(doc)\n",
    "        with requests.get(doc['pdf_url'], stream=False) as pdf_f:\n",
    "            pdf_f.raise_for_status()\n",
    "            pdf_doc = pdf_f.content\n",
    "\n",
    "        attachment_filename = 'article.pdf'\n",
    "        attachment_content_type = 'application/pdf'\n",
    "        attachment_file = BytesIO(pdf_doc)\n",
    "\n",
    "        db.put_attachment(doc, attachment_file, attachment_filename, attachment_content_type)\n",
    "\n",
    "        print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b323f98-5bd8-4106-b79c-2ff9ac1ac74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_ingenta(\n",
    "        db: couchdb.Database,\n",
    "        rss_url: str,\n",
    "        rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest documents from an Ingenta RSS feed.\"\"\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    feed_meta = {\n",
    "        'url': rss_url,\n",
    "        'title': feed.feed.title,\n",
    "        'link': feed.feed.link,\n",
    "        'description': feed.feed.description,\n",
    "    }\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        entry_meta = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "        }\n",
    "        if hasattr(entry, 'summary'):\n",
    "            entry_meta['summary'] = entry.summary\n",
    "        if hasattr(entry, 'description'):\n",
    "            entry_meta['description'] = entry.description\n",
    "\n",
    "        bibtex_link = f'{entry.link}?format=bib'\n",
    "        print(f\"bibtex_link: {bibtex_link}\")\n",
    "\n",
    "        if not rp.can_fetch(user_agent, bibtex_link):\n",
    "            print(f\"Robot permission denied {bibtex_link}\")\n",
    "            continue\n",
    "\n",
    "        with requests.get(bibtex_link, stream=False) as bibtex_f:\n",
    "            bibtex_f.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            ingest_from_bibtex(\n",
    "                db=db,\n",
    "                content=bibtex_f.content\\\n",
    "                    .replace(b\"\\\"\\nparent\", b\"\\\",\\nparent\")\\\n",
    "                    .replace(b\"\\n\", b\"\"),\n",
    "                bibtex_link=bibtex_link,\n",
    "                meta={\n",
    "                    'feed': feed_meta,\n",
    "                    'entry': entry_meta,\n",
    "                },\n",
    "                rp=rp\n",
    "            )\n",
    "        print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234f9cc5-9b7e-4559-ad10-f24ad269e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_local_bibtex(\n",
    "    db: couchdb.Database,\n",
    "    root: Path,\n",
    "    rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest from a local directory with Ingenta bibtext files in it.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if not filename.endswith('format=bib'):\n",
    "                continue\n",
    "            full_filepath = os.path.join(dirpath, filename)\n",
    "            bibtex_link = f\"https://www.ingentaconnect.com/{full_filepath[len(str(root)):]}\"\n",
    "            with open(full_filepath) as f:\n",
    "                content = f.read()\\\n",
    "                    .replace(\"\\\"\\nparent\", \"\\\",\\nparent\")\\\n",
    "                    .replace(\"\\n\", \"\")\n",
    "                ingest_from_bibtex(db, content, bibtex_link, meta={}, rp=rp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58829ec0-93f7-44dd-87bc-511ab2d238d0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mycotaxon\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/mtax/mt?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40602cc3-e231-497d-adc7-d6d4471514cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Studies in Mycology\n",
    "\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/wfbi/sim?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "651f82ea-b3b8-4f0d-adf0-f5550d7ed6d5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ingest_from_local_bibtex(\n",
    "    db=db,\n",
    "    root=Path(\"/data/skol/www/www.ingentaconnect.com\"),\n",
    "    rp=ingenta_rp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf05ad-d2bf-4716-b253-07455cfefcd0",
   "metadata": {},
   "source": [
    "Download the RSS\n",
    "\n",
    "Read bibtex files and create records for each article.\n",
    "\n",
    "Download the PDFs at the URLs in the bibtex entries.\n",
    "\n",
    "Create a JSON record with the PDF as an attachment.\n",
    "\n",
    "### Text extraction\n",
    "\n",
    "We extract the text, optionally with OCR. Add as an additional attachment on the source record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5e9725-0c3a-4c41-8ebf-2929c4dc4334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\n",
    "    format=\"org.apache.bahir.cloudant\",\n",
    "    database=ingest_db_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e75c64e-95cb-478a-9a85-46d1297e4225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _id: string, _rev: string, abstract: string, author: string, doi: string, eissn: string, issn: string, itemtype: string, journal: string, number: string, pages: string, parent_itemid: string, pdf_url: string, publication date: string, publishercode: string, title: string, url: string, volume: string, year: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9befa546-a895-4a13-8744-d512587d9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Type: text/html; charset=UTF-8\n",
    "\n",
    "def pdf_to_text(pdf_contents: bytes) -> bytes:\n",
    "    doc = fitz.open(stream=BytesIO(pdf_contents), filetype=\"pdf\")\n",
    "\n",
    "    full_text = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        # Possibly perform OCR on the page\n",
    "        text = page.get_text(\"text\", flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_DEHYPHENATE)\n",
    "        # full_text += f\"\\n--- PDF Page {page_num+1} ---\\n\"\n",
    "        full_text += text\n",
    "\n",
    "    return full_text.encode(\"utf-8\")\n",
    "\n",
    "def add_text_to_partition(iterator) -> None:\n",
    "    couch = couchdb.Server(couchdb_url)\n",
    "    couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "    local_db = couch[ingest_db_name]\n",
    "    for row in iterator:\n",
    "        if not row:\n",
    "            continue\n",
    "        if not row._attachments:\n",
    "            continue\n",
    "        row_dict = row.asDict()\n",
    "        attachment_dict = row._attachments.asDict()\n",
    "        for pdf_filename in attachment_dict:\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            if pdf_path.suffix != '.pdf':\n",
    "                continue\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            txt_path_str = pdf_path.stem + '.txt'\n",
    "            # if txt_path_str in attachment_dict:\n",
    "            #     # TODO(piggy): Recalculate text if text is terrible. Too much noise vocabulary?\n",
    "            #     print(f\"Already have text for {row.pdf_url}\")\n",
    "            #     continue\n",
    "            print(f\"{row._id}, {row.pdf_url}\")\n",
    "            pdf_file = local_db.get_attachment(row._id, str(pdf_path)).read()\n",
    "            txt_file = pdf_to_text(pdf_file)\n",
    "            attachment_content_type = 'text/simple; charset=UTF-8'\n",
    "            attachment_file = BytesIO(txt_file)\n",
    "            local_db.put_attachment(row_dict, attachment_file, txt_path_str, attachment_content_type)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a51014-ee58-449b-8ce4-8d0a986e68c1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df.select(\"*\").foreachPartition(add_text_to_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11805c62-ae42-4959-9673-1e09261a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to skol_classifier.CouchDBConnection.\n",
    "from skol_classifier import CouchDBConnection as CDBC\n",
    "\n",
    "class CouchDBConnection(CDBC):\n",
    "    \"\"\"\n",
    "    Manages CouchDB connection and provides I/O operations.\n",
    "\n",
    "    This class encapsulates connection parameters and provides an idempotent\n",
    "    connection method that can be safely called multiple times.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08a0fa58-e693-4978-b33e-67010e066948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main classifier module for SKOL text classification\n",
    "\"\"\"\n",
    "class SkolClassifierV2(SC):\n",
    "    \"\"\"\n",
    "    Text classifier for taxonomic literature.\n",
    "\n",
    "    This version only includes the redis and couchdb I/O methods.\n",
    "    All other methods are in SC.\n",
    "\n",
    "    Supports multiple classification models (Logistic Regression, Random Forest)\n",
    "    and feature types (word TF-IDF, suffix TF-IDF, combined).\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab2d58-0ea7-4288-889c-b1bf9c360743",
   "metadata": {},
   "source": [
    "## Build a classifier to identify paragraph types.\n",
    "\n",
    "We save this to redis so that we don't need to train the model every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "m4m45o9n97i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping generation of model skol:classifier:model:v1.4.\n"
     ]
    }
   ],
   "source": [
    "# Train classifier on annotated data and save to Redis using SkolClassifierV2\n",
    "# Connect to Redis\n",
    "redis_client = redis.Redis(\n",
    "    host='localhost',\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=False\n",
    ")\n",
    "classifier_model_name = \"skol:classifier:model:v1.4\"\n",
    "classifier_model_expire = 60 * 60 * 24  # Expire after 1 day.\n",
    "\n",
    "if not redis_client.exists(classifier_model_name):\n",
    "    # Get annotated training files\n",
    "    annotated_path = Path.cwd().parent / \"data\" / \"annotated\"\n",
    "    print(f\"Loading annotated files from: {annotated_path}\")\n",
    "\n",
    "    if annotated_path.exists():\n",
    "        annotated_files = get_file_list(str(annotated_path), pattern=\"**/*.ann\")\n",
    "\n",
    "        if len(annotated_files) > 0:\n",
    "            print(f\"Found {len(annotated_files)} annotated files\")\n",
    "\n",
    "            # Train using SkolClassifierV2 with unified API\n",
    "            print(\"Training classifier with SkolClassifierV2...\")\n",
    "            classifier = SkolClassifierV2(\n",
    "                spark=spark,\n",
    "\n",
    "                # Input\n",
    "                input_source='files',\n",
    "                file_paths=annotated_files,\n",
    "\n",
    "                # Model I/O\n",
    "                auto_load_model=False,  # Fit a new model.\n",
    "                model_storage='redis',\n",
    "                redis_client=redis_client,\n",
    "                redis_key=classifier_model_name,\n",
    "                redis_expire=classifier_model_expire,\n",
    "\n",
    "                # Model and preprocssing options\n",
    "                line_level=True,\n",
    "                use_suffixes=False,\n",
    "                model_type='logistic',\n",
    "\n",
    "                # Output options\n",
    "                output_dest='couchdb',\n",
    "                couchdb_url=couchdb_url,\n",
    "                couchdb_database=ingest_db_name,\n",
    "                output_couchdb_suffix='.ann'\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            results = classifier.fit()\n",
    "\n",
    "            print(f\"Training complete!\")\n",
    "            print(f\"  Accuracy: {results.get('accuracy', 0):.4f}\")\n",
    "            print(f\"  F1 Score: {results.get('f1_score', 0):.4f}\")\n",
    "            print(f\"✓ Model automatically saved to Redis with key: {classifier_model_name}\")\n",
    "        else:\n",
    "            print(f\"No annotated files found in {annotated_path}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {annotated_path}\")\n",
    "        print(\"Please ensure annotated training data is available.\")\n",
    "else:\n",
    "    print(f\"Skipping generation of model {classifier_model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fe9eb-1f1b-4aa8-9796-169faeeaf432",
   "metadata": {},
   "source": [
    "## Extract the taxa names and descriptions\n",
    "\n",
    "We use a classifier to extract taxa names and descriptions from articles, issues, and books. The YEDA annotated texts are written back to CouchDB."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1545285-9904-478d-8122-8002caa05645",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Predict from CouchDB and save back to CouchDB using SkolClassifierV2\n",
    "print(\"Initializing classifier with unified V2 API...\")\n",
    "classifier = SkolClassifierV2(\n",
    "    spark=spark,\n",
    "    input_source='couchdb',\n",
    "    couchdb_url=couchdb_url,\n",
    "    couchdb_database=ingest_db_name,\n",
    "    couchdb_username=couchdb_username,\n",
    "    couchdb_password=couchdb_password,\n",
    "    couchdb_pattern='*.txt',\n",
    "    output_dest='couchdb',\n",
    "    output_couchdb_suffix='.ann',\n",
    "    model_storage='redis',\n",
    "    redis_client=redis_client,\n",
    "    redis_key=classifier_model_name,\n",
    "    auto_load_model=True,\n",
    "    line_level=True,\n",
    "    coalesce_labels=True,\n",
    "    output_format='annotated'\n",
    ")\n",
    "\n",
    "print(f\"Model loaded from Redis: {classifier_model_name}\")\n",
    "\n",
    "# Load, predict, and save in a streamlined workflow\n",
    "print(\"\\nLoading and classifying documents from CouchDB...\")\n",
    "raw_df = classifier.load_raw()\n",
    "print(f\"Loaded {raw_df.count()} text documents\")\n",
    "\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = classifier.predict(raw_df)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "predictions.select(\n",
    "    \"doc_id\", \"attachment_name\", \"predicted_label\"\n",
    ").show(5, truncate=50)\n",
    "\n",
    "# Save results back to CouchDB\n",
    "print(\"\\nSaving predictions back to CouchDB...\")\n",
    "classifier.save_annotated(predictions)\n",
    "\n",
    "print(f\"\\n✓ Predictions saved to CouchDB as .ann attachments\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aef5c46b-08bf-4b94-aa8a-aff7f545fc44",
   "metadata": {},
   "source": [
    "predictions.select(\"predicted_label\", \"annotated_value\").where('predicted_label = \"Nomenclature\"').show()\n",
    "predictions.groupBy(\"predicted_label\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002ae67-9dc9-4335-adf4-d249905d2ad9",
   "metadata": {},
   "source": [
    "Here we estimate an upper bound for the Taxon structures we'd like to find. The abbreviation \"nov.\" (\"novum\") indicates a new taxon in the current article."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad36a72c-2d93-402a-a95b-1c169a5e1088",
   "metadata": {},
   "source": [
    "predictions.select(\"*\").filter(col(\"annotated_value\").contains(\"nov.\")).where(\"predicted_label = 'Nomenclature'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca929b6a-75e5-4dcb-ace2-8d420f42ce41",
   "metadata": {},
   "source": [
    "## Build the Taxon objects and store them in CouchDB\n",
    "We use CouchDB to store a full record for each taxon. We copy all metadata to the taxon records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e285b7-0afa-486e-94f6-41bcb1d7524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDBFile(CDBF):\n",
    "    \"\"\"\n",
    "    File-like object that reads from CouchDB attachment content.\n",
    "\n",
    "    This class extends FileObject to support reading text from CouchDB\n",
    "    attachments while preserving database metadata (doc_id, attachment_name,\n",
    "    and database name).\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3de964f-332f-470e-a32e-ffef1a338047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from couchdb_file import read_couchdb_partition, read_couchdb_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7201b4fb-3b89-414e-b0d7-d62367bd05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_taxa_to_couchdb import (\n",
    "    TaxonExtractor,\n",
    "    generate_taxon_doc_id,\n",
    "    extract_taxa_from_partition,\n",
    "    convert_taxa_to_rows\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22648996-4d9f-47f1-ab4f-6b5fd54f97e9",
   "metadata": {},
   "source": [
    "## Build Taxon objects\n",
    "\n",
    "Here we extract the Taxon objects from the annotated attachments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08069896-722b-45c7-8b95-78cbb89e8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_couchdb_url = couchdb_url\n",
    "ingest_username = couchdb_username\n",
    "ingest_password = couchdb_password\n",
    "taxon_couchdb_url = couchdb_url\n",
    "taxon_username = couchdb_username\n",
    "taxon_password = couchdb_password\n",
    "pattern = '*.txt.ann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ed264e3-3e3c-4c32-bb99-2205edaa4bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaxonExtractor initialized\n",
      "  Ingest DB: skol_dev\n",
      "  Taxon DB:  skol_taxa_dev\n"
     ]
    }
   ],
   "source": [
    "# Create TaxonExtractor instance with database configuration\n",
    "extractor = TaxonExtractor(\n",
    "    spark=spark,\n",
    "    ingest_couchdb_url=ingest_couchdb_url,\n",
    "    ingest_db_name=ingest_db_name,\n",
    "    taxon_db_name=taxon_db_name,\n",
    "    ingest_username=ingest_username,\n",
    "    ingest_password=ingest_password,\n",
    "    taxon_username=taxon_username,\n",
    "    taxon_password=taxon_password\n",
    ")\n",
    "\n",
    "print(\"TaxonExtractor initialized\")\n",
    "print(f\"  Ingest DB: {ingest_db_name}\")\n",
    "print(f\"  Taxon DB:  {taxon_db_name}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d87b1aa9-d6e0-45ee-a7f3-a9e423713e70",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Step 1: Load annotated documents\n",
    "print(\"\\nStep 1: Loading annotated documents from CouchDB...\")\n",
    "annotated_df = extractor.load_annotated_documents(pattern='*.txt.ann')\n",
    "print(f\"Loaded {annotated_df.count()} annotated documents\")\n",
    "annotated_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d13e66b8-5f19-451b-8274-50d5141597e7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Step 2: Extract taxa to DataFrame\n",
    "print(\"\\nStep 2: Extracting taxa from annotated documents...\")\n",
    "taxa_df = extractor.extract_taxa(annotated_df)\n",
    "print(f\"Extracted {taxa_df.count()} taxa\")\n",
    "taxa_df.printSchema()\n",
    "taxa_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e677327c-0dc1-4736-bded-5f77f5389ad1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Step 3: Inspect actual Taxon objects from the RDD (optional debugging)\n",
    "print(\"\\n=== Sample Taxon Objects ===\")\n",
    "taxa_rdd = annotated_df.rdd.mapPartitions(\n",
    "    lambda partition: extract_taxa_from_partition(iter(partition), ingest_db_name)  # type: ignore[reportUnknownArgumentType]\n",
    ")\n",
    "for i, taxon in enumerate(taxa_rdd.take(3)):\n",
    "    print(f\"\\nTaxon {i+1}:\")\n",
    "    print(f\"  Type: {type(taxon)}\")\n",
    "    print(f\"  Has nomenclature: {taxon.has_nomenclature()}\")\n",
    "    taxon_row = taxon.as_row()\n",
    "    print(f\"  Taxon name: {taxon_row['taxon'][:80]}...\")\n",
    "    print(f\"  Source: {taxon_row['source']}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68c5390b-63dd-4bff-8a32-d780333d90a1",
   "metadata": {},
   "source": [
    "# Step 4: Save taxa to CouchDB\n",
    "print(\"\\nStep 4: Saving taxa to CouchDB...\")\n",
    "results_df = extractor.save_taxa(taxa_df)\n",
    "\n",
    "# Show detailed results\n",
    "results_df.groupBy(\"success\").count().show(truncate=False)\n",
    "\n",
    "# If there are failures, show error messages\n",
    "print(\"\\nError messages:\")\n",
    "results_df.filter(\"success = false\").select(\"error_message\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ewsk11g9lpl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run the complete pipeline in one step\n",
    "# Uncomment to use the simplified one-step approach:\n",
    "\n",
    "# print(\"\\nRunning complete pipeline...\")\n",
    "# results = extractor.run_pipeline(pattern='*.txt.ann')\n",
    "#\n",
    "# successful = results.filter(\"success = true\").count()\n",
    "# failed = results.filter(\"success = false\").count()\n",
    "#\n",
    "# print(f\"\\nPipeline Results:\")\n",
    "# print(f\"  Successful: {successful}\")\n",
    "# print(f\"  Failed:     {failed}\")\n",
    "#\n",
    "# results.groupBy(\"success\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ee719-eb73-425b-ad44-6f30158c8d5e",
   "metadata": {},
   "source": [
    "## Dr. Drafts document embedding\n",
    "\n",
    "Dr. Drafts loads documents from CouchDB. Save the embedding to redis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad6c550d-15f4-426c-b11e-d7ac6691e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dr_drafts_mycosearch.data import SKOL_TAXA as STX\n",
    "from dr_drafts_mycosearch.compute_embeddings import EmbeddingsComputer as EC\n",
    "\n",
    "class SKOL_TAXA(STX):\n",
    "    \"\"\"Data interface for Synopotic Key of Life Taxa in CouchDB.\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a79b3c-4ac4-4208-9346-0a5bc90e0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsComputer(EC):\n",
    "    \"\"\"Class for computing and storing embeddings from narrative data.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345caed-a301-49ca-a182-ce0ace7e839b",
   "metadata": {},
   "source": [
    "## Compute Embeddings\n",
    "\n",
    "We use SBERT to embed the taxa into a search space."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bc48f1d-c951-4ff1-80f3-ceb8d5b46f10",
   "metadata": {},
   "source": [
    "skol_taxa = SKOL_TAXA(\n",
    "    couchdb_url=\"http://localhost:5984\",\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password,\n",
    "    db_name=taxon_db_name\n",
    ")\n",
    "descriptions = skol_taxa.get_descriptions()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87453e29-2392-48ba-b0fe-91fe3517e1a6",
   "metadata": {},
   "source": [
    "if not redis_client.exists(embedding_name):\n",
    "\n",
    "    embedder = EmbeddingsComputer(\n",
    "        idir='/dev/null',\n",
    "        redis_url='redis://localhost:6379',\n",
    "        redis_expire=embedding_expire,\n",
    "        embedding_name=embedding_name,\n",
    "    )\n",
    "\n",
    "    embedding_result = embedder.run(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1498755-a114-4c46-985d-47f47e7abee8",
   "metadata": {},
   "source": [
    "## Compute JSON versions of all descriptions\n",
    "\n",
    "### Load the output of TaxonExtractor.save_taxa as a pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "904ef2d8-ce08-400a-91d6-f089d0f2e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaxaJSONTranslator initialized\n",
      "  Base model: mistralai/Mistral-7B-Instruct-v0.3\n",
      "  Checkpoint: None (using base model)\n",
      "  Device: cuda\n",
      "  4-bit quantization: True\n"
     ]
    }
   ],
   "source": [
    "translator = TaxaJSONTranslator(\n",
    "    spark=spark,\n",
    "    base_model_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    max_length=2048,\n",
    "    max_new_tokens=1024,\n",
    "    device=\"cuda\",\n",
    "    load_in_4bit=True,\n",
    "    use_auth_token=True,\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b0a40-6839-47c9-ac9e-409b2803f93e",
   "metadata": {},
   "source": [
    "### Run the mistral model to generate JSON from each Taxon description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e6091-e188-4500-9e85-ccc18bb03fd8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c87a7c5c-8d32-45e6-a271-0b11a1743e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Before the try.\n",
      "DEBUG: Connected to CouchDB database: skol_taxa_dev\n",
      "DEBUG: Total documents in DB: 5278\n",
      "DEBUG: Fetching document IDs with pattern: *\n",
      "DEBUG: Sample document IDs: ['taxon_0003fb9bf605d24e1f341b922b042c544ccde034321dbf19ca6e9455ca6f440c', 'taxon_000636a03e28108f6f2c3e5dd5f45103f41af82bb8e10238d7d46ea56b5452e5', 'taxon_0016a0981cc3ced613f11ce027b386bc7e4926dacead6b0478b1866b5f39061c', 'taxon_00182d1dc6748dfb5d4959ed841b0f37372fb45d483a32f6c1f64cc467e70338', 'taxon_001a414b35117487d6bf9556f7450fd946a8162d4b6bff64278808f1e07a5f17', 'taxon_0025983456627481e62fa2d5cffb95c360b99c29b4d5ad4d4577826b4f737b78', 'taxon_0036b5b5ac85008f0ff1792205a7dade8267dfb99f83689b96fa4a9805f0b593', 'taxon_004b7127256c0b76e6bbbfdfbfb069d58266b6b5f8ef2b0ff0d1c048368ac2e1', 'taxon_00527f58b55ebf751083f4df9b905e4393f316908afad43feec911ebc75b6fee', 'taxon_0067b36502861e202e46fcd6845151f23ba6c40acd8c3aa335f670118cb538a2']\n",
      "Loading 5278 taxa from skol_taxa_dev...\n",
      "✓ Loaded 0 taxa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "Error connecting to CouchDB: name 'username' is not defined\n",
      "Error connecting to CouchDB: name 'username' is not defined\n"
     ]
    }
   ],
   "source": [
    "descriptions_df = translator.load_taxa(couchdb_url=couchdb_url, db_name=taxon_db_name).limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f3b6128-af59-464c-8ed0-46ac9eb11ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting document IDs from CouchDB: 'CouchDBConnection' object has no attribute 'url'\n",
      "No documents found matching pattern '*'\n",
      "Translating descriptions to JSON (batch mode)...\n",
      "  Input column: description\n",
      "  Output column: json_annotated\n",
      "  Batch size: 10\n",
      "Processing 0 descriptions...\n"
     ]
    },
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkValueError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m descriptions_df = translator.load_taxa(couchdb_url=couchdb_url, db_name=taxon_db_name).limit(\u001b[32m3\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m json_annotated_df = \u001b[43mtranslator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate_descriptions_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtaxa_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescriptions_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdescription\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_annotated\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/piggy/src/github.com/piggyatbaqaqi/skol/taxa_json_translator.py:609\u001b[39m, in \u001b[36mTaxaJSONTranslator.translate_descriptions_batch\u001b[39m\u001b[34m(self, taxa_df, description_col, output_col, batch_size)\u001b[39m\n\u001b[32m    606\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDEBUG: Generated JSON:\u001b[39m\u001b[33m\"\u001b[39m, json_obj)\n\u001b[32m    608\u001b[39m \u001b[38;5;66;03m# Create DataFrame from results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m results_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[38;5;66;03m# Join back to original DataFrame on _id\u001b[39;00m\n\u001b[32m    612\u001b[39m enriched_df = taxa_df.join(results_df, on=\u001b[33m\"\u001b[39m\u001b[33m_id\u001b[39m\u001b[33m\"\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/sql/session.py:1443\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd.DataFrame):\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m).createDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m   1441\u001b[39m         data, schema, samplingRatio, verifySchema\n\u001b[32m   1442\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/sql/session.py:1485\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1483\u001b[39m     rdd, struct = \u001b[38;5;28mself\u001b[39m._createFromRDD(data.map(prepare), schema, samplingRatio)\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m     rdd, struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1487\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/sql/session.py:1093\u001b[39m, in \u001b[36mSparkSession._createFromLocal\u001b[39m\u001b[34m(self, data, schema)\u001b[39m\n\u001b[32m   1090\u001b[39m     data = \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m     converter = _create_converter(struct)\n\u001b[32m   1095\u001b[39m     tupled_data: Iterable[Tuple] = \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/sql/session.py:948\u001b[39m, in \u001b[36mSparkSession._inferSchemaFromList\u001b[39m\u001b[34m(self, data, names)\u001b[39m\n\u001b[32m    933\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    934\u001b[39m \u001b[33;03mInfer schema from list of Row, dict, or tuple.\u001b[39;00m\n\u001b[32m    935\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m:class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m    949\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    950\u001b[39m         message_parameters={},\n\u001b[32m    951\u001b[39m     )\n\u001b[32m    952\u001b[39m infer_dict_as_struct = \u001b[38;5;28mself\u001b[39m._jconf.inferDictAsStruct()\n\u001b[32m    953\u001b[39m infer_array_from_first_element = \u001b[38;5;28mself\u001b[39m._jconf.legacyInferArrayTypeFromFirstElement()\n",
      "\u001b[31mPySparkValueError\u001b[39m: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset."
     ]
    }
   ],
   "source": [
    "json_annotated_df = translator.translate_descriptions_batch(\n",
    "    taxa_df=descriptions_df,\n",
    "    batch_size=10,\n",
    "    description_col=\"description\",\n",
    "    output_col=\"json_annotated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e317f-b02a-417e-a03a-cfcb27ec2e87",
   "metadata": {},
   "source": [
    "### Add the generated fields as a field on the objects generated by save_taxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b7fc0-6b32-4466-a56a-7df7ebe009d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = translator.save_taxa(json_annotated_df)\n",
    "\n",
    "results_df.groupBy(\"success\").count().show(truncate=False)\n",
    "\n",
    "if failed > 0:\n",
    "    print(\"\\nError messages:\")\n",
    "    results_df.filter(\"success = false\").select(\"error_message\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f1457-b1bf-4e02-9b0c-ce194f33969d",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* doi Foundation, \"DOI Citation Formatter HTTP API\", https://citation.doi.org/api-docs.html, accessed 2025-11-12.\n",
    "* Yang, Jie and Zhang, Yue and Li, Linwei and Li, Xingxuan, 2018, \"YEDDA: A Lightweight Collaborative Text Span Annotation Tool\", Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, http://aclweb.org/anthology/P18-4006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82412f-0fd7-4526-a949-3732c2b26624",
   "metadata": {},
   "source": [
    "## Appendix: On the use of an AI Coder\n",
    "\n",
    "Portions of this work were completed with the aid of Claude Code Pro. I wish to give a clarifying example of how I've used this very powerful tool, and reveal why I am comfortable with claiming authorship of the resulting code.\n",
    "\n",
    "For this project I needed results from an earlier class project in which a trio of students built and evaluated models for classifying paragraphs. The earlier work was built as a iPython Notebook, with many examples and inline code. Just copying the earlier notebook would have introduced many irrelevant details and would not further the overall project.\n",
    "\n",
    "I asked Claude Code to translate the notebook into a module that I could import. It did a pretty good job. Without being told, it made a submodule, extract the illustrative code as examples, wrote reasonable documentation and created packaging for the module.\n",
    "\n",
    "The skill level of the coding was roughly that of a highly disciplined average junior programmer. The architecture was simplistic and violated several design constraints such as DRY. I requested specific refactorings, such as asking for a group of functions to be converted into an object that shared duplicated parameters.\n",
    "\n",
    "The initial code used REST interfaces directly, and read all the data into a single machine, not using pyspark correctly. Through a series of refactorings, I asked that the code use appropriate libraries I named, and create correct udf functions to execute transformations in parallel.\n",
    "\n",
    "I walked the AI through creating an object that I could use to illustrate my use of redis and couchdb interfaces, while leaving the irrelevant details in a separate library.\n",
    "\n",
    "In short, I still have to understand good design principles. I have to be able to recognize where appropriate libraries were applicable. I still have to understand the frameworks I am working with.\n",
    "\n",
    "I now have a strong understanding of the difference between \"vibe coding\" and AI-assisted software engineering. In my first 4 hours with Claude Code, I was able to produce roughly 4 days' worth of professional-grade working code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
