{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a626f7e-4127-4227-9906-dc08dd9135ce",
   "metadata": {},
   "source": [
    "# SKOL IV: All the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ae6b20-7abd-4ef6-b2cf-8d221a40d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/07 10:24:52 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 10.1.10.58 instead (on interface wlp130s0f0)\n",
      "25/12/07 10:24:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d44d1225-ad6b-4ff7-8122-b981df8e8c96;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 220ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d44d1225-ad6b-4ff7-8122-b981df8e8c96\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/9ms)\n",
      "25/12/07 10:24:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Spark context Web UI available at http://10.1.10.58:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1765103095502).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 21.0.9)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> :quit\n"
     ]
    }
   ],
   "source": [
    "bahir_package = 'org.apache.bahir:spark-sql-cloudant_2.12:2.4.0'\n",
    "!spark-shell --packages $bahir_package < /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9615b87-4962-47ca-b10f-98558713196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103097.862104  467519 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103098.480251  467519 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "import pickle\n",
    "import requests\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/nvidia/cusparselt/lib'\n",
    "\n",
    "# Be sure to get version 2: https://simple-repository.app.cern.ch/project/bibtexparser/2.0.0b8/description\n",
    "import bibtexparser\n",
    "import couchdb\n",
    "import feedparser\n",
    "import fitz # PyMuPDF\n",
    "\n",
    "import pandas as pd  # TODO(piggy): Remove this dependency in favor of pure pyspark DataFrames.\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, CountVectorizer, IDF, StringIndexer, VectorAssembler, IndexToString\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name, collect_list, regexp_extract, col, udf,\n",
    "    explode, trim, row_number, min, expr, concat, lit\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, BooleanType, IntegerType, MapType, NullType,\n",
    "    StringType, StructType, StructField\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import redis\n",
    "from uuid import uuid4\n",
    "\n",
    "# Local modules\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "parent_path = Path(parent_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from couchdb_file import CouchDBFile as CDBF\n",
    "from fileobj import FileObject\n",
    "from finder import parse_annotated, remove_interstitials\n",
    "import line\n",
    "from line import Line\n",
    "\n",
    "# Import SKOL classifiers\n",
    "from skol_classifier.classifier_v2 import SkolClassifierV2 as SC\n",
    "from skol_classifier.preprocessing import SuffixTransformer, ParagraphExtractor\n",
    "from skol_classifier.model import SkolModel\n",
    "from skol_classifier.output_formatters import CouchDBOutputWriter as CDBOW, YeddaFormatter\n",
    "from skol_classifier.utils import calculate_stats, get_file_list\n",
    "\n",
    "from taxon import group_paragraphs, Taxon\n",
    "\n",
    "from taxa_json_translator import TaxaJSONTranslator as TJT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd14626-a4bd-4684-81ef-ec5682a1d9aa",
   "metadata": {},
   "source": [
    "## Important constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc77721a-05a7-48f0-8780-fa71734ff01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "couchdb_host = \"127.0.0.1:5984\" # e.g., \"ACCOUNT.cloudant.com\" or \"localhost\"\n",
    "couchdb_username = \"admin\"\n",
    "couchdb_password = \"SU2orange!\"\n",
    "ingest_db_name = \"skol_dev\"  # Development ingestion database\n",
    "taxon_db_name = \"skol_taxa_dev\"  # Development Taxa database\n",
    "json_taxon_db_name = \"skol_taxa_full_dev\"  # Development Taxa database with JSON translations\n",
    "\n",
    "redis_host = 'localhost'\n",
    "redis_port = 6379\n",
    "\n",
    "embedding_name = 'skol:embedding:v1.3'\n",
    "embedding_expire = 60 * 60 * 24  # Expire after 24 hours\n",
    "classifier_model_name = \"skol:classifier:model:rnn-v1.6\"\n",
    "classifier_model_expire = 60 * 60 * 24  # Expire after 1 day.\n",
    "\n",
    "neo4j_uri = \"bolt://localhost:7687\"\n",
    "\n",
    "couchdb_url = f'http://{couchdb_host}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe0cba-ccae-4b9f-aff7-adccf30df427",
   "metadata": {},
   "source": [
    "## robots.txt\n",
    "\n",
    "We want to be a well-behaved web scraper. Respect `robots.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9570ec3-8bb3-41af-99df-d96907293a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"synoptickeyof.life\"\n",
    "\n",
    "ingenta_rp = RobotFileParser()\n",
    "ingenta_rp.set_url(\"https://www.ingentaconnect.com/robots.txt\")\n",
    "ingenta_rp.read() # Reads and parses the robots.txt file from the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994d34d5-1b5c-48b6-aa89-2ad6b015563a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: cloudant.protocol\n",
      "Warning: Ignoring non-Spark config property: cloudant.password\n",
      "Warning: Ignoring non-Spark config property: cloudant.host\n",
      "Warning: Ignoring non-Spark config property: cloudant.username\n",
      "25/12/07 10:25:01 WARN Utils: Your hostname, puchpuchobs resolves to a loopback address: 127.0.1.1; using 10.1.10.58 instead (on interface wlp130s0f0)\n",
      "25/12/07 10:25:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/data/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/piggy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/piggy/.ivy2/jars\n",
      "org.apache.bahir#spark-sql-cloudant_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1303f920-8aac-4967-9d49-85232ace338c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-sql-cloudant_2.12;2.4.0 in central\n",
      "\tfound org.apache.bahir#bahir-common_2.12;2.4.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound com.cloudant#cloudant-client;2.17.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.2 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.cloudant#cloudant-http;2.17.0 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.2 in central\n",
      "\tfound com.squareup.okio#okio;1.15.0 in central\n",
      "\tfound com.typesafe#config;1.3.1 in central\n",
      "\tfound org.scalaj#scalaj-http_2.12;2.3.0 in central\n",
      ":: resolution report :: resolve 247ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.cloudant#cloudant-client;2.17.0 from central in [default]\n",
      "\tcom.cloudant#cloudant-http;2.17.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.2 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.2 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\torg.apache.bahir#bahir-common_2.12;2.4.0 from central in [default]\n",
      "\torg.apache.bahir#spark-sql-cloudant_2.12;2.4.0 from central in [default]\n",
      "\torg.scalaj#scalaj-http_2.12;2.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1303f920-8aac-4967-9d49-85232ace338c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/7ms)\n",
      "25/12/07 10:25:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CouchDB Spark SQL Example in Python using dataframes\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"cloudant.protocol\", \"http\") \\\n",
    "    .config(\"cloudant.host\", couchdb_host) \\\n",
    "    .config(\"cloudant.username\", couchdb_username) \\\n",
    "    .config(\"cloudant.password\", couchdb_password) \\\n",
    "    .config(\"spark.jars.packages\", bahir_package) \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .config(\"spark.submit.pyFiles\",\n",
    "            f'{parent_path / \"line.py\"},{parent_path / \"fileobj.py\"},'\n",
    "            f'{parent_path / \"couchdb_file.py\"},{parent_path / \"finder.py\"},'\n",
    "            f'{parent_path / \"taxon.py\"},{parent_path / \"paragraph.py\"},'\n",
    "            f'{parent_path / \"label.py\"},{parent_path / \"file.py\"},'\n",
    "            f'{parent_path / \"extract_taxa_to_couchdb.py\"}'\n",
    "           ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!\n",
    "\n",
    "couch = couchdb.Server(couchdb_url)\n",
    "couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "\n",
    "if ingest_db_name not in couch:\n",
    "    db = couch.create(ingest_db_name)\n",
    "else:\n",
    "    db = couch[ingest_db_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2510209d-6652-49b2-b8c7-59fc98865510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Redis\n",
    "redis_client = redis.Redis(\n",
    "    host=redis_host,\n",
    "    port=redis_port,\n",
    "    db=0,\n",
    "    decode_responses=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e7f93-bb66-4922-b3f9-c9429bc304ac",
   "metadata": {},
   "source": [
    "## The Data Sources\n",
    "\n",
    "The goal is to collect all the open access taxonomic literature in Mycology. Most of the sources below mainly cover macro-fungi and slime molds.\n",
    "\n",
    "### Ingested Data Sources\n",
    "\n",
    "* [Mycotaxon at Ingenta Connect](https://www.ingentaconnect.com/content/mtax/mt)\n",
    "* [Studies in Mycology at Ingenta Connect](https://www.studiesinmycology.org/)\n",
    "\n",
    "### Source of many older public domain and open access works\n",
    "\n",
    "Mycoweb includes scans of many older works in mycology. I have local copies but need to write ingesters for them.\n",
    "\n",
    "* [Mycoweb](https://mykoweb.com/)\n",
    "\n",
    "### Journals in hand\n",
    "\n",
    "These are journals we've collected over the years. The initial annotated issues are from early years of Mycotaxon. We still need to write ingesters for all of these.\n",
    "\n",
    "* Mycologia (back issues)\n",
    "* [Mycologia at Taylor and Francis](https://www.tandfonline.com/journals/umyc20)\n",
    "  Mycologia is the main journal of the Mycological Society of America. It is a mix of open access and traditional access articles. The connector for this journal will need to identify the open access articles.\n",
    "* Persoonia (all issues)\n",
    "  Persoonia is no longer published.\n",
    "* Mycotaxon (back issues)\n",
    "  Mycotaxon is no longer published.\n",
    "\n",
    "### Journals that need connectors\n",
    "\n",
    "These are journals we're aware that include open access articles.\n",
    "\n",
    "* [Amanitaceae.org](http://www.tullabs.com/amanita/?home)\n",
    "* [Mycosphere](https://mycosphere.org/)\n",
    "* [Mycoscience](https://mycoscience.org/)\n",
    "* [Journal of Fungi](https://www.mdpi.com/journal/jof)\n",
    "* [Mycology](https://www.tandfonline.com/journals/tmyc20)\n",
    "* [Open Access Journal of Mycology & Mycological Sciences](https://www.medwinpublishers.com/OAJMMS/)\n",
    "* [Mycokeys](https://mycokeys.pensoft.net/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a4213-adb5-49e5-b973-570a75cc2cce",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Each journal or other data source gets an ingester that puts PDFs into our document store along with any metadata we can collect. The metadata is sufficient to create citations for each issue, book, or article. If bibtex citations are available we prefer to store these verbatim.\n",
    "\n",
    "### Ingenta RSS ingestion\n",
    "\n",
    "Ingenta Connect is an electronic publisher that holds two Mycology journals. New articles are available via RSS (Really Simple Syndication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d303cc7-f01c-4e15-87ff-6c259d010591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_bibtex(\n",
    "        db: couchdb.Database,\n",
    "        content: bytes,\n",
    "        bibtex_link: str,\n",
    "        meta: Dict[str, Any],\n",
    "        rp\n",
    "        ) -> None:\n",
    "    \"\"\"Load documents referenced in an Ingenta BibTeX database.\"\"\"\n",
    "    bib_database = bibtexparser.parse_string(content)\n",
    "\n",
    "    bibtex_data = {\n",
    "        'link': bibtex_link,\n",
    "        'bibtex': bibtexparser.write_string(bib_database),\n",
    "    }\n",
    "\n",
    "    for bib_entry in bib_database.entries:\n",
    "        doc = {\n",
    "            '_id': uuid4().hex,\n",
    "            'meta': meta,\n",
    "            'pdf_url': f\"{bib_entry['url']}?crawler=true\",\n",
    "        }\n",
    "\n",
    "        # Do not fetch if we already have an entry.\n",
    "        selector = {'selector': {'pdf_url': doc['pdf_url']}}\n",
    "        found = False\n",
    "        for e in db.find(selector):\n",
    "            found = True\n",
    "        if found:\n",
    "            print(f\"Skipping {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        if not rp.can_fetch(user_agent, doc['pdf_url']):\n",
    "            # TODO(piggy): We should probably log blocked URLs.\n",
    "            print(f\"Robot permission denied {doc['pdf_url']}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Adding {doc['pdf_url']}\")\n",
    "        for k in bib_entry.fields_dict.keys():\n",
    "            doc[k] = bib_entry[k]\n",
    "\n",
    "        doc_id, doc_rev = db.save(doc)\n",
    "        with requests.get(doc['pdf_url'], stream=False) as pdf_f:\n",
    "            pdf_f.raise_for_status()\n",
    "            pdf_doc = pdf_f.content\n",
    "\n",
    "        attachment_filename = 'article.pdf'\n",
    "        attachment_content_type = 'application/pdf'\n",
    "        attachment_file = BytesIO(pdf_doc)\n",
    "\n",
    "        db.put_attachment(doc, attachment_file, attachment_filename, attachment_content_type)\n",
    "\n",
    "        print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b323f98-5bd8-4106-b79c-2ff9ac1ac74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_ingenta(\n",
    "        db: couchdb.Database,\n",
    "        rss_url: str,\n",
    "        rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest documents from an Ingenta RSS feed.\"\"\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    feed_meta = {\n",
    "        'url': rss_url,_utils/#/_al\n",
    "        'title': feed.feed.title,\n",
    "        'link': feed.feed.link,\n",
    "        'description': feed.feed.description,\n",
    "    }\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        entry_meta = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "        }\n",
    "        if hasattr(entry, 'summary'):\n",
    "            entry_meta['summary'] = entry.summary\n",
    "        if hasattr(entry, 'description'):\n",
    "            entry_meta['description'] = entry.description\n",
    "\n",
    "        bibtex_link = f'{entry.link}?format=bib'\n",
    "        print(f\"bibtex_link: {bibtex_link}\")\n",
    "\n",
    "        if not rp.can_fetch(user_agent, bibtex_link):\n",
    "            print(f\"Robot permission denied {bibtex_link}\")\n",
    "            continue\n",
    "\n",
    "        with requests.get(bibtex_link, stream=False) as bibtex_f:\n",
    "            bibtex_f.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            ingest_from_bibtex(\n",
    "                db=db,\n",
    "                content=bibtex_f.content\\\n",
    "                    .replace(b\"\\\"\\nparent\", b\"\\\",\\nparent\")\\\n",
    "                    .replace(b\"\\n\", b\"\"),\n",
    "                bibtex_link=bibtex_link,\n",
    "                meta={\n",
    "                    'feed': feed_meta,\n",
    "                    'entry': entry_meta,\n",
    "                },\n",
    "                rp=rp\n",
    "            )\n",
    "        print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234f9cc5-9b7e-4559-ad10-f24ad269e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_from_local_bibtex(\n",
    "    db: couchdb.Database,\n",
    "    root: Path,\n",
    "    rp\n",
    ") -> None:\n",
    "    \"\"\"Ingest from a local directory with Ingenta bibtext files in it.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if not filename.endswith('format=bib'):\n",
    "                continue\n",
    "            full_filepath = os.path.join(dirpath, filename)\n",
    "            bibtex_link = f\"https://www.ingentaconnect.com/{full_filepath[len(str(root)):]}\"\n",
    "            with open(full_filepath) as f:\n",
    "                # Paper over a syntax problem in Ingenta Connect Bibtex files.\n",
    "                content = f.read()\\\n",
    "                    .replace(\"\\\"\\nparent\", \"\\\",\\nparent\")\\\n",
    "                    .replace(\"\\n\", \"\")\n",
    "                ingest_from_bibtex(db, content, bibtex_link, meta={}, rp=rp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58829ec0-93f7-44dd-87bc-511ab2d238d0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mycotaxon\n",
    "\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/mtax/mt?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40602cc3-e231-497d-adc7-d6d4471514cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Studies in Mycology\n",
    "\n",
    "ingest_ingenta(db=db, rss_url='https://api.ingentaconnect.com/content/wfbi/sim?format=rss', rp=ingenta_rp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "651f82ea-b3b8-4f0d-adf0-f5550d7ed6d5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ingest_from_local_bibtex(\n",
    "    db=db,\n",
    "    root=Path(\"/data/skol/www/www.ingentaconnect.com\"),\n",
    "    rp=ingenta_rp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf05ad-d2bf-4716-b253-07455cfefcd0",
   "metadata": {},
   "source": [
    "### Text extraction\n",
    "\n",
    "We extract the text, optionally with OCR. Add as an additional attachment on the source record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e5e9725-0c3a-4c41-8ebf-2929c4dc4334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\n",
    "    format=\"org.apache.bahir.cloudant\",\n",
    "    database=ingest_db_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e75c64e-95cb-478a-9a85-46d1297e4225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _id: string, _rev: string, abstract: string, author: string, doi: string, eissn: string, issn: string, itemtype: string, journal: string, number: string, pages: string, parent_itemid: string, pdf_url: string, publication date: string, publishercode: string, title: string, url: string, volume: string, year: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9befa546-a895-4a13-8744-d512587d9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Type: text/html; charset=UTF-8\n",
    "\n",
    "def pdf_to_text(pdf_contents: bytes) -> bytes:\n",
    "    doc = fitz.open(stream=BytesIO(pdf_contents), filetype=\"pdf\")\n",
    "\n",
    "    full_text = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        # Possibly perform OCR on the page\n",
    "        text = page.get_text(\"text\", flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_DEHYPHENATE)\n",
    "        # full_text += f\"\\n--- PDF Page {page_num+1} ---\\n\"  # TODO(piggy): Introduce PDF page tracking in line-by-line and paragraph parsers.\n",
    "        full_text += text\n",
    "\n",
    "    return full_text.encode(\"utf-8\")\n",
    "\n",
    "def add_text_to_partition(iterator) -> None:\n",
    "    couch = couchdb.Server(couchdb_url)\n",
    "    couch.resource.credentials = (couchdb_username, couchdb_password)\n",
    "    local_db = couch[ingest_db_name]\n",
    "    for row in iterator:\n",
    "        if not row:\n",
    "            continue\n",
    "        if not row._attachments:\n",
    "            continue\n",
    "        row_dict = row.asDict()\n",
    "        attachment_dict = row._attachments.asDict()\n",
    "        for pdf_filename in attachment_dict:\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            if pdf_path.suffix != '.pdf':\n",
    "                continue\n",
    "            pdf_path = PurePath(pdf_filename)\n",
    "            txt_path_str = pdf_path.stem + '.txt'\n",
    "            # if txt_path_str in attachment_dict:\n",
    "            #     # TODO(piggy): Recalculate text if text is terrible. Too much noise vocabulary?\n",
    "            #     print(f\"Already have text for {row.pdf_url}\")\n",
    "            #     continue\n",
    "            print(f\"{row._id}, {row.pdf_url}\")\n",
    "            pdf_file = local_db.get_attachment(row._id, str(pdf_path)).read()\n",
    "            txt_file = pdf_to_text(pdf_file)\n",
    "            attachment_content_type = 'text/simple; charset=UTF-8'\n",
    "            attachment_file = BytesIO(txt_file)\n",
    "            local_db.put_attachment(row_dict, attachment_file, txt_path_str, attachment_content_type)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74366376-ec05-474f-b172-216093c62865",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df.select(\"*\").foreachPartition(add_text_to_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11805c62-ae42-4959-9673-1e09261a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to skol_classifier.CouchDBConnection.\n",
    "from skol_classifier.couchdb_io import CouchDBConnection as CDBC\n",
    "\n",
    "class CouchDBConnection(CDBC):\n",
    "    \"\"\"\n",
    "    Manages CouchDB connection and provides I/O operations.\n",
    "\n",
    "    This class encapsulates connection parameters and provides an idempotent\n",
    "    connection method that can be safely called multiple times.\n",
    "    \"\"\"\n",
    "\n",
    "    # Shared schema definitions (DRY principle)\n",
    "    LOAD_SCHEMA = StructType([\n",
    "        StructField(\"doc_id\", StringType(), False),\n",
    "        StructField(\"human_url\", StringType(), False),\n",
    "        StructField(\"attachment_name\", StringType(), False),\n",
    "        StructField(\"value\", StringType(), False),\n",
    "    ])\n",
    "\n",
    "    SAVE_SCHEMA = StructType([\n",
    "        StructField(\"doc_id\", StringType(), False),\n",
    "        StructField(\"attachment_name\", StringType(), False),\n",
    "        StructField(\"success\", BooleanType(), False),\n",
    "    ])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize CouchDB connection parameters.\n",
    "\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL (e.g., \"http://localhost:5984\")\n",
    "            database: Database name\n",
    "            username: Optional username for authentication\n",
    "            password: Optional password for authentication\n",
    "        \"\"\"\n",
    "        self.couchdb_url = couchdb_url\n",
    "        self.database = database\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self._server = None\n",
    "        self._db = None\n",
    "\n",
    "    def _connect(self):\n",
    "        \"\"\"\n",
    "        Idempotent connection method that returns a CouchDB server object.\n",
    "\n",
    "        This method can be called multiple times safely - it will only create\n",
    "        a connection if one doesn't already exist.\n",
    "\n",
    "        Returns:\n",
    "            couchdb.Server: Connected CouchDB server object\n",
    "        \"\"\"\n",
    "        if self._server is None:\n",
    "            self._server = couchdb.Server(self.couchdb_url)\n",
    "            if self.username and self.password:\n",
    "                self._server.resource.credentials = (self.username, self.password)\n",
    "\n",
    "        if self._db is None:\n",
    "            self._db = self._server[self.database]\n",
    "\n",
    "        return self._server\n",
    "\n",
    "    @property\n",
    "    def db(self):\n",
    "        \"\"\"Get the database object, connecting if necessary.\"\"\"\n",
    "        if self._db is None:\n",
    "            self._connect()\n",
    "        return self._db\n",
    "\n",
    "    def get_all_doc_ids(self, pattern: str = \"*\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of document IDs matching the pattern from CouchDB.\n",
    "\n",
    "        Args:\n",
    "            pattern: Pattern for document IDs (e.g., \"taxon_*\", \"*\")\n",
    "                    - \"*\" matches all non-design documents\n",
    "                    - \"prefix*\" matches documents starting with prefix\n",
    "                    - \"exact\" matches exactly\n",
    "\n",
    "        Returns:\n",
    "            List of matching document IDs\n",
    "        \"\"\"\n",
    "        db = self.db\n",
    "        \n",
    "        # Get all document IDs (excluding design documents)\n",
    "        all_doc_ids = [doc_id for doc_id in list(db) if not doc_id.startswith('_design/')]\n",
    "\n",
    "        # Filter by pattern\n",
    "        if pattern == \"*\":\n",
    "            # Return all non-design documents\n",
    "            return all_doc_ids\n",
    "        elif pattern.endswith('*'):\n",
    "            # Prefix matching\n",
    "            prefix = pattern[:-1]\n",
    "            return [doc_id for doc_id in all_doc_ids if doc_id.startswith(prefix)]\n",
    "        else:\n",
    "            # Exact match\n",
    "            return [doc_id for doc_id in all_doc_ids if doc_id == pattern]\n",
    "\n",
    "    def get_document_list(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Get a list of documents with text attachments from CouchDB.\n",
    "\n",
    "        This only fetches document metadata (not content) to create a DataFrame\n",
    "        that can be processed in parallel. Creates ONE ROW per attachment, so if\n",
    "        a document has multiple attachments matching the pattern, it will have\n",
    "        multiple rows in the resulting DataFrame.\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names (e.g., \"*.txt\")\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, attachment_name\n",
    "            One row per (doc_id, attachment_name) pair\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB (driver only)\n",
    "        db = self.db\n",
    "\n",
    "        # Get all documents with attachments matching pattern\n",
    "        doc_list = []\n",
    "        for doc_id in db:\n",
    "            try:\n",
    "                doc = db[doc_id]\n",
    "                attachments = doc.get('_attachments', {})\n",
    "\n",
    "                # Loop through ALL attachments in the document\n",
    "                for att_name in attachments.keys():\n",
    "                    # Check if attachment matches pattern\n",
    "                    # Pattern matching: \"*.txt\" matches files ending with .txt\n",
    "                    if pattern == \"*.txt\" and att_name.endswith('.txt'):\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern == \"*.*\" or pattern == \"*\":\n",
    "                        # Match all attachments\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "                    elif pattern.startswith(\"*.\") and att_name.endswith(pattern[1:]):\n",
    "                        # Generic pattern matching for *.ext\n",
    "                        doc_list.append((doc_id, att_name))\n",
    "            except Exception:\n",
    "                # Skip documents we can't read\n",
    "                continue\n",
    "\n",
    "        # Create DataFrame with document IDs and attachment names\n",
    "        schema = StructType([\n",
    "            StructField(\"doc_id\", StringType(), False),\n",
    "            StructField(\"attachment_name\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        return spark.createDataFrame(doc_list, schema)\n",
    "\n",
    "    def fetch_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row]\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Fetch CouchDB attachments for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id and attachment_name\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, human_url, attachment_name, and value (content).\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document has multiple .txt attachments, there will be multiple rows\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Get the specific attachment for this row\n",
    "                    if row.attachment_name in doc.get('_attachments', {}):\n",
    "                        attachment = db.get_attachment(doc, row.attachment_name)\n",
    "                        if attachment:\n",
    "                            content = attachment.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "                            yield Row(\n",
    "                                doc_id=row.doc_id,\n",
    "                                human_url=doc.get('url', 'missing_human_url'),\n",
    "                                attachment_name=row.attachment_name,\n",
    "                                value=content\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    # Log error but continue processing\n",
    "                    print(f\"Error fetching {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            return\n",
    "\n",
    "    def save_partition(\n",
    "        self,\n",
    "        partition: Iterator[Row],\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> Iterator[Row]:\n",
    "        \"\"\"\n",
    "        Save annotated content to CouchDB for an entire partition.\n",
    "\n",
    "        This function is designed to be used with foreachPartition or mapPartitions.\n",
    "        It creates a single CouchDB connection per partition and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            partition: Iterator of Rows with doc_id, attachment_name, final_aggregated_pg\n",
    "                       and optionally human_url\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Yields:\n",
    "            Rows with doc_id, attachment_name, and success status.\n",
    "        \"\"\"\n",
    "        # Connect to CouchDB once per partition\n",
    "        try:\n",
    "            db = self.db\n",
    "\n",
    "            # Process all rows in partition with same connection\n",
    "            # Note: Each row represents one (doc_id, attachment_name) pair\n",
    "            # If a document had multiple .txt files, we save multiple .ann files\n",
    "            for row in partition:\n",
    "                success = False\n",
    "                try:\n",
    "                    doc = db[row.doc_id]\n",
    "\n",
    "                    # Update human_url field if provided\n",
    "                    if hasattr(row, 'human_url') and row.human_url:\n",
    "                        doc['url'] = row.human_url\n",
    "                        db.save(doc)\n",
    "                        # Reload doc to get updated _rev\n",
    "                        doc = db[row.doc_id]\n",
    "\n",
    "                    # Create new attachment name by appending suffix\n",
    "                    # e.g., \"article.txt\" becomes \"article.txt.ann\"\n",
    "                    new_attachment_name = f\"{row.attachment_name}{suffix}\"\n",
    "\n",
    "                    # Save the annotated content as a new attachment\n",
    "                    db.put_attachment(\n",
    "                        doc,\n",
    "                        row.final_aggregated_pg.encode('utf-8'),\n",
    "                        filename=new_attachment_name,\n",
    "                        content_type='text/plain'\n",
    "                    )\n",
    "\n",
    "                    success = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving {row.doc_id}/{row.attachment_name}: {e}\")\n",
    "\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=success\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to CouchDB: {e}\")\n",
    "            # Yield failures for all rows\n",
    "            for row in partition:\n",
    "                yield Row(\n",
    "                    doc_id=row.doc_id,\n",
    "                    attachment_name=row.attachment_name,\n",
    "                    success=False\n",
    "                )\n",
    "\n",
    "    def load_distributed(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        pattern: str = \"*.txt\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Load text attachments from CouchDB using foreachPartition.\n",
    "\n",
    "        This function:\n",
    "        1. Gets list of documents (on driver)\n",
    "        2. Creates a DataFrame with doc IDs\n",
    "        3. Uses mapPartitions to fetch content efficiently (one connection per partition)\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession\n",
    "            pattern: Pattern for attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: doc_id, human_url, attachment_name, and value.\n",
    "        \"\"\"\n",
    "        # Get document list\n",
    "        doc_df = self.get_document_list(spark, pattern)\n",
    "\n",
    "        # Use mapPartitions for efficient batch fetching\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def fetch_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.fetch_partition(partition)\n",
    "\n",
    "        # Apply mapPartitions using shared schema\n",
    "        result_df = doc_df.rdd.mapPartitions(fetch_partition).toDF(self.LOAD_SCHEMA)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def save_distributed(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        suffix: str = \".ann\"\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Save annotated predictions to CouchDB using foreachPartition.\n",
    "\n",
    "        This function uses mapPartitions where each partition creates a single\n",
    "        CouchDB connection and reuses it for all rows.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with columns: doc_id, attachment_name, final_aggregated_pg\n",
    "            suffix: Suffix to append to attachment names\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with doc_id, attachment_name, and success columns\n",
    "        \"\"\"\n",
    "        # Use mapPartitions for efficient batch saving\n",
    "        # Create new connection instance with same params for workers\n",
    "        conn_params = (self.couchdb_url, self.database, self.username, self.password)\n",
    "\n",
    "        def save_partition(partition):\n",
    "            # Each worker creates its own connection\n",
    "            conn = CouchDBConnection(*conn_params)\n",
    "            return conn.save_partition(partition, suffix)\n",
    "\n",
    "        # Apply mapPartitions using shared schema\n",
    "        result_df = df.rdd.mapPartitions(save_partition).toDF(self.SAVE_SCHEMA)\n",
    "\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "689d4df6-59c4-4ac4-9f9a-e849bd3b21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDBOutputWriter(CDBOW):\n",
    "    \"\"\"\n",
    "    Writes predictions back to CouchDB as attachments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        couchdb_url: str,\n",
    "        database: str,\n",
    "        username: str,\n",
    "        password: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the writer.\n",
    "\n",
    "        Args:\n",
    "            couchdb_url: CouchDB server URL\n",
    "            database: Database name\n",
    "            username: CouchDB username\n",
    "            password: CouchDB password\n",
    "        \"\"\"\n",
    "        self.conn = CouchDBConnection(\n",
    "            couchdb_url=couchdb_url,\n",
    "            database=database,\n",
    "            username=username,\n",
    "            password=password\n",
    "        )\n",
    "\n",
    "    def save_annotated(\n",
    "        self,\n",
    "        predictions: DataFrame,\n",
    "        suffix: str = \".ann\",\n",
    "        coalesce_labels: bool = False,\n",
    "        line_level: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save predictions to CouchDB as attachments.\n",
    "\n",
    "        Args:\n",
    "            predictions: DataFrame with predictions\n",
    "            suffix: Suffix for attachment names\n",
    "            coalesce_labels: Whether to coalesce consecutive labels\n",
    "            line_level: Whether data is line-level\n",
    "        \"\"\"\n",
    "        # Format predictions\n",
    "        if \"annotated_value\" not in predictions.columns:\n",
    "            predictions = YeddaFormatter.format_predictions(predictions)\n",
    "\n",
    "        # Coalesce if requested\n",
    "        if coalesce_labels and line_level:\n",
    "            predictions = YeddaFormatter.coalesce_consecutive_labels(\n",
    "                predictions, line_level=True\n",
    "            )\n",
    "            # For coalesced output, we have coalesced_annotations column\n",
    "            # We need to join them into final_aggregated_pg\n",
    "            from pyspark.sql.functions import expr\n",
    "            predictions = predictions.withColumn(\n",
    "                \"final_aggregated_pg\",\n",
    "                expr(\"array_join(coalesced_annotations, '\\n')\")\n",
    "            )\n",
    "        else:\n",
    "            # Aggregate annotated values by document\n",
    "            groupby_col = \"doc_id\" if \"doc_id\" in predictions.columns else \"filename\"\n",
    "            attachment_col = \"attachment_name\" if \"attachment_name\" in predictions.columns else \"filename\"\n",
    "\n",
    "            # Check if we have line_number for ordering\n",
    "            if \"line_number\" in predictions.columns:\n",
    "                from pyspark.sql.functions import expr\n",
    "                predictions = (\n",
    "                    predictions.groupBy(groupby_col, attachment_col)\n",
    "                    .agg(\n",
    "                        expr(\"sort_array(collect_list(struct(line_number, annotated_value))) AS sorted_list\")\n",
    "                    )\n",
    "                    .withColumn(\"annotated_value_ordered\", expr(\"transform(sorted_list, x -> x.annotated_value)\"))\n",
    "                    .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotated_value_ordered, '\\n')\"))\n",
    "                    .select(groupby_col, attachment_col, \"final_aggregated_pg\")\n",
    "                )\n",
    "            else:\n",
    "                from pyspark.sql.functions import collect_list, expr\n",
    "                predictions = (\n",
    "                    predictions.groupBy(groupby_col, attachment_col)\n",
    "                    .agg(\n",
    "                        collect_list(\"annotated_value\").alias(\"annotations\")\n",
    "                    )\n",
    "                    .withColumn(\"final_aggregated_pg\", expr(\"array_join(annotations, '\\n')\"))\n",
    "                    .select(groupby_col, attachment_col, \"final_aggregated_pg\")\n",
    "                )\n",
    "\n",
    "            # Rename columns for CouchDB save\n",
    "            if groupby_col != \"doc_id\":\n",
    "                predictions = predictions.withColumnRenamed(groupby_col, \"doc_id\")\n",
    "            if attachment_col != \"attachment_name\":\n",
    "                predictions = predictions.withColumnRenamed(attachment_col, \"attachment_name\")\n",
    "\n",
    "        # Use CouchDB connection to save\n",
    "        self.conn.save_distributed(predictions, suffix=suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a0fa58-e693-4978-b33e-67010e066948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main classifier module for SKOL text classification\n",
    "\"\"\"\n",
    "class SkolClassifierV2(SC):\n",
    "    \"\"\"\n",
    "    Text classifier for taxonomic literature.\n",
    "\n",
    "    This version only includes the redis and couchdb I/O methods.\n",
    "    All other methods are in SC.\n",
    "\n",
    "    Supports multiple classification models (Logistic Regression, Random Forest)\n",
    "    and feature types (word TF-IDF, suffix TF-IDF, combined).\n",
    "    \"\"\"\n",
    "    def _load_raw_from_couchdb(self) -> DataFrame:\n",
    "        \"\"\"Load raw text from CouchDB.\"\"\"\n",
    "        conn = CouchDBConnection(\n",
    "            self.couchdb_url,\n",
    "            self.couchdb_database,\n",
    "            self.couchdb_username,\n",
    "            self.couchdb_password\n",
    "        )\n",
    "\n",
    "        df = conn.load_distributed(self.spark, self.couchdb_pattern)\n",
    "\n",
    "        # Split into lines if line_level mode\n",
    "        if self.line_level:\n",
    "            from pyspark.sql.functions import explode, split, col, trim, row_number, lit\n",
    "            from pyspark.sql.window import Window\n",
    "\n",
    "            df = df.withColumn(\"value\", explode(split(col(\"value\"), \"\\\\n\")))\n",
    "            df = df.filter(trim(col(\"value\")) != \"\")\n",
    "\n",
    "            # Add line numbers\n",
    "            window_spec = Window.partitionBy(\"doc_id\", \"attachment_name\").orderBy(lit(1))\n",
    "            df = df.withColumn(\"line_number\", row_number().over(window_spec) - 1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _load_annotated_from_couchdb(self) -> DataFrame:\n",
    "        \"\"\"Load annotated text from CouchDB.\"\"\"\n",
    "        # Load raw annotations from CouchDB\n",
    "        conn = CouchDBConnection(\n",
    "            self.couchdb_url,\n",
    "            self.couchdb_database,\n",
    "            self.couchdb_username,\n",
    "            self.couchdb_password\n",
    "        )\n",
    "\n",
    "        # Look for .ann files for training\n",
    "        pattern = self.couchdb_pattern\n",
    "        if not pattern.endswith('.ann'):\n",
    "            pattern = pattern.replace('.txt', '.txt.ann')\n",
    "\n",
    "        df = conn.load_distributed(self.spark, pattern)\n",
    "\n",
    "        # Parse annotations\n",
    "        from .preprocessing import AnnotatedTextParser\n",
    "\n",
    "        parser = AnnotatedTextParser(line_level=self.line_level)\n",
    "        return parser.parse(df)\n",
    "        \n",
    "    def _save_to_couchdb(self, predictions: DataFrame) -> None:\n",
    "        \"\"\"Save predictions to CouchDB.\"\"\"\n",
    "\n",
    "        writer = CouchDBOutputWriter(\n",
    "            couchdb_url=self.couchdb_url,\n",
    "            database=self.couchdb_database,\n",
    "            username=self.couchdb_username,\n",
    "            password=self.couchdb_password\n",
    "        )\n",
    "\n",
    "        writer.save_annotated(\n",
    "            predictions,\n",
    "            suffix=self.output_couchdb_suffix,\n",
    "            coalesce_labels=self.coalesce_labels,\n",
    "            line_level=self.line_level\n",
    "        )\n",
    "\n",
    "    def _save_model_to_redis(self) -> None:\n",
    "        \"\"\"Save model to Redis using tar archive.\"\"\"\n",
    "        import json\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        import tarfile\n",
    "        import io\n",
    "\n",
    "        if self._model is None or self._feature_pipeline is None:\n",
    "            raise ValueError(\"No model to save. Train a model first.\")\n",
    "\n",
    "        temp_dir = None\n",
    "        try:\n",
    "            # Create temporary directory\n",
    "            temp_dir = tempfile.mkdtemp(prefix=\"skol_model_v2_\")\n",
    "            temp_path = Path(temp_dir)\n",
    "\n",
    "            # Save feature pipeline\n",
    "            pipeline_path = temp_path / \"feature_pipeline\"\n",
    "            self._feature_pipeline.save(str(pipeline_path))\n",
    "\n",
    "            # Save classifier model\n",
    "            classifier_model = self._model.get_model()\n",
    "            if classifier_model is None:\n",
    "                raise ValueError(\"Classifier model not trained\")\n",
    "            classifier_path = temp_path / \"classifier_model\"\n",
    "            classifier_model.save(str(classifier_path))\n",
    "\n",
    "            # Save metadata\n",
    "            metadata = {\n",
    "                'label_mapping': self._label_mapping,\n",
    "                'config': {\n",
    "                    'line_level': self.line_level,\n",
    "                    'use_suffixes': self.use_suffixes,\n",
    "                    'min_doc_freq': self.min_doc_freq,\n",
    "                    'model_type': self.model_type,\n",
    "                    'model_params': self.model_params\n",
    "                },\n",
    "                'version': '2.0'\n",
    "            }\n",
    "            metadata_path = temp_path / \"metadata.json\"\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "\n",
    "            # Create tar archive\n",
    "            archive_buffer = io.BytesIO()\n",
    "            with tarfile.open(fileobj=archive_buffer, mode='w:gz') as tar:\n",
    "                tar.add(temp_path, arcname='.')\n",
    "\n",
    "            # Save to Redis\n",
    "            archive_data = archive_buffer.getvalue()\n",
    "            self.redis_client.set(self.redis_key, archive_data)\n",
    "            if self.redis_expire is not None:\n",
    "                self.redis_client.expire(self.redis_key, self.redis_expire)\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary directory\n",
    "            if temp_dir and Path(temp_dir).exists():\n",
    "                shutil.rmtree(temp_dir)\n",
    "\n",
    "    def _load_model_from_redis(self) -> None:\n",
    "        \"\"\"Load model from Redis tar archive.\"\"\"\n",
    "        import json\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        import tarfile\n",
    "        import io\n",
    "        from pyspark.ml import PipelineModel\n",
    "\n",
    "        serialized = self.redis_client.get(self.redis_key)\n",
    "        if not serialized:\n",
    "            raise ValueError(f\"No model found in Redis with key: {self.redis_key}\")\n",
    "\n",
    "        temp_dir = None\n",
    "        try:\n",
    "            # Create temporary directory\n",
    "            temp_dir = tempfile.mkdtemp(prefix=\"skol_model_load_v2_\")\n",
    "            temp_path = Path(temp_dir)\n",
    "\n",
    "            # Extract tar archive\n",
    "            archive_buffer = io.BytesIO(serialized)\n",
    "            with tarfile.open(fileobj=archive_buffer, mode='r:gz') as tar:\n",
    "                tar.extractall(temp_path)\n",
    "\n",
    "            # Load feature pipeline\n",
    "            pipeline_path = temp_path / \"feature_pipeline\"\n",
    "            self._feature_pipeline = PipelineModel.load(str(pipeline_path))\n",
    "\n",
    "            # Load classifier model\n",
    "            classifier_path = temp_path / \"classifier_model\"\n",
    "            classifier_model = PipelineModel.load(str(classifier_path))\n",
    "\n",
    "            # Load metadata\n",
    "            metadata_path = temp_path / \"metadata.json\"\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            self._label_mapping = metadata['label_mapping']\n",
    "            self._reverse_label_mapping = {v: k for k, v in self._label_mapping.items()}\n",
    "\n",
    "            # Recreate the SkolModel wrapper\n",
    "            features_col = self._feature_extractor.get_features_col() if self._feature_extractor else \"combined_idf\"\n",
    "            self._model = SkolModel(\n",
    "                model_type=metadata['config']['model_type'],\n",
    "                features_col=features_col,\n",
    "                **metadata['config'].get('model_params', {})\n",
    "            )\n",
    "            self._model.set_model(classifier_model)\n",
    "            self._model.set_labels(list(self._label_mapping.keys()))\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary directory\n",
    "            if temp_dir and Path(temp_dir).exists():\n",
    "                shutil.rmtree(temp_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab2d58-0ea7-4288-889c-b1bf9c360743",
   "metadata": {},
   "source": [
    "## Build a classifier to identify paragraph types.\n",
    "\n",
    "We save this to redis so that we don't need to train the model every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "m4m45o9n97i",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotated files from: /data/piggy/src/github.com/piggyatbaqaqi/skol/data/annotated\n",
      "Found 190 annotated files\n",
      "Training classifier with SkolClassifierV2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103115.095419  468662 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103115.109263  468667 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103115.757752  468662 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103115.761404  468667 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103119.373325  468502 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103120.011118  468502 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103122.916522  468500 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103123.478732  468500 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103126.228952  469099 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103126.333443  469112 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103126.892241  469099 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765103126.994445  469112 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "/home/piggy/miniconda3/envs/skol/lib/python3.13/site-packages/couchdb/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __version__ = __import__('pkg_resources').get_distribution('CouchDB').version\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RNN Fit] Training RNN model with generator-based approach...\n",
      "[RNN Fit]   Estimated sequences: ~21850\n",
      "[RNN Fit]   Batch size: 16\n",
      "[RNN Fit]   Steps per epoch: 1365\n",
      "[RNN Fit]   Epochs: 4\n",
      "[RNN Fit]   Window size: 20\n",
      "[RNN Fit]   Input size: 1000\n",
      "[RNN Fit] Creating data generator...\n",
      "[RNN Fit] Starting Keras model.fit()...\n",
      "Epoch 1/4\n",
      "\u001b[1m 581/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:16\u001b[0m 327ms/step - accuracy: 0.8999 - loss: 0.3053"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1325/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 334ms/step - accuracy: 0.9145 - loss: 0.2568"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 344:=================================================>   (186 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1334/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 332ms/step - accuracy: 0.9147 - loss: 0.2564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1365/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 336ms/step - accuracy: 0.9331 - loss: 0.1964\n",
      "Epoch 2/4\n",
      "\u001b[1m 670/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:01\u001b[0m 347ms/step - accuracy: 0.9632 - loss: 0.1146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1365/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 342ms/step - accuracy: 0.9662 - loss: 0.1020\n",
      "Epoch 3/4\n",
      "\u001b[1m  58/1365\u001b[0m \u001b[37m\u001b[0m \u001b[1m8:10\u001b[0m 375ms/step - accuracy: 0.9757 - loss: 0.0642"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 808/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:12\u001b[0m 345ms/step - accuracy: 0.9723 - loss: 0.0822"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 716:==================================================>  (190 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 816/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:07\u001b[0m 342ms/step - accuracy: 0.9723 - loss: 0.0821"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1365/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 358ms/step - accuracy: 0.9758 - loss: 0.0707\n",
      "Epoch 4/4\n",
      "\u001b[1m 206/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:26\u001b[0m 230ms/step - accuracy: 0.9822 - loss: 0.0498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 840:=================================================>   (187 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 217/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:12\u001b[0m 220ms/step - accuracy: 0.9822 - loss: 0.0500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 964/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:06\u001b[0m 315ms/step - accuracy: 0.9806 - loss: 0.0543"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 964:===================================================> (193 + 3) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 971/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:03\u001b[0m 313ms/step - accuracy: 0.9806 - loss: 0.0543"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1365/1365\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 338ms/step - accuracy: 0.9817 - loss: 0.0517\n",
      "[RNN Fit] Keras model.fit() completed successfully\n",
      "[RNN Fit] Training completed successfully\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "label_indexed does not exist. Available: filename, prediction",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     32\u001b[39m classifier = SkolClassifierV2(\n\u001b[32m     33\u001b[39m     spark=spark,\n\u001b[32m     34\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     **model_config\n\u001b[32m     55\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m results = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults.get(\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/piggy/src/github.com/piggyatbaqaqi/skol/skol_classifier/classifier_v2.py:355\u001b[39m, in \u001b[36mSkolClassifierV2.fit\u001b[39m\u001b[34m(self, annotated_data)\u001b[39m\n\u001b[32m    352\u001b[39m test_predictions = \u001b[38;5;28mself\u001b[39m._model.predict(test_data)\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Calculate stats\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m stats = \u001b[43mcalculate_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m stats[\u001b[33m'\u001b[39m\u001b[33mtrain_size\u001b[39m\u001b[33m'\u001b[39m] = train_data.count()\n\u001b[32m    357\u001b[39m stats[\u001b[33m'\u001b[39m\u001b[33mtest_size\u001b[39m\u001b[33m'\u001b[39m] = test_data.count()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/piggy/src/github.com/piggyatbaqaqi/skol/skol_classifier/utils.py:88\u001b[39m, in \u001b[36mcalculate_stats\u001b[39m\u001b[34m(predictions, evaluators, verbose)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m evaluators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     85\u001b[39m     evaluators = create_evaluators()\n\u001b[32m     87\u001b[39m stats = {\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mevaluators\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     89\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m: evaluators[\u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m].evaluate(predictions),\n\u001b[32m     90\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m: evaluators[\u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m].evaluate(predictions),\n\u001b[32m     91\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m'\u001b[39m: evaluators[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m].evaluate(predictions)\n\u001b[32m     92\u001b[39m }\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/ml/evaluation.py:111\u001b[39m, in \u001b[36mEvaluator.evaluate\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    109\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._evaluate(dataset)\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/ml/evaluation.py:148\u001b[39m, in \u001b[36mJavaEvaluator._evaluate\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/skol/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/skol/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: label_indexed does not exist. Available: filename, prediction"
     ]
    }
   ],
   "source": [
    "# Train classifier on annotated data and save to Redis using SkolClassifierV2\n",
    "model_config = {\n",
    "    \"name\": \"RNN BiLSTM (line-level, advanced config)\",\n",
    "    \"model_type\": \"rnn\",\n",
    "    \"use_suffixes\": True,\n",
    "    \"line_level\": True,\n",
    "    \"input_size\": 1000,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": 3,\n",
    "    \"dropout\": 0.3,\n",
    "    \"window_size\": 20,\n",
    "    \"batch_size\": 16,\n",
    "    #\"epochs\": 4,\n",
    "    \"epochs\": 1,\n",
    "    \"num_workers\": 2,\n",
    "    \"verbosity\": 1,\n",
    "}\n",
    "\n",
    "if not redis_client.exists(classifier_model_name):\n",
    "    # Get annotated training files\n",
    "    annotated_path = Path.cwd().parent / \"data\" / \"annotated\"\n",
    "    print(f\"Loading annotated files from: {annotated_path}\")\n",
    "\n",
    "    if annotated_path.exists():\n",
    "        annotated_files = get_file_list(str(annotated_path), pattern=\"**/*.ann\")\n",
    "\n",
    "        if len(annotated_files) > 0:\n",
    "            print(f\"Found {len(annotated_files)} annotated files\")\n",
    "\n",
    "            # Train using SkolClassifierV2 with unified API\n",
    "            print(\"Training classifier with SkolClassifierV2...\")\n",
    "            classifier = SkolClassifierV2(\n",
    "                spark=spark,\n",
    "\n",
    "                # Input\n",
    "                input_source='files',\n",
    "                file_paths=annotated_files,\n",
    "\n",
    "                # Model I/O\n",
    "                auto_load_model=False,  # Fit a new model.\n",
    "                model_storage='redis',\n",
    "                redis_client=redis_client,\n",
    "                redis_key=classifier_model_name,\n",
    "                redis_expire=classifier_model_expire,\n",
    "\n",
    "\n",
    "                # Output options\n",
    "                output_dest='couchdb',\n",
    "                couchdb_url=couchdb_url,\n",
    "                couchdb_database=ingest_db_name,\n",
    "                output_couchdb_suffix='.ann',\n",
    "                \n",
    "                # Model and preprocssing options\n",
    "                **model_config\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            results = classifier.fit()\n",
    "\n",
    "            print(f\"Training complete!\")\n",
    "            print(f\"  Accuracy: {results.get('accuracy', 0):.4f}\")\n",
    "            print(f\"  F1 Score: {results.get('f1_score', 0):.4f}\")\n",
    "            print(f\" Model automatically saved to Redis with key: {classifier_model_name}\")\n",
    "        else:\n",
    "            print(f\"No annotated files found in {annotated_path}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {annotated_path}\")\n",
    "        print(\"Please ensure annotated training data is available.\")\n",
    "else:\n",
    "    print(f\"Skipping generation032.1 of model {classifier_model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fe9eb-1f1b-4aa8-9796-169faeeaf432",
   "metadata": {},
   "source": [
    "## Extract the taxa names and descriptions\n",
    "\n",
    "We use a classifier to extract taxa names and descriptions from articles, issues, and books. The YEDDA annotated texts are written back to CouchDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d42f1-76de-4900-a2d8-7c3001e7ea52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict from CouchDB and save back to CouchDB using SkolClassifierV2\n",
    "print(\"Initializing classifier with unified V2 API...\")\n",
    "classifier = SkolClassifierV2(\n",
    "    spark=spark,\n",
    "    input_source='couchdb',\n",
    "    couchdb_url=couchdb_url,\n",
    "    couchdb_database=ingest_db_name,\n",
    "    couchdb_username=couchdb_username,\n",
    "    couchdb_password=couchdb_password,\n",
    "    couchdb_pattern='*.txt',\n",
    "    output_dest='couchdb',\n",
    "    output_couchdb_suffix='.ann',\n",
    "    model_storage='redis',\n",
    "    redis_client=redis_client,\n",
    "    redis_key=classifier_model_name,\n",
    "    auto_load_model=True,\n",
    "    coalesce_labels=True,\n",
    "    output_format='annotated',\n",
    "    **model_config\n",
    ")\n",
    "\n",
    "print(f\"Model loaded from Redis: {classifier_model_name}\")\n",
    "\n",
    "# Load, predict, and save in a streamlined workflow\n",
    "print(\"\\nLoading and classifying documents from CouchDB...\")\n",
    "raw_df = classifier.load_raw()\n",
    "print(f\"Loaded {raw_df.count()} text documents\")\n",
    "raw_df.show(10)\n",
    "\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = classifier.predict(raw_df)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "predictions.select(\n",
    "    \"doc_id\", \"attachment_name\", \"predicted_label\"\n",
    ").show(5, truncate=50)\n",
    "\n",
    "# Save results back to CouchDB\n",
    "print(\"\\nSaving predictions back to CouchDB...\")\n",
    "classifier.save_annotated(predictions)\n",
    "\n",
    "print(f\"\\n Predictions saved to CouchDB as .ann attachments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1701a1c-864f-494a-a393-7ec77ac14f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"predicted_label\", \"annotated_value\").where('predicted_label = \"Nomenclature\"').show()\n",
    "predictions.groupBy(\"predicted_label\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002ae67-9dc9-4335-adf4-d249905d2ad9",
   "metadata": {},
   "source": [
    "Here we estimate an approximation for the number of Taxon structures we'd like to find. The abbreviation \"nov.\" (\"novum\") indicates a new taxon in the current article. This should be a lower bound, as it is not unusual to redescribe a species, e.g. in a survey article or monograph on a genus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9de42c-8b11-4dfe-aa4b-faa838f3dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"*\").filter(col(\"annotated_value\").contains(\"nov.\")).where(\"predicted_label = 'Nomenclature'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca929b6a-75e5-4dcb-ace2-8d420f42ce41",
   "metadata": {},
   "source": [
    "## Build the Taxon objects and store them in CouchDB\n",
    "We use CouchDB to store a full record for each taxon. We copy all metadata to the taxon records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e285b7-0afa-486e-94f6-41bcb1d7524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchDBFile(CDBF):\n",
    "    \"\"\"\n",
    "    File-like object that reads from CouchDB attachment content.\n",
    "\n",
    "    This class extends FileObject to support reading text from CouchDB\n",
    "    attachments while preserving database metadata (doc_id, attachment_name,\n",
    "    and database name).\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201b4fb-3b89-414e-b0d7-d62367bd05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_taxa_to_couchdb import (\n",
    "    TaxonExtractor,\n",
    "    generate_taxon_doc_id,\n",
    "    extract_taxa_from_partition,\n",
    "    convert_taxa_to_rows\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22648996-4d9f-47f1-ab4f-6b5fd54f97e9",
   "metadata": {},
   "source": [
    "## Build Taxon objects\n",
    "\n",
    "Here we extract the Taxon objects from the annotated attachments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08069896-722b-45c7-8b95-78cbb89e8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_couchdb_url = couchdb_url\n",
    "ingest_username = couchdb_username\n",
    "ingest_password = couchdb_password\n",
    "taxon_couchdb_url = couchdb_url\n",
    "taxon_username = couchdb_username\n",
    "taxon_password = couchdb_password\n",
    "pattern = '*.txt.ann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed264e3-3e3c-4c32-bb99-2205edaa4bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create TaxonExtractor instance with database configuration\n",
    "extractor = TaxonExtractor(\n",
    "    spark=spark,\n",
    "    ingest_couchdb_url=ingest_couchdb_url,\n",
    "    ingest_db_name=ingest_db_name,\n",
    "    taxon_db_name=taxon_db_name,\n",
    "    ingest_username=ingest_username,\n",
    "    ingest_password=ingest_password,\n",
    "    taxon_username=taxon_username,\n",
    "    taxon_password=taxon_password\n",
    ")\n",
    "\n",
    "print(\"TaxonExtractor initialized\")\n",
    "print(f\"  Ingest DB: {ingest_db_name}\")\n",
    "print(f\"  Taxon DB:  {taxon_db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5acdf9-f40f-4ef7-b18b-b5f8236b248b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Load annotated documents\n",
    "print(\"\\nStep 1: Loading annotated documents from CouchDB...\")\n",
    "annotated_df = extractor.load_annotated_documents(pattern='*.txt.ann')\n",
    "print(f\"Loaded {annotated_df.count()} annotated documents\")\n",
    "annotated_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952917da-82e8-4ed0-8876-e1705f4631ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Extract taxa to DataFrame\n",
    "print(\"\\nStep 2: Extracting taxa from annotated documents...\")\n",
    "taxa_df = extractor.extract_taxa(annotated_df)\n",
    "print(f\"Extracted {taxa_df.count()} taxa\")\n",
    "taxa_df.printSchema()\n",
    "taxa_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f01347-0543-4499-a2e7-065a54e61938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Inspect actual Taxon objects from the RDD (optional debugging)\n",
    "print(\"\\n=== Sample Taxon Objects ===\")\n",
    "taxa_rdd = annotated_df.rdd.mapPartitions(\n",
    "    lambda partition: extract_taxa_from_partition(iter(partition), ingest_db_name)  # type: ignore[reportUnknownArgumentType]\n",
    ")\n",
    "for i, taxon in enumerate(taxa_rdd.take(3)):\n",
    "    print(f\"\\nTaxon {i+1}:\")\n",
    "    print(f\"  Type: {type(taxon)}\")\n",
    "    print(f\"  Has nomenclature: {taxon.has_nomenclature()}\")\n",
    "    taxon_row = taxon.as_row()\n",
    "    print(f\"  Taxon name: {taxon_row['taxon'][:80]}...\")\n",
    "    print(f\"  Source: {taxon_row['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7af36b-f190-4dd7-8f29-a9ed11f64c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Save taxa to CouchDB\n",
    "print(\"\\nStep 4: Saving taxa to CouchDB...\")\n",
    "results_df = extractor.save_taxa(taxa_df)\n",
    "\n",
    "# Show detailed results\n",
    "results_df.groupBy(\"success\").count().show(truncate=False)\n",
    "\n",
    "# If there are failures, show error messages\n",
    "print(\"\\nError messages:\")\n",
    "results_df.filter(\"success = false\").select(\"error_message\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ewsk11g9lpl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run the complete pipeline in one step\n",
    "# Uncomment to use the simplified one-step approach:\n",
    "\n",
    "# print(\"\\nRunning complete pipeline...\")\n",
    "# results = extractor.run_pipeline(pattern='*.txt.ann')\n",
    "#\n",
    "# successful = results.filter(\"success = true\").count()\n",
    "# failed = results.filter(\"success = false\").count()\n",
    "#\n",
    "# print(f\"\\nPipeline Results:\")\n",
    "# print(f\"  Successful: {successful}\")\n",
    "# print(f\"  Failed:     {failed}\")\n",
    "#\n",
    "# results.groupBy(\"success\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769c280-8139-4228-b5a7-d21dab100910",
   "metadata": {},
   "source": [
    "### Observations on the classification models\n",
    "\n",
    "The line-by-line classification model is classifying many Description lines as Misc-exposition. It works reasonably well for Nomenclature.\n",
    "\n",
    "The problem with the paragraph classification model is that the heuristic paragrph parser does not generalize well to the more modern journals.\n",
    "\n",
    "One possible approach to investigate is adding heuristics to the label-merging code to convert some Misc-exposition lines to Description if they are surrounded by Description paragraphs.\n",
    "\n",
    "A more sophisticated approach is to use a completely new model that has some memory, such as an RNN, or a two pass model that adds the label of the previous line(s) as added features for each line.\n",
    "\n",
    "It may become necessary to hand annotate some of the more modern journals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ee719-eb73-425b-ad44-6f30158c8d5e",
   "metadata": {},
   "source": [
    "## Dr. Drafts document embedding\n",
    "\n",
    "Dr. Drafts is the framework we use to embed all the descriptions into a searchable space. SBERT is a model that can embed sentences into a semantic space such that sentences with similar meaning are near each other. The data structure that we build here is central to the eventual function of the SKOL web site.\n",
    "\n",
    "Dr. Drafts loads taxon documents from the CouchDB, and builds an embedding which it saves to redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c550d-15f4-426c-b11e-d7ac6691e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dr_drafts_mycosearch.data import SKOL_TAXA as STX\n",
    "from dr_drafts_mycosearch.compute_embeddings import EmbeddingsComputer as EC\n",
    "\n",
    "class SKOL_TAXA(STX):\n",
    "    \"\"\"Data interface for Synopotic Key of Life Taxa in CouchDB.\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a79b3c-4ac4-4208-9346-0a5bc90e0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsComputer(EC):\n",
    "    \"\"\"Class for computing and storing embeddings from narrative data.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345caed-a301-49ca-a182-ce0ace7e839b",
   "metadata": {},
   "source": [
    "## Compute Embeddings\n",
    "\n",
    "We use SBERT to embed the taxa into a search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c806139-15d0-4904-9ec9-8f7e6ada5b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skol_taxa = SKOL_TAXA(\n",
    "    couchdb_url=\"http://localhost:5984\",\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password,\n",
    "    db_name=taxon_db_name\n",
    ")\n",
    "descriptions = skol_taxa.get_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e661cb-fc85-4c1a-9617-b61ca7d1199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not redis_client.exists(embedding_name):\n",
    "\n",
    "    embedder = EmbeddingsComputer(\n",
    "        idir='/dev/null',\n",
    "        redis_url='redis://localhost:6379',\n",
    "        redis_expire=embedding_expire,\n",
    "        embedding_name=embedding_name,\n",
    "    )\n",
    "\n",
    "    embedding_result = embedder.run(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1498755-a114-4c46-985d-47f47e7abee8",
   "metadata": {},
   "source": [
    "## Compute JSON versions of all descriptions\n",
    "\n",
    "There is an anticipated need for the details of each description to be available as a nested JSON structure, which can be used to build menus with features, subfeatures, and values.\n",
    "\n",
    "The TaxaJSONTranslator reads taxa from the CouchDB and writes annotated taxa back out to CouchDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83954366-44a3-430f-92b9-526ac08637d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxaJSONTranslator(TJT):\n",
    "    \"\"\"\n",
    "    Translates taxa descriptions to structured JSON using a fine-tuned Mistral model.\n",
    "\n",
    "    This class is optimized for processing PySpark DataFrames created by\n",
    "    TaxonExtractor.load_taxa(), adding a new column with JSON-formatted features.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ef2d8-ce08-400a-91d6-f089d0f2e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TaxaJSONTranslator(\n",
    "    spark=spark,\n",
    "    base_model_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    max_length=2048,\n",
    "    max_new_tokens=1024,\n",
    "    device=\"cuda\",\n",
    "    load_in_4bit=True,\n",
    "    use_auth_token=True,\n",
    "    couchdb_url=couchdb_url,\n",
    "    username=couchdb_username,\n",
    "    password=couchdb_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b0a40-6839-47c9-ac9e-409b2803f93e",
   "metadata": {},
   "source": [
    "### Run the mistral model to generate JSON from each Taxon description."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d39221ce-9e84-41fc-b741-0917afd8ecf6",
   "metadata": {},
   "source": [
    "descriptions_df = translator.load_taxa(db_name=taxon_db_name).limit(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "334f6d67-fef5-4d4c-b98c-71014a74e4ea",
   "metadata": {},
   "source": [
    "descriptions_df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14202225-e1be-4e68-8b7a-46f3002695b0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "json_annotated_df = translator.translate_descriptions_batch(\n",
    "    taxa_df=descriptions_df,\n",
    "    batch_size=10,\n",
    "    description_col=\"description\",\n",
    "    output_col=\"json_annotated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e317f-b02a-417e-a03a-cfcb27ec2e87",
   "metadata": {},
   "source": [
    "### Add the generated fields as a field on the objects generated by save_taxa."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c528c46-3018-408e-b2d2-6407456b3862",
   "metadata": {},
   "source": [
    "results_df = translator.save_taxa(json_annotated_df, db_name=json_taxon_db_name)\n",
    "\n",
    "results_df.groupBy(\"success\").count().show(truncate=False)\n",
    "\n",
    "print(\"\\nError messages:\")\n",
    "results_df.filter(\"success = false\").select(\"error_message\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526beef-5503-4fac-a9a5-427367d625a5",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "We use Agglomerative Clustering to group the taxa into \"clades\" based in cosine similarity of their SBERT embeddings. We then load them into neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e0d92-b22b-40ec-ad75-1aab43355c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxon_clusterer import TaxonClusterer as TC\n",
    "\n",
    "class TaxonClusterer(TC):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f97734-e9d9-4f2e-96f0-3f298d4a93a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterer = TaxonClusterer(\n",
    "    redis_host=\"localhost\",\n",
    "    redis_port=6379,\n",
    "    redis_db=0,\n",
    "    neo4j_uri=neo4j_uri,\n",
    ")\n",
    "\n",
    "# Load embeddings from Redis\n",
    "(embeddings, taxon_names, metadata) = clusterer.load_embeddings(embedding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9004117-e280-448e-ab39-b7f4aa449c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26683a0-6879-44be-869a-a87748aac888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "linkage_matrix = clusterer.cluster(method=\"average\", metric=\"cosine\")\n",
    "\n",
    "# Store in Neo4j with root named \"Fungi\"\n",
    "clusterer.store_in_neo4j(root_name=\"Fungi\", clear_existing=True)\n",
    "\n",
    "print(\" Clustering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f1457-b1bf-4e02-9b0c-ce194f33969d",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* doi Foundation, \"DOI Citation Formatter HTTP API\", https://citation.doi.org/api-docs.html, accessed 2025-11-12.\n",
    "* Yang, Jie and Zhang, Yue and Li, Linwei and Li, Xingxuan, 2018, \"YEDDA: A Lightweight Collaborative Text Span Annotation Tool\", Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, http://aclweb.org/anthology/P18-4006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82412f-0fd7-4526-a949-3732c2b26624",
   "metadata": {},
   "source": [
    "## Appendix: On the use of an AI Coder\n",
    "\n",
    "Portions of this work were completed with the aid of Claude Code Pro. I wish to give a clarifying example of how I've used this very powerful tool, and reveal why I am comfortable with claiming authorship of the resulting code.\n",
    "\n",
    "For this project I needed results from an earlier class project in which a trio of students built and evaluated models for classifying paragraphs. The earlier work was built as a iPython Notebook, with many examples and inline code. Just copying the earlier notebook would have introduced many irrelevant details and would not further the overall project.\n",
    "\n",
    "I asked Claude Code to translate the notebook into a module that I could import. It did a pretty good job. Without being told, it made a submodule, extract the illustrative code as examples, wrote reasonable documentation and created packaging for the module.\n",
    "\n",
    "The skill level of the coding was roughly that of a highly disciplined average junior programmer. The architecture was simplistic and violated several design constraints such as DRY. I requested specific refactorings, such as asking for a group of functions to be converted into an object that shared duplicated parameters.\n",
    "\n",
    "The initial code used REST interfaces directly, and read all the data into a single machine, not using pyspark correctly. Through a series of refactorings, I asked that the code use appropriate libraries I named, and create correct udf functions to execute transformations in parallel.\n",
    "\n",
    "I walked the AI through creating an object that I could use to illustrate my use of redis and couchdb interfaces, while leaving the irrelevant details in a separate library.\n",
    "\n",
    "In short, I still have to understand good design principles. I have to be able to recognize where appropriate libraries were applicable. I still have to understand the frameworks I am working with.\n",
    "\n",
    "I now have a strong understanding of the difference between \"vibe coding\" and AI-assisted software engineering. In my first 4 hours with Claude Code, I was able to produce roughly 4 days' worth of professional-grade working code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
